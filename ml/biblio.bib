%%%%%%%
% BEGIN KYLE
% Bib file from "Machine learning and the physical sciences" https://arxiv.org/abs/1903.10563
%%%%%%%
@inproceedings{Belkin18,
  author    = {Mikhail Belkin and
               Siyuan Ma and
               Soumik Mandal},
  title     = {To Understand Deep Learning We Need to Understand Kernel Learning},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July
               10-15, 2018},
  pages     = {540--548},
  year      = {2018},
  crossref  = {DBLP:conf/icml/2018},
  url       = {http://proceedings.mlr.press/v80/belkin18a.html},
  timestamp = {Wed, 03 Apr 2019 18:17:30 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/BelkinMM18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@proceedings{DBLP:conf/icml/2018,
  editor    = {Jennifer G. Dy and
               Andreas Krause},
  title     = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July
               10-15, 2018},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  publisher = {{PMLR}},
  year      = {2018},
  url       = {http://proceedings.mlr.press/v80/},
  timestamp = {Mon, 09 Aug 2021 15:41:17 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/2018.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Carleo:2019ptp,
    author = "Carleo, Giuseppe and Cirac, Ignacio and Cranmer, Kyle and Daudet, Laurent and Schuld, Maria and Tishby, Naftali and Vogt-Maranto, Leslie and Zdeborov\'a, Lenka",
    title = "{Machine learning and the physical sciences}",
    eprint = "1903.10563",
    archivePrefix = "arXiv",
    primaryClass = "physics.comp-ph",
    doi = "10.1103/RevModPhys.91.045002",
    journal = "Rev. Mod. Phys.",
    volume = "91",
    number = "4",
    pages = "045002",
    year = "2019"
}

%% To add

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}

@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M},
  year={2006},
  publisher={springer}
}


@article{mannelli2018marvels,
  title={Marvels and pitfalls of the langevin algorithm in noisy high-dimensional inference},
  author={Mannelli, Stefano Sarao and Biroli, Giulio and Cammarota, Chiara and Krzakala, Florent and Urbani, Pierfrancesco and Zdeborov{\'a}, Lenka},
  journal={arXiv preprint arXiv:1812.09066},
  year={2018}
}

@article{mannelli2019passed,
  title={Passed \& Spurious: analysing descent algorithms and local minima in spiked matrix-tensor model},
  author={Mannelli, Stefano Sarao and Krzakala, Florent and Urbani, Pierfrancesco and Zdeborov\'{a}, Lenka},
  journal={arXiv preprint arXiv:1902.00139},
  year={2019}
}

@article{goldt2019generalisation,
  title={Generalisation dynamics of online learning in over-parameterised neural networks},
  author={Goldt, Sebastian and Advani, Madhu S and Saxe, Andrew M and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  journal={arXiv preprint arXiv:1901.09085},
  year={2019}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@book{geron2017hands,
  title={Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems},
  author={G{\'e}ron, Aur{\'e}lien},
  year={2017},
  publisher={" O'Reilly Media, Inc."}
}



@inproceedings{larochelle2011neural,
  title={The neural autoregressive distribution estimator},
  author={Larochelle, Hugo and Murray, Iain},
  booktitle={Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages={29--37},
  year={2011}
}

@inproceedings{papamakarios2017masked,
  title={Masked autoregressive flow for density estimation},
  author={Papamakarios, George and Murray, Iain and Pavlakou, Theo},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2335--2344},
  year={2017}
}

@inproceedings{2015arXiv150505770J,
  title={Variational Inference with Normalizing Flows},
  author={Rezende, Danilo and Mohamed, Shakir},
  booktitle={International Conference on Machine Learning},
  eprint = {1505.05770},
  pages={1530--1538},
  year={2015}
}




@article{Cranmer:2016lzt,
      author         = "Cranmer, Kyle and Louppe, Gilles",
      title          = "{Unifying generative models and exact likelihood-free
                        inference with conditional bijections}",
      journal        = "J. Brief Ideas",
      year           = "2016",
      note            = "10.5281/zenodo.198541",
      SLACcitation   = "%%CITATION = INSPIRE-1503115;%%"
}


@inproceedings{le2017inference,
  title={Inference Compilation and Universal Probabilistic Programming},
  author={Le, Tuan Anh and Baydin, Atilim Gunes and Wood, Frank},
  booktitle={Artificial Intelligence and Statistics},
  pages={1338--1348},
  eprint={arXiv:1610.09900},
  year={2017}
}

@ARTICLE{2014arXiv1412.4446A,
   author = {{Ajakan}, H. and {Germain}, P. and {Larochelle}, H. and {Laviolette}, F. and 
	{Marchand}, M.},
    title = "{Domain-Adversarial Neural Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1412.4446},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
     year = 2014,
    month = dec,
   adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1412.4446A},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{Guest:2016iqz,
      author         = "Guest, Daniel and Collado, Julian and Baldi, Pierre and
                        Hsu, Shih-Chieh and Urban, Gregor and Whiteson, Daniel",
      title          = "{Jet Flavor Classification in High-Energy Physics with
                        Deep Neural Networks}",
      journal        = "Phys. Rev.",
      volume         = "D94",
      year           = "2016",
      number         = "11",
      pages          = "112002",
      doi            = "10.1103/PhysRevD.94.112002",
      eprint         = "1607.08633",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ex",
      SLACcitation   = "%%CITATION = ARXIV:1607.08633;%%"
}


@article{Adam:2003kg,
      author         = "Adam, W. and Fruhwirth, R. and Strandlie, A. and Todorov,
                        T.",
      title          = "{Reconstruction of electrons with the Gaussian sum filter
                        in the CMS tracker at LHC}",
      booktitle      = "{Proceedings, 13th International Conference on Computing
                        in High-Enery and Nuclear Physics (CHEP 2003): La Jolla,
                        California, March 24-28, 2003}",
      journal        = "eConf",
      volume         = "C0303241",
      year           = "2003",
      pages          = "TULT009",
      doi            = "10.1088/0954-3899/31/9/N01",
      note           = "[J. Phys.G31,N9(2005)]",
      eprint         = "physics/0306087",
      archivePrefix  = "arXiv",
      primaryClass   = "physics.data-an",
      reportNumber   = "CHEP-2003-TULT009",
      SLACcitation   = "%%CITATION = PHYSICS/0306087;%%"
}

@ARTICLE{deepkalman1,
   author = {{Krishnan}, R.~G. and {Shalit}, U. and {Sontag}, D.},
    title = "{Deep Kalman Filters}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1511.05121},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Learning},
     year = 2015,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151105121K},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@inproceedings{deepkalman2,
  title={Structured Inference Networks for Nonlinear State Space Models.},
  author={Krishnan, Rahul G and Shalit, Uri and Sontag, David},
  booktitle={AAAI},
  pages={2101--2109},
  year={2017}
}


% end to add


@article{Radovic:2018dip,
      author         = "Radovic, Alexander and Williams, Mike and Rousseau, David
                        and Kagan, Michael and Bonacorsi, Daniele and Himmel,
                        Alexander and Aurisano, Adam and Terao, Kazuhiro and
                        Wongjirad, Taritree",
      title          = "{Machine learning at the energy and intensity frontiers
                        of particle physics}",
      journal        = "Nature",
      volume         = "560",
      year           = "2018",
      number         = "7716",
      pages          = "41-48",
      doi            = "10.1038/s41586-018-0361-2",
      reportNumber   = "FERMILAB-PUB-18-436-ND",
      SLACcitation   = "%%CITATION = NATUA,560,41;%%"
}


@article{Guest:2018yhq,
      author         = "Guest, Dan and Cranmer, Kyle and Whiteson, Daniel",
      title          = "{Deep Learning and its Application to LHC Physics}",
      journal        = "Ann. Rev. Nucl. Part. Sci.",
      volume         = "68",
      year           = "2018",
      pages          = "161-181",
      doi            = "10.1146/annurev-nucl-101917-021019",
      eprint         = "1806.11484",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ex",
      SLACcitation   = "%%CITATION = ARXIV:1806.11484;%%"
}

% BDTs
@inproceedings{Breiman1984ClassificationAR,
  title={Classification and Regression Trees},
  author={Leo Breiman and Joseph H Friedman and R. A. Olshen and C. J. Stone},
  year={1984}
}

@article{Freund:1997xna,
      author         = "Freund, Yoav and Schapire, Robert E.",
      title          = "{A Decision-Theoretic Generalization of On-Line Learning
                        and an Application to Boosting}",
      journal        = "J. Comput. Syst. Sci.",
      volume         = "55",
      year           = "1997",
      number         = "1",
      pages          = "119-139",
      doi            = "10.1006/jcss.1997.1504",
      SLACcitation   = "%%CITATION = JCSSB,55,119;%%"
}

@article{Roe:2004na,
      author         = "Roe, Byron P. and Yang, Hai-Jun and Zhu, Ji and Liu, Yong
                        and Stancu, Ion and McGregor, Gordon",
      title          = "{Boosted decision trees, an alternative to artificial
                        neural networks}",
      journal        = "Nucl. Instrum. Meth.",
      volume         = "A543",
      year           = "2005",
      number         = "2-3",
      pages          = "577-584",
      doi            = "10.1016/j.nima.2004.12.018",
      eprint         = "physics/0408124",
      archivePrefix  = "arXiv",
      primaryClass   = "physics",
      SLACcitation   = "%%CITATION = PHYSICS/0408124;%%"
}


% Deep learning
@article{Baldi:2014kfa,
      author         = "Baldi, Pierre and Sadowski, Peter and Whiteson, Daniel",
      title          = "{Searching for Exotic Particles in High-Energy Physics
                        with Deep Learning}",
      journal        = "Nature Commun.",
      volume         = "5",
      year           = "2014",
      pages          = "4308",
      doi            = "10.1038/ncomms5308",
      eprint         = "1402.4735",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ph",
      SLACcitation   = "%%CITATION = ARXIV:1402.4735;%%"
}

@article{deOliveira:2015xxd,
      author         = "de Oliveira, Luke and Kagan, Michael and Mackey, Lester
                        and Nachman, Benjamin and Schwartzman, Ariel",
      title          = "{Jet-images — deep learning edition}",
      journal        = "JHEP",
      volume         = "07",
      year           = "2016",
      pages          = "069",
      doi            = "10.1007/JHEP07(2016)069",
      eprint         = "1511.05190",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ph",
      SLACcitation   = "%%CITATION = ARXIV:1511.05190;%%"
}

% Likelihood-free inference

@article{Cranmer:2015bka,
      author         = "Cranmer, Kyle and Pavez, Juan and Louppe, Gilles",
      title          = "{Approximating Likelihood Ratios with Calibrated
                        Discriminative  Classifiers}",
      year           = "2015",
      eprint         = "1506.02169",
      archivePrefix  = "arXiv",
      primaryClass   = "stat.AP",
      SLACcitation   = "%%CITATION = ARXIV:1506.02169;%%"
}

@article{Brehmer:2018hga,
      author         = "Brehmer, Johann and Louppe, Gilles and Pavez, Juan and
                        Cranmer, Kyle",
      title          = "{Mining gold from implicit models to improve
                        likelihood-free inference}",
      year           = "2018",
      eprint         = "1805.12244",
      archivePrefix  = "arXiv",
      primaryClass   = "stat.ML",
      SLACcitation   = "%%CITATION = ARXIV:1805.12244;%%"
}

@article{Brehmer:2018eca,
      author         = "Brehmer, Johann and Cranmer, Kyle and Louppe, Gilles and
                        Pavez, Juan",
      title          = "{A Guide to Constraining Effective Field Theories with
                        Machine Learning}",
      journal        = "Phys. Rev.",
      volume         = "D98",
      year           = "2018",
      number         = "5",
      pages          = "052004",
      doi            = "10.1103/PhysRevD.98.052004",
      eprint         = "1805.00020",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ph",
      SLACcitation   = "%%CITATION = ARXIV:1805.00020;%%"
}

@article{Brehmer:2018kdj,
      author         = "Brehmer, Johann and Cranmer, Kyle and Louppe, Gilles and
                        Pavez, Juan",
      title          = "{Constraining Effective Field Theories with Machine
                        Learning}",
      journal        = "Phys. Rev. Lett.",
      volume         = "121",
      year           = "2018",
      number         = "11",
      pages          = "111801",
      doi            = "10.1103/PhysRevLett.121.111801",
      eprint         = "1805.00013",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ph",
      SLACcitation   = "%%CITATION = ARXIV:1805.00013;%%"
}

@article{baldi2016jet,
  title={Jet substructure classification in high-energy physics with deep neural networks},
  author={Baldi, Pierre and Bauer, Kevin and Eng, Clara and Sadowski, Peter and Whiteson, Daniel},
  journal={Physical Review D},
  volume={93},
  number={9},
  pages={094034},
  year={2016},
  publisher={APS}
}

@article{Baldi:2016fzo,
      author         = "Baldi, Pierre and Cranmer, Kyle and Faucett, Taylor and
                        Sadowski, Peter and Whiteson, Daniel",
      title          = "{Parameterized neural networks for high-energy physics}",
      journal        = "Eur. Phys. J.",
      volume         = "C76",
      year           = "2016",
      number         = "5",
      pages          = "235",
      doi            = "10.1140/epjc/s10052-016-4099-4",
      eprint         = "1601.07913",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ex",
      SLACcitation   = "%%CITATION = ARXIV:1601.07913;%%"
}

@article{Alsing:2018eau,
      author         = "Alsing, Justin and Wandelt, Benjamin and Feeney, Stephen",
      title          = "{Massive optimal data compression and density estimation
                        for scalable, likelihood-free inference in cosmology}",
      journal        = "Mon. Not. Roy. Astron. Soc.",
      volume         = "477",
      year           = "2018",
      number         = "3",
      pages          = "2874-2885",
      doi            = "10.1093/mnras/sty819",
      eprint         = "1801.01497",
      archivePrefix  = "arXiv",
      primaryClass   = "astro-ph.CO",
      SLACcitation   = "%%CITATION = ARXIV:1801.01497;%%"
}

@article{Charnock:2018ogm,
      author         = "Charnock, Tom and Lavaux, Guilhem and Wandelt, Benjamin
                        D.",
      title          = "{Automatic physical inference with information maximizing
                        neural networks}",
      journal        = "Phys. Rev.",
      volume         = "D97",
      year           = "2018",
      number         = "8",
      pages          = "083004",
      doi            = "10.1103/PhysRevD.97.083004",
      eprint         = "1802.03537",
      archivePrefix  = "arXiv",
      primaryClass   = "astro-ph.IM",
      SLACcitation   = "%%CITATION = ARXIV:1802.03537;%%"
}


@article{Louppe:2017pay,
      author         = "Louppe, Gilles and Hermans, Joeri and Cranmer, Kyle",
      title          = "{Adversarial Variational Optimization of
                        Non-Differentiable Simulators}",
      year           = "2017",
      eprint         = "1707.07113",
      archivePrefix  = "arXiv",
      primaryClass   = "stat.ML",
      SLACcitation   = "%%CITATION = ARXIV:1707.07113;%%"
}

@inproceedings{Casado:2017cif,
      author         = "Casado, Mario Lezcano and others",
      title          = "{Improvements to Inference Compilation for Probabilistic
                        Programming in  Large-Scale Scientific Simulators}",
      year           = "2017",
      eprint         = "1712.07901",
      archivePrefix  = "arXiv",
      primaryClass   = "cs.AI",
      SLACcitation   = "%%CITATION = ARXIV:1712.07901;%%"
}

@article{Baydin:2018npr,
      author         = "Baydin, Atilim Gunes and Heinrich, Lukas and Bhimji,
                        Wahid and Gram-Hansen, Bradley and Louppe, Gilles and
                        Shao, Lei and Prabhat and Cranmer, Kyle and Wood, Frank",
      title          = "{Efficient Probabilistic Inference in the Quest for
                        Physics Beyond the Standard Model}",
      year           = "2018",
      eprint         = "1807.07706",
      archivePrefix  = "arXiv",
      primaryClass   = "cs.LG",
      SLACcitation   = "%%CITATION = ARXIV:1807.07706;%%"
}

@article{Englert:2018cfo,
      author         = "Englert, Christoph and Galler, Peter and Harris, Philip
                        and Spannowsky, Michael",
      title          = "{Machine Learning Uncertainties with Adversarial Neural
                        Networks}",
      year           = "2018",
      eprint         = "1807.08763",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ph",
      reportNumber   = "IPPP/18/61",
      SLACcitation   = "%%CITATION = ARXIV:1807.08763;%%"
}

% GANs for particle physics

@article{Paganini:2017hrr,
      author         = "Paganini, Michela and de Oliveira, Luke and Nachman,
                        Benjamin",
      title          = "{Accelerating Science with Generative Adversarial
                        Networks: An Application to 3D Particle Showers in
                        Multilayer Calorimeters}",
      journal        = "Phys. Rev. Lett.",
      volume         = "120",
      year           = "2018",
      number         = "4",
      pages          = "042003",
      doi            = "10.1103/PhysRevLett.120.042003",
      eprint         = "1705.02355",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ex",
      SLACcitation   = "%%CITATION = ARXIV:1705.02355;%%"
}

@article{Paganini:2017dwg,
      author         = "Paganini, Michela and de Oliveira, Luke and Nachman,
                        Benjamin",
      title          = "{CaloGAN : Simulating 3D high energy particle showers in
                        multilayer electromagnetic calorimeters with generative
                        adversarial networks}",
      journal        = "Phys. Rev.",
      volume         = "D97",
      year           = "2018",
      number         = "1",
      pages          = "014021",
      doi            = "10.1103/PhysRevD.97.014021",
      eprint         = "1712.10321",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ex",
      SLACcitation   = "%%CITATION = ARXIV:1712.10321;%%"
}

@techreport{ATL-SOFT-PUB-2018-001,
      title         = "{Deep generative models for fast shower simulation in
                       ATLAS}",
      institution   = "CERN",
      author = "{ATLAS Collaboration}",
      address       = "Geneva",
      number        = "ATL-SOFT-PUB-2018-001",
      month         = "Jul",
      year          = "2018",
      reportNumber  = "ATL-SOFT-PUB-2018-001",
      url           = "https://cds.cern.ch/record/2630433",
}

%inductive bias in jet physics

@article{Butter:2017cot,
      author         = "Butter, Anja and Kasieczka, Gregor and Plehn, Tilman and
                        Russell, Michael",
      title          = "{Deep-learned Top Tagging with a Lorentz Layer}",
      journal        = "SciPost Phys.",
      volume         = "5",
      year           = "2018",
      number         = "3",
      pages          = "028",
      doi            = "10.21468/SciPostPhys.5.3.028",
      eprint         = "1707.08966",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ph",
      SLACcitation   = "%%CITATION = ARXIV:1707.08966;%%"
}

@article{Datta:2017lxt,
      author         = "Datta, Kaustuv and Larkoski, Andrew J.",
      title          = "{Novel Jet Observables from Machine Learning}",
      journal        = "JHEP",
      volume         = "03",
      year           = "2018",
      pages          = "086",
      doi            = "10.1007/JHEP03(2018)086",
      eprint         = "1710.01305",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ph",
      SLACcitation   = "%%CITATION = ARXIV:1710.01305;%%"
}


@article{Louppe:2017ipp,
      author         = "Louppe, Gilles and Cho, Kyunghyun and Becot, Cyril and
                        Cranmer, Kyle",
      title          = "{QCD-Aware Recursive Neural Networks for Jet Physics}",
      year           = "2017",
      eprint         = "1702.00748",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ph",
      SLACcitation   = "%%CITATION = ARXIV:1702.00748;%%"
}


% Jet Physics

@article{Larkoski:2017jix,
      author         = "Larkoski, Andrew J. and Moult, Ian and Nachman, Benjamin",
      title          = "{Jet Substructure at the Large Hadron Collider: A Review
                        of Recent Advances in Theory and Machine Learning}",
      year           = "2017",
      eprint         = "1709.04464",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ph",
      SLACcitation   = "%%CITATION = ARXIV:1709.04464;%%"
}

% Neutrino physics

@ARTICLE{2018arXiv180906166C,
       author = {{Choma}, Nicholas and {Monti}, Federico and {Gerhardt}, Lisa and
         {Palczewski}, Tomasz and {Ronaghi}, Zahra and {Prabhat} and
         {Bhimji}, Wahid and {Bronstein}, Michael M. and {Klein}, Spencer R. and
         {Bruna}, Joan},
        title = "{Graph Neural Networks for IceCube Signal Classification}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Astrophysics - Instrumentation and Methods for Astrophysics, Statistics - Machine Learning},
         year = "2018",
        month = "Sep",
          eid = {arXiv:1809.06166},
        pages = {arXiv:1809.06166},
archivePrefix = {arXiv},
       eprint = {1809.06166},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2018arXiv180906166C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{Aurisano:2016jvx,
      author         = "Aurisano, A. and Radovic, A. and Rocco, D. and Himmel, A.
                        and Messier, M. D. and Niner, E. and Pawloski, G. and
                        Psihas, F. and Sousa, A. and Vahle, P.",
      title          = "{A Convolutional Neural Network Neutrino Event
                        Classifier}",
      journal        = "JINST",
      volume         = "11",
      year           = "2016",
      number         = "09",
      pages          = "P09001",
      doi            = "10.1088/1748-0221/11/09/P09001",
      eprint         = "1604.01444",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ex",
      reportNumber   = "FERMILAB-PUB-16-082-ND",
      SLACcitation   = "%%CITATION = ARXIV:1604.01444;%%"
}

@article{Acciarri:2016ryt,
      author         = "Acciarri, R. and others",
      title          = "{Convolutional Neural Networks Applied to Neutrino Events
                        in a Liquid Argon Time Projection Chamber}",
      collaboration  = "MicroBooNE",
      journal        = "JINST",
      volume         = "12",
      year           = "2017",
      number         = "03",
      pages          = "P03011",
      doi            = "10.1088/1748-0221/12/03/P03011",
      eprint         = "1611.05531",
      archivePrefix  = "arXiv",
      primaryClass   = "physics.ins-det",
      reportNumber   = "FERMILAB-PUB-16-538-ND",
      SLACcitation   = "%%CITATION = ARXIV:1611.05531;%%"
}

@article{Renner:2016trj,
      author         = "Renner, J. and others",
      title          = "{Background rejection in NEXT using deep neural
                        networks}",
      collaboration  = "NEXT",
      journal        = "JINST",
      volume         = "12",
      year           = "2017",
      number         = "01",
      pages          = "T01004",
      doi            = "10.1088/1748-0221/12/01/T01004",
      eprint         = "1609.06202",
      archivePrefix  = "arXiv",
      primaryClass   = "physics.ins-det",
      reportNumber   = "FERMILAB-PUB-16-422-CD",
      SLACcitation   = "%%CITATION = ARXIV:1609.06202;%%"
}

@inproceedings{ren2015faster,
  title={Faster r-cnn: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  booktitle={Advances in neural information processing systems},
  pages={91--99},
  year={2015}
}

@inproceedings{DBLP:journals/corr/abs-1809-06166,
  title={Graph Neural Networks for IceCube Signal Classification},
  author={Choma, Nicholas and Monti, Federico and Gerhardt, Lisa and Palczewski, Tomasz and Ronaghi, Zahra and Prabhat, Prabhat and Bhimji, Wahid and Bronstein, Michael and Klein, Spencer and Bruna, Joan},
  booktitle={2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)},
  pages={386--391},
  year={2018},
  organization={IEEE}
}



% systematics

@article{Louppe:2016ylz,
      author         = "Louppe, Gilles and Kagan, Michael and Cranmer, Kyle",
      title          = "{Learning to Pivot with Adversarial Networks}",
      year           = "2016",
      eprint         = "1611.01046",
      archivePrefix  = "arXiv",
      primaryClass   = "stat.ME",
      SLACcitation   = "%%CITATION = ARXIV:1611.01046;%%"
}

@article{Englert:2018cfo,
      author         = "Englert, Christoph and Galler, Peter and Harris, Philip
                        and Spannowsky, Michael",
      title          = "{Machine Learning Uncertainties with Adversarial Neural
                        Networks}",
      year           = "2018",
      eprint         = "1807.08763",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ph",
      reportNumber   = "IPPP/18/61",
      SLACcitation   = "%%CITATION = ARXIV:1807.08763;%%"
}

@article{deCastro:2018mgh,
      author         = "De Castro, Pablo and Dorigo, Tommaso",
      title          = "{INFERNO: Inference-Aware Neural Optimisation}",
      year           = "2018",
      eprint         = "1806.04743",
      archivePrefix  = "arXiv",
      primaryClass   = "stat.ML",
      SLACcitation   = "%%CITATION = ARXIV:1806.04743;%%"
}

@article{Shimmin:2017mfk,
      author         = "Shimmin, Chase and Sadowski, Peter and Baldi, Pierre and
                        Weik, Edison and Whiteson, Daniel and Goul, Edward and
                        Søgaard, Andreas",
      title          = "{Decorrelated Jet Substructure Tagging using Adversarial
                        Neural Networks}",
      journal        = "Phys. Rev.",
      volume         = "D96",
      year           = "2017",
      number         = "7",
      pages          = "074034",
      doi            = "10.1103/PhysRevD.96.074034",
      eprint         = "1703.03507",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ex",
      SLACcitation   = "%%CITATION = ARXIV:1703.03507;%%"
}

@article{Stevens:2013dya,
      author         = "Stevens, Justin and Williams, Mike",
      title          = "{uBoost: A boosting method for producing uniform
                        selection efficiencies from multivariate classifiers}",
      journal        = "JINST",
      volume         = "8",
      year           = "2013",
      pages          = "P12013",
      doi            = "10.1088/1748-0221/8/12/P12013",
      eprint         = "1305.7248",
      archivePrefix  = "arXiv",
      primaryClass   = "nucl-ex",
      SLACcitation   = "%%CITATION = ARXIV:1305.7248;%%"
}

@article{Rogozhnikov:2014zea,
      author         = "Rogozhnikov, Alex and Bukva, Aleksandar and Gligorov, V.
                        V. and Ustyuzhanin, Andrey and Williams, Mike",
      title          = "{New approaches for boosting to uniformity}",
      journal        = "JINST",
      volume         = "10",
      year           = "2015",
      number         = "03",
      pages          = "T03002",
      doi            = "10.1088/1748-0221/10/03/T03002",
      eprint         = "1410.4140",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ex",
      SLACcitation   = "%%CITATION = ARXIV:1410.4140;%%"
}

%WEakly supervised

@article{Komiske:2018oaa,
      author         = "Komiske, Patrick T. and Metodiev, Eric M. and Nachman,
                        Benjamin and Schwartz, Matthew D.",
      title          = "{Learning to classify from impure samples with
                        high-dimensional data}",
      journal        = "Phys. Rev.",
      volume         = "D98",
      year           = "2018",
      number         = "1",
      pages          = "011502",
      doi            = "10.1103/PhysRevD.98.011502",
      eprint         = "1801.10158",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ph",
      reportNumber   = "MIT-CTP-4968, MIT-CTP 4968",
      SLACcitation   = "%%CITATION = ARXIV:1801.10158;%%"
}

@article{Metodiev:2017vrx,
      author         = "Metodiev, Eric M. and Nachman, Benjamin and Thaler,
                        Jesse",
      title          = "{Classification without labels: Learning from mixed
                        samples in high energy physics}",
      journal        = "JHEP",
      volume         = "10",
      year           = "2017",
      pages          = "174",
      doi            = "10.1007/JHEP10(2017)174",
      eprint         = "1708.02949",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ph",
      reportNumber   = "MIT--CTP-4922",
      SLACcitation   = "%%CITATION = ARXIV:1708.02949;%%"
}

@article{Andrews:2018gew,
      author         = "Andrews, M. and Paulini, M. and Gleyzer, S. and Poczos,
                        B.",
      title          = "{End-to-End Event Classification of High-Energy Physics
                        Data}",
      booktitle      = "{Proceedings, 18th International Workshop on Advanced
                        Computing and Analysis Techniques in Physics Research
                        (ACAT 2017): Seattle, WA, USA, August 21-25, 2017}",
      journal        = "J. Phys. Conf. Ser.",
      volume         = "1085",
      year           = "2018",
      number         = "4",
      pages          = "042022",
      doi            = "10.1088/1742-6596/1085/4/042022",
      SLACcitation   = "%%CITATION = 00462,1085,042022;%%"
}

@article{Komiske:2018cqr,
      author         = "Komiske, Patrick T. and Metodiev, Eric M. and Thaler,
                        Jesse",
      title          = "{Energy Flow Networks: Deep Sets for Particle Jets}",
      journal        = "JHEP",
      volume         = "01",
      year           = "2019",
      pages          = "121",
      doi            = "10.1007/JHEP01(2019)121",
      eprint         = "1810.05165",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ph",
      reportNumber   = "MIT-CTP 5064",
      SLACcitation   = "%%CITATION = ARXIV:1810.05165;%%"
}

@article{Komiske:2017aww,
      author         = "Komiske, Patrick T. and Metodiev, Eric M. and Thaler,
                        Jesse",
      title          = "{Energy flow polynomials: A complete linear basis for jet
                        substructure}",
      journal        = "JHEP",
      volume         = "04",
      year           = "2018",
      pages          = "013",
      doi            = "10.1007/JHEP04(2018)013",
      eprint         = "1712.07124",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ph",
      reportNumber   = "MIT-CTP-4965",
      SLACcitation   = "%%CITATION = ARXIV:1712.07124;%%"
}

% anomaly detection

@article{DAgnolo:2018cun,
      author         = "D'Agnolo, Raffaele Tito and Wulzer, Andrea",
      title          = "{Learning New Physics from a Machine}",
      year           = "2018",
      eprint         = "1806.02350",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ph",
      SLACcitation   = "%%CITATION = ARXIV:1806.02350;%%"
}

@article{Collins:2018epr,
      author         = "Collins, Jack H. and Howe, Kiel and Nachman, Benjamin",
      title          = "{Anomaly Detection for Resonant New Physics with Machine
                        Learning}",
      year           = "2018",
      eprint         = "1805.02664",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ph",
      reportNumber   = "FERMILAB-PUB-18-180-T",
      SLACcitation   = "%%CITATION = ARXIV:1805.02664;%%"
}

%generative models

@article{Andreassen:2018apy,
      author         = "Andreassen, Anders and Feige, Ilya and Frye, Christopher
                        and Schwartz, Matthew D.",
      title          = "{JUNIPR: a Framework for Unsupervised Machine Learning in
                        Particle Physics}",
      year           = "2018",
      eprint         = "1804.09720",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ph",
      SLACcitation   = "%%CITATION = ARXIV:1804.09720;%%"
}

% FPGA

@article{Duarte:2018ite,
      author         = "Duarte, Javier and others",
      title          = "{Fast inference of deep neural networks in FPGAs for
                        particle physics}",
      journal        = "JINST",
      volume         = "13",
      year           = "2018",
      number         = "07",
      pages          = "P07027",
      doi            = "10.1088/1748-0221/13/07/P07027",
      eprint         = "1804.06913",
      archivePrefix  = "arXiv",
      primaryClass   = "physics.ins-det",
      reportNumber   = "FERMILAB-PUB-18-089-E",
      SLACcitation   = "%%CITATION = ARXIV:1804.06913;%%"
}

@article{Kasieczka:2019dbj,
      author={Kasieczka, G and Plehn, T and Butter, A and Debnath, D and Fairbairn, M and Fedorko, W and Gay, C and Gouskos, L and Komiske, PT and Leiss, S and others},
      title          = "{The Machine Learning Landscape of Top Taggers}",
      year           = "2019",
      eprint         = "1902.09914",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ph",
      SLACcitation   = "%%CITATION = ARXIV:1902.09914;%%"
}

% Cosmology 

%photo-z template

%{2000ApJ...536..571B,2008ApJ...686.1503B,2006MNRAS.372..565F,}

@ARTICLE{2000ApJ...536..571B,
       author = {{Ben{\'\i}tez}, Narciso},
        title = "{Bayesian Photometric Redshift Estimation}",
      journal = {\apj},
     keywords = {Galaxies: Distances and Redshifts, Galaxies: Photometry, Methods: Statistical, Astrophysics},
         year = "2000",
        month = "Jun",
       volume = {536},
        pages = {571-583},
          doi = {10.1086/308947},
archivePrefix = {arXiv},
       eprint = {astro-ph/9811189},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2000ApJ...536..571B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{2008ApJ...686.1503B,
       author = {{Brammer}, Gabriel B. and {van Dokkum}, Pieter G. and {Coppi}, Paolo},
        title = "{EAZY: A Fast, Public Photometric Redshift Code}",
      journal = {\apj},
     keywords = {cosmology: observations, galaxies: evolution, galaxies: formation, Astrophysics},
         year = "2008",
        month = "Oct",
       volume = {686},
        pages = {1503-1513},
          doi = {10.1086/591786},
archivePrefix = {arXiv},
       eprint = {0807.1533},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2008ApJ...686.1503B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2006MNRAS.372..565F,
       author = {{Feldmann}, R. and {Carollo}, C.~M. and {Porciani}, C. and
         {Lilly}, S.~J. and {Capak}, P. and {Taniguchi}, Y. and
         {Le F{\`e}vre}, O. and {Renzini}, A. and {Scoville}, N. and
         {Ajiki}, M. and {Aussel}, H. and {Contini}, T. and {McCracken}, H. and
         {Mobasher}, B. and {Murayama}, T. and {Sanders}, D. and {Sasaki}, S. and
         {Scarlata}, C. and {Scodeggio}, M. and {Shioya}, Y. and
         {Silverman}, J. and {Takahashi}, M. and {Thompson}, D. and
         {Zamorani}, G.},
        title = "{The Zurich Extragalactic Bayesian Redshift Analyzer and its first application: COSMOS}",
      journal = {MNRAS},
     keywords = {methods: statistical, galaxies: distances and redshifts, galaxies: evolution, galaxies: formation, galaxies: photometry, Astrophysics},
         year = "2006",
        month = "Oct",
       volume = {372},
        pages = {565-577},
          doi = {10.1111/j.1365-2966.2006.10930.x},
archivePrefix = {arXiv},
       eprint = {astro-ph/0609044},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2006MNRAS.372..565F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

%photo-z ML

%{2003MNRAS.339.1195F,2004PASP..116..345C,2013MNRAS.432.1483C,2018arXiv180701391L,}

@ARTICLE{2003MNRAS.339.1195F,
       author = {{Firth}, Andrew E. and {Lahav}, Ofer and {Somerville}, Rachel S.},
        title = "{Estimating photometric redshifts with artificial neural networks}",
      journal = {MNRAS},
     keywords = {methods: data analysis, surveys, galaxies: distances and redshifts, Astrophysics},
         year = "2003",
        month = "Mar",
       volume = {339},
        pages = {1195-1202},
          doi = {10.1046/j.1365-8711.2003.06271.x},
archivePrefix = {arXiv},
       eprint = {astro-ph/0203250},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2003MNRAS.339.1195F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2004PASP..116..345C,
       author = {{Collister}, Adrian A. and {Lahav}, Ofer},
        title = "{ANNz: Estimating Photometric Redshifts Using Artificial Neural Networks}",
      journal = {Publications of the Astronomical Society of the Pacific},
     keywords = {Surveys, Galaxies: Distances and Redshifts, Methods: Data Analysis, Astrophysics},
         year = "2004",
        month = "Apr",
       volume = {116},
        pages = {345-351},
          doi = {10.1086/383254},
archivePrefix = {arXiv},
       eprint = {astro-ph/0311058},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2004PASP..116..345C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013MNRAS.432.1483C,
       author = {{Carrasco Kind}, Matias and {Brunner}, Robert J.},
        title = "{TPZ: photometric redshift PDFs and ancillary information by using prediction trees and random forests}",
      journal = {MNRAS},
     keywords = {methods: data analysis, methods: statistical, surveys, galaxies: distances and redshift, galaxies: statistics, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2013",
        month = "Jun",
       volume = {432},
        pages = {1483-1501},
          doi = {10.1093/mnras/stt574},
archivePrefix = {arXiv},
       eprint = {1303.7269},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2013MNRAS.432.1483C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016PASP..128j4502S,
       author = {{Sadeh}, I. and {Abdalla}, F.~B. and {Lahav}, O.},
        title = "{ANNz2: Photometric Redshift and Probability Distribution Function Estimation using Machine Learning}",
      journal = {Publications of the Astronomical Society of the Pacific},
     keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2016",
        month = "Oct",
       volume = {128},
        pages = {104502},
          doi = {10.1088/1538-3873/128/968/104502},
archivePrefix = {arXiv},
       eprint = {1507.00490},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2016PASP..128j4502S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018arXiv180701391L,
       author = {{Leistedt}, Boris and {Hogg}, David W. and {Wechsler}, Risa H. and
         {DeRose}, Joe},
        title = "{Hierarchical modeling and statistical calibration for photometric redshifts}",
      journal = {arXiv e-prints},
     keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2018",
        month = "Jul",
          eid = {arXiv:1807.01391},
        pages = {arXiv:1807.01391},
archivePrefix = {arXiv},
       eprint = {1807.01391},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2018arXiv180701391L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


% lensing classification

@ARTICLE{2018MNRAS.473.3895L,
       author = {{Lanusse}, Fran{\c{c}}ois and {Ma}, Quanbin and {Li}, Nan and
         {Collett}, Thomas E. and {Li}, Chun-Liang and {Ravanbakhsh}, Siamak and
         {Mandelbaum}, Rachel and {P{\'o}czos}, Barnab{\'a}s},
        title = "{CMU DeepLens: deep learning for automatic image-based galaxy-galaxy strong lens finding}",
      journal = {MNRAS},
     keywords = {gravitational lensing: strong, methods: statistical, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "Jan",
       volume = {473},
        pages = {3895-3906},
          doi = {10.1093/mnras/stx1665},
archivePrefix = {arXiv},
       eprint = {1703.02642},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2018MNRAS.473.3895L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017arXiv171102033R,
       author = {{Ravanbakhsh}, Siamak and {Oliva}, Junier and {Fromenteau}, Sebastien and
         {Price}, Layne C. and {Ho}, Shirley and {Schneider}, Jeff and
         {Poczos}, Barnabas},
        title = "{Estimating Cosmological Parameters from the Dark Matter Distribution}",
      journal = {arXiv e-prints},
     keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2017",
        month = "Nov",
          eid = {arXiv:1711.02033},
        pages = {arXiv:1711.02033},
archivePrefix = {arXiv},
       eprint = {1711.02033},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2017arXiv171102033R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018arXiv181106533H,
       author = {{He}, Siyu and {Li}, Yin and {Feng}, Yu and {Ho}, Shirley and
         {Ravanbakhsh}, Siamak and {Chen}, Wei and {P{\'o}czos}, Barnab{\'a}s},
        title = "{Learning to Predict the Cosmological Structure Formation}",
      journal = {arXiv e-prints},
     keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
         year = "2018",
        month = "Nov",
archivePrefix = {arXiv},
       eprint = {1811.06533},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2018arXiv181106533H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

% Generative models in Cosmo

@ARTICLE{2016arXiv160905796R,
       author = {{Ravanbakhsh}, Siamak and {Lanusse}, Francois and {Mandelbaum}, Rachel and
         {Schneider}, Jeff and {Poczos}, Barnabas},
        title = "{Enabling Dark Energy Science with Deep Generative Models of Galaxy Images}",
      journal = {arXiv e-prints},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
         year = "2016",
        month = "Sep",
          eid = {arXiv:1609.05796},
        pages = {arXiv:1609.05796},
archivePrefix = {arXiv},
       eprint = {1609.05796},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2016arXiv160905796R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ComAC...5....4R,
       author = {{Rodr{\'\i}guez}, Andres C. and {Kacprzak}, Tomasz and
         {Lucchi}, Aurelien and {Amara}, Adam and {Sgier}, Rapha{\"e}l and
         {Fluri}, Janis and {Hofmann}, Thomas and {R{\'e}fr{\'e}gier}, Alexandre},
        title = "{Fast cosmic web simulations with generative adversarial networks}",
      journal = {Computational Astrophysics and Cosmology},
     keywords = {Methods: numerical, Cosmology, Large-scale structure of Universe, Astrophysics - Cosmology and Nongalactic Astrophysics, Statistics - Machine Learning},
         year = "2018",
        month = "Nov",
       volume = {5},
          eid = {4},
        pages = {4},
          doi = {10.1186/s40668-018-0026-4},
archivePrefix = {arXiv},
       eprint = {1801.09070},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2018ComAC...5....4R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

% other cosmo classification and regression 

@ARTICLE{2019MNRAS.484.1526A,
       author = {{Armitage}, Thomas J. and {Kay}, Scott T. and {Barnes}, David J.},
        title = "{An application of machine learning techniques to galaxy cluster mass estimation using the MACSIS simulations}",
      journal = {MNRAS},
     keywords = {galaxies: clusters: general, galaxies: kinematics and dynamics, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2019",
        month = "Apr",
       volume = {484},
        pages = {1526-1537},
          doi = {10.1093/mnras/stz039},
archivePrefix = {arXiv},
       eprint = {1810.08430},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2019MNRAS.484.1526A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{10.1093/mnras/stw2722,
    author = {Henson, Monique A. and Kay, Scott T. and Barnes, David J. and McCarthy, Ian G. and Schaye, Joop and Jenkins, Adrian},
    title = "{The redshift evolution of massive galaxy clusters in the MACSIS simulations}",
    journal = {Monthly Notices of the Royal Astronomical Society},
    volume = {465},
    number = {1},
    pages = {213-233},
    year = {2016},
    month = {10},
    issn = {0035-8711},
    doi = {10.1093/mnras/stw2722},
    url = {https://dx.doi.org/10.1093/mnras/stw2722},
    eprint = {http://oup.prod.sis.lan/mnras/article-pdf/465/1/213/8593372/stw2722.pdf},
}

@ARTICLE{2015ApJ...803...50N,
       author = {{Ntampaka}, M. and {Trac}, H. and {Sutherland}, D.~J. and
         {Battaglia}, N. and {P{\'o}czos}, B. and {Schneider}, J.},
        title = "{A Machine Learning Approach for Dynamical Mass Measurements of Galaxy Clusters}",
      journal = {\apj},
     keywords = {cosmology: theory, dark matter, galaxies: clusters: general, galaxies: kinematics and dynamics, gravitation, large-scale structure of universe, methods: statistical, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2015",
        month = "Apr",
       volume = {803},
          eid = {50},
        pages = {50},
          doi = {10.1088/0004-637X/803/2/50},
archivePrefix = {arXiv},
       eprint = {1410.0686},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2015ApJ...803...50N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{DBLP:journals/corr/abs-1202-0302,
  author    = {Barnab{\'{a}}s P{\'{o}}czos and
               Liang Xiong and
               Dougal J. Sutherland and
               Jeff G. Schneider},
  title     = {Support Distribution Machines},
  journal   = {CoRR},
  volume    = {abs/1202.0302},
  year      = {2012},
  url       = {http://arxiv.org/abs/1202.0302},
  archivePrefix = {arXiv},
  eprint    = {1202.0302},
  timestamp = {Mon, 13 Aug 2018 16:47:51 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1202-0302},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@ARTICLE{2016ApJ...831..135N,
       author = {{Ntampaka}, M. and {Trac}, H. and {Sutherland}, D.~J. and
         {Fromenteau}, S. and {P{\'o}czos}, B. and {Schneider}, J.},
        title = "{Dynamical Mass Measurements of Contaminated Galaxy Clusters Using Machine Learning}",
      journal = {\apj},
     keywords = {cosmology: theory, dark matter, galaxies: clusters: general, galaxies: kinematics and dynamics, gravitation, large-scale structure of universe, methods: statistical, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2016",
        month = "Nov",
       volume = {831},
          eid = {135},
        pages = {135},
          doi = {10.3847/0004-637X/831/2/135},
archivePrefix = {arXiv},
       eprint = {1509.05409},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2016ApJ...831..135N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{2018arXiv181007703N,
       author = {{Ntampaka}, M. and {ZuHone}, J. and {Eisenstein}, D. and {Nagai}, D. and
         {Vikhlinin}, A. and {Hernquist}, L. and {Marinacci}, F. and
         {Nelson}, D. and {Pakmor}, R. and {Pillepich}, A. and {Torrey}, P. and
         {Vogelsberger}, M.},
        title = "{A Deep Learning Approach to Galaxy Cluster X-ray Masses}",
      journal = {arXiv e-prints},
     keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2018",
        month = "Oct",
          eid = {arXiv:1810.07703},
        pages = {arXiv:1810.07703},
archivePrefix = {arXiv},
       eprint = {1810.07703},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2018arXiv181007703N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{2018arXiv181011030P,
       author = {{Peel}, Austin and {Lalande}, Florian and {Starck}, Jean-Luc and
         {Pettorino}, Valeria and {Merten}, Julian and {Giocoli}, Carlo and
         {Meneghetti}, Massimo and {Baldi}, Marco},
        title = "{Distinguishing standard and modified gravity cosmologies with machine learning}",
      journal = {arXiv e-prints},
     keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2018",
        month = "Oct",
          eid = {arXiv:1810.11030},
        pages = {arXiv:1810.11030},
archivePrefix = {arXiv},
       eprint = {1810.11030},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2018arXiv181011030P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{Golkar:2018xkw,
      author         = "Golkar, Siavash and Cranmer, Kyle",
      title          = "{Backdrop: Stochastic Backpropagation}",
      year           = "2018",
      eprint         = "1806.01337",
      archivePrefix  = "arXiv",
      primaryClass   = "stat.ML",
      SLACcitation   = "%%CITATION = ARXIV:1806.01337;%%"
}


% Inference

@ARTICLE{2019arXiv190101359M,
       author = {{Morningstar}, Warren R. and {Perreault Levasseur}, Laurence and
         {Hezaveh}, Yashar D. and {Blandford}, Roger and {Marshall}, Phil and
         {Putzky}, Patrick and {Rueter}, Thomas D. and {Wechsler}, Risa and
         {Welling}, Max},
        title = "{Data-Driven Reconstruction of Gravitationally Lensed Galaxies using Recurrent Inference Machines}",
      journal = {arXiv e-prints},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2019",
        month = "Jan",
          eid = {arXiv:1901.01359},
        pages = {arXiv:1901.01359},
archivePrefix = {arXiv},
       eprint = {1901.01359},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2019arXiv190101359M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

% lensing classification

@article{Li_2016,
	doi = {10.3847/0004-637x/828/1/54},
	url = {https://doi.org/10.3847%2F0004-637x%2F828%2F1%2F54},
	year = 2016,
	month = {aug},
	publisher = {American Astronomical Society},
	volume = {828},
	number = {1},
	pages = {54},
	author = {Nan Li and Michael D. Gladders and Esteban M. Rangel and Michael K. Florian and Lindsey E. Bleem and Katrin Heitmann and Salman Habib and Patricia Fasel},
	title = {{PICS}: {SIMULATIONS} {OF} {STRONG} {GRAVITATIONAL} {LENSING} {IN} {GALAXY} {CLUSTERS}},
	journal = {The Astrophysical Journal},
	abstract = {Gravitational lensing has become one of the most powerful tools available for investigating the “dark side” of the universe. Cosmological strong gravitational lensing, in particular, probes the properties of the dense cores of dark matter halos over decades in mass and offers the opportunity to study the distant universe at flux levels and spatial resolutions otherwise unavailable. Studies of strongly lensed variable sources offer even further scientific opportunities. One of the challenges in realizing the potential of strong lensing is to understand the statistical context of both the individual systems that receive extensive follow-up study, as well as that of the larger samples of strong lenses that are now emerging from survey efforts. Motivated by these challenges, we have developed an image simulation pipeline, Pipeline for Images of Cosmological Strong lensing (PICS), to generate realistic strong gravitational lensing signals from group- and cluster-scale lenses. PICS uses a low-noise and unbiased density estimator based on (resampled) Delaunay Tessellations to calculate the density field; lensed images are produced by ray-tracing images of actual galaxies from deep Hubble Space Telescope observations. Other galaxies, similarly sampled, are added to fill in the light cone. The pipeline further adds cluster member galaxies and foreground stars into the lensed images. The entire image ensemble is then observed using a realistic point-spread function that includes appropriate detector artifacts for bright stars. Noise is further added, including such non-Gaussian elements as noise window-paning from mosaiced observations, residual bad pixels, and cosmic rays. The aim is to produce simulated images that appear identical—to the eye (expert or otherwise)—to real observations in various imaging surveys.}
}

@article{Collett_2015,
	doi = {10.1088/0004-637x/811/1/20},
	url = {https://doi.org/10.1088%2F0004-637x%2F811%2F1%2F20},
	year = 2015,
	month = {sep},
	publisher = {{IOP} Publishing},
	volume = {811},
	number = {1},
	pages = {20},
	author = {Thomas E. Collett},
	title = {{THE} {POPULATION} {OF} {GALAXY}{\textendash}{GALAXY} {STRONG} {LENSES} {IN} {FORTHCOMING} {OPTICAL} {IMAGING} {SURVEYS}},
	journal = {The Astrophysical Journal},
	abstract = {Ongoing and future imaging surveys represent significant improvements in depth, area, and seeing compared to current data sets. These improvements offer the opportunity to discover up to three orders of magnitude more galaxy–galaxy strong lenses than are currently known. In this work we forecast the number of lenses that will be discoverable in forthcoming surveys and simulate their properties. We generate a population of statistically realistic strong lenses and simulate observations of this population for the Dark Energy Survey (DES), the Large Synoptic Survey Telescope (LSST), and Euclid surveys. We verify our model against the galaxy-scale lens search of the Canada–France–Hawaii Telescope Legacy Survey, predicting 250 discoverable lenses compared to 220 found by Gavazzi et al. The predicted Einstein radius distribution is also remarkably similar to that found by Sonnenfeld et al. For future surveys we find that, assuming Poisson limited lens galaxy subtraction, searches of the DES, LSST, and Euclid data sets should discover 2400, 120000, and 170000 galaxy–galaxy strong lenses, respectively. Finders using blue-minus-red () difference imaging for lens subtraction can discover 1300 and 62000 lenses in DES and LSST. The uncertainties on the model are dominated by the high-redshift source population, which typically gives fractional errors on the discoverable lens number at the level of tens of percent. We find that doubling the signal-to-noise ratio required for a lens to be detectable approximately halves the number of detectable lenses in each survey, indicating the importance of understanding the selection function and the sensitivity of future lens finders in interpreting strong lens statistics. We make our population forecasting and simulated observation codes publicly available so that the selection function of strong lens finders can easily be calibrated.}
}

% NNPDF

@article{Forte:2002fg,
      author         = "Forte, Stefano and Garrido, Lluis and Latorre, Jose I.
                        and Piccione, Andrea",
      title          = "{Neural network parametrization of deep inelastic
                        structure functions}",
      journal        = "JHEP",
      volume         = "05",
      year           = "2002",
      pages          = "062",
      doi            = "10.1088/1126-6708/2002/05/062",
      eprint         = "hep-ph/0204232",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ph",
      reportNumber   = "GEF-TH-3-02, RM3-TH-02-01",
      SLACcitation   = "%%CITATION = HEP-PH/0204232;%%"
}

@article{Ball:2014uwa,
      author         = "Ball, Richard D. and others",
      title          = "{Parton distributions for the LHC Run II}",
      collaboration  = "NNPDF",
      journal        = "JHEP",
      volume         = "04",
      year           = "2015",
      pages          = "040",
      doi            = "10.1007/JHEP04(2015)040",
      eprint         = "1410.8849",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ph",
      reportNumber   = "EDINBURGH-2014-15, IFUM-1034-FT, CERN-PH-TH-2013-253,
                        OUTP-14-11P, CAVENDISH-HEP-14-11",
      SLACcitation   = "%%CITATION = ARXIV:1410.8849;%%"
}


% Tracking
@inproceedings{Farrell:2018cjr,
      author         = "Farrell, Steven and others",
      title          = "{Novel deep learning methods for track reconstruction}",
      booktitle      = "{4th International Workshop Connecting The Dots 2018
                        (CTD2018) Seattle, Washington, USA, March 20-22, 2018}",
      url            = "http://lss.fnal.gov/archive/2018/conf/fermilab-conf-18-598-cd.pdf",
      year           = "2018",
      eprint         = "1810.06111",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ex",
      reportNumber   = "FERMILAB-CONF-18-598-CD",
      SLACcitation   = "%%CITATION = ARXIV:1810.06111;%%"
}

%HL-LHC


@article{Apollinari:2017cqg,
      author         = "Apollinari, G. and Brüning, O. and Nakamoto, T. and
                        Rossi, Lucio",
      title          = "{High Luminosity Large Hadron Collider HL-LHC}",
      journal        = "CERN Yellow Report",
      year           = "2015",
      number         = "5",
      pages          = "1-19",
      doi            = "10.5170/CERN-2015-005.1",
      eprint         = "1705.08830",
      archivePrefix  = "arXiv",
      primaryClass   = "physics.acc-ph",
      reportNumber   = "FERMILAB-DESIGN-2017-02",
      SLACcitation   = "%%CITATION = ARXIV:1705.08830;%%"
}

@article{Albertsson:2018maf,
      author         = "Albertsson, Kim and others",
      title          = "{Machine Learning in High Energy Physics Community White
                        Paper}",
      booktitle      = "{Proceedings, 18th International Workshop on Advanced
                        Computing and Analysis Techniques in Physics Research
                        (ACAT 2017): Seattle, WA, USA, August 21-25, 2017}",
      journal        = "J. Phys. Conf. Ser.",
      volume         = "1085",
      year           = "2018",
      number         = "2",
      pages          = "022008",
      doi            = "10.1088/1742-6596/1085/2/022008",
      eprint         = "1807.02876",
      archivePrefix  = "arXiv",
      primaryClass   = "physics.comp-ph",
      reportNumber   = "FERMILAB-PUB-18-318-CD-DI-PPD",
      SLACcitation   = "%%CITATION = ARXIV:1807.02876;%%"
}

%GP
@article{Frate:2017mai,
      author         = "Frate, Meghan and Cranmer, Kyle and Kalia, Saarik and
                        Vandenberg-Rodes, Alexander and Whiteson, Daniel",
      title          = "{Modeling Smooth Backgrounds and Generic Localized
                        Signals with Gaussian Processes}",
      year           = "2017",
      eprint         = "1709.05681",
      archivePrefix  = "arXiv",
      primaryClass   = "physics.data-an",
      SLACcitation   = "%%CITATION = ARXIV:1709.05681;%%"
}

@article{Bozson:2018asz,
      author         = "Bozson, Adam and Cowan, Glen and Spanò, Francesco",
      title          = "{Unfolding with Gaussian Processes}",
      year           = "2018",
      eprint         = "1811.01242",
      archivePrefix  = "arXiv",
      primaryClass   = "physics.data-an",
      SLACcitation   = "%%CITATION = ARXIV:1811.01242;%%"
}

@inproceedings{duvenaud2013structure,
  title={Structure Discovery in Nonparametric Regression through Compositional Kernel Search},
  author={Duvenaud, David and Lloyd, James and Grosse, Roger and Tenenbaum, Joshua and Zoubin, Ghahramani},
  booktitle={International Conference on Machine Learning},
  eprint={1302.4922},
  pages={1166--1174},
  year={2013}
}

% Geometric

@article{Bronstein:2016thv,
      author         = "Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and
                        Szlam, Arthur and Vandergheynst, Pierre",
      title          = "{Geometric Deep Learning: Going beyond Euclidean data}",
      journal        = "IEEE Sig. Proc. Mag.",
      volume         = "34",
      year           = "2017",
      number         = "4",
      pages          = "18-42",
      doi            = "10.1109/MSP.2017.2693418",
      eprint         = "1611.08097",
      archivePrefix  = "arXiv",
      primaryClass   = "cs.CV",
      SLACcitation   = "%%CITATION = ARXIV:1611.08097;%%"
}

%Theory

@article{Hashimoto:2018ftp,
      author         = "Hashimoto, Koji and Sugishita, Sotaro and Tanaka, Akinori
                        and Tomiya, Akio",
      title          = "{Deep learning and the AdS/CFT correspondence}",
      journal        = "Phys. Rev.",
      volume         = "D98",
      year           = "2018",
      number         = "4",
      pages          = "046019",
      doi            = "10.1103/PhysRevD.98.046019",
      eprint         = "1802.08313",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-th",
      reportNumber   = "OU-HET-961, RIKEN-iTHEMS-Report-18",
      SLACcitation   = "%%CITATION = ARXIV:1802.08313;%%"
}

@article{Shanahan:2018vcv,
      author         = "Shanahan, Phiala E. and Trewartha, Daniel and Detmold,
                        William",
      title          = "{Machine learning action parameters in lattice quantum
                        chromodynamics}",
      journal        = "Phys. Rev.",
      volume         = "D97",
      year           = "2018",
      number         = "9",
      pages          = "094506",
      doi            = "10.1103/PhysRevD.97.094506",
      eprint         = "1801.05784",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-lat",
      reportNumber   = "MIT-CTP-4980, JLAB-THY-18-2627",
      SLACcitation   = "%%CITATION = ARXIV:1801.05784;%%"
}

@article{Carifio:2017bov,
      author         = "Carifio, Jonathan and Halverson, James and Krioukov,
                        Dmitri and Nelson, Brent D.",
      title          = "{Machine Learning in the String Landscape}",
      journal        = "JHEP",
      volume         = "09",
      year           = "2017",
      pages          = "157",
      doi            = "10.1007/JHEP09(2017)157",
      eprint         = "1707.00655",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-th",
      SLACcitation   = "%%CITATION = ARXIV:1707.00655;%%"
}

%ABC

@article{beaumont2002approximate,
  title={Approximate Bayesian computation in population genetics},
  author={Beaumont, Mark A and Zhang, Wenyang and Balding, David J},
  journal={Genetics},
  volume={162},
  number={4},
  pages={2025--2035},
  year={2002},
  publisher={Genetics Soc America}
}

@article{marjoram2003markov,
  title={Markov chain Monte Carlo without likelihoods},
  author={Marjoram, Paul and Molitor, John and Plagnol, Vincent and Tavar{\'e}, Simon},
  journal={Proceedings of the National Academy of Sciences},
  volume={100},
  number={26},
  pages={15324--15328},
  year={2003},
  publisher={National Acad Sciences}
}

@book{sisson2011likelihood,
  title={Likelihood-free MCMC},
  author={Sisson, Scott A and Fan, Yanan},
  year={2011},
  publisher={Chapman \& Hall/CRC, New York.[839]}
}

@article{sisson2007sequential,
  title={Sequential monte carlo without likelihoods},
  author={Sisson, Scott A and Fan, Yanan and Tanaka, Mark M},
  journal={Proceedings of the National Academy of Sciences},
  volume={104},
  number={6},
  pages={1760--1765},
  year={2007},
  publisher={National Acad Sciences}
}

@article{marin2012approximate,
  title={Approximate Bayesian computational methods},
  author={Marin, Jean-Michel and Pudlo, Pierre and Robert, Christian P and Ryder, Robin J},
  journal={Statistics and Computing},
  pages={1--14},
  year={2012},
  publisher={Springer Netherlands}
}


% HVM LFI
@inproceedings{Tran:2017jgo,
      author         = "Tran, Dustin and Ranganath, Rajesh and Blei, David M.",
      title          = "{Hierarchical Implicit Models and Likelihood-Free
                        Variational Inference}",
      year           = "2017",
      eprint         = "1702.08896",
      archivePrefix  = "arXiv",
      primaryClass   = "stat.ML",
      SLACcitation   = "%%CITATION = ARXIV:1702.08896;%%"
}

% symmetries

@inproceedings{cohen2016group,
  title={Group equivariant convolutional networks},
  author={Cohen, Taco and Welling, Max},
  booktitle={International conference on machine learning},
  pages={2990--2999},
  year={2016}
}

@article{cohen2018spherical,
  title={Spherical cnns},
  author={Cohen, Taco S and Geiger, Mario and K{\"o}hler, Jonas and Welling, Max},
  journal={Proceedings of the 6th International Conference on Learning Representations (ICLR)},
  note={arXiv preprint arXiv:1801.10130},
  year={2018}
}

@ARTICLE{2019arXiv190204615C,
       author = {{Cohen}, Taco S. and {Weiler}, Maurice and {Kicanaoglu}, Berkay and
         {Welling}, Max},
        title = "{Gauge Equivariant Convolutional Networks and the Icosahedral CNN}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
         year = "2019",
        month = "Feb",
archivePrefix = {arXiv},
       eprint = {1902.04615},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2019arXiv190204615C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018arXiv180609231K,
       author = {{Kondor}, Risi and {Lin}, Zhen and {Trivedi}, Shubhendu},
        title = "{Clebsch-Gordan Nets: a Fully Fourier Space Spherical Convolutional Neural Network}",
      journal = {NeurIPS 2018},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = "2018",
        month = "Jun",
          eid = {arXiv:1806.09231},
archivePrefix = {arXiv},
       eprint = {1806.09231},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2018arXiv180609231K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{2018arXiv180203690K,
  title={On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups},
  author={Kondor, Risi and Trivedi, Shubhendu},
  booktitle={International Conference on Machine Learning},
  eprint={1802.03690},
  pages={2747--2755},
  year={2018}
}

@article{reddy2018glider,
  title={Glider soaring via reinforcement learning in the field},
  author={Reddy, Gautam and Wong-Ng, Jerome and Celani, Antonio and Sejnowski, Terrence J and Vergassola, Massimo},
  journal={Nature},
  volume={562},
  number={7726},
  pages={236},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{Behler:2007kp,
author = {Behler, J{\"o}rg and Parrinello, Michele},
title = {{Generalized Neural-Network Representation of High-Dimensional Potential-Energy Surfaces}},
journal = {Phys. Rev. Lett.},
year = {2007},
volume = {98},
number = {14},
pages = {583},
month = apr
}

@article{Bartok:2018ih,
author = {Bart{\'o}k, Albert P and Kermode, James and Bernstein, Noam and Cs{\'a}nyi, G{\'a}bor},
title = {{PhysRevX.8.041048}},
journal = {Physical Review X},
year = {2018},
volume = {8},
number = {4},
pages = {041048},
month = dec
}



@article{stokes2019probabilistic,
  title={Probabilistic Modeling with Matrix Product States},
  author={Stokes, James and Terilla, John},
  journal={arXiv preprint arXiv:1902.06888},
  year={2019}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{reddy2016learning,
  title={Learning to soar in turbulent environments},
  author={Reddy, Gautam and Celani, Antonio and Sejnowski, Terrence J and Vergassola, Massimo},
  journal={Proceedings of the National Academy of Sciences},
  volume={113},
  number={33},
  pages={E4877--E4884},
  year={2016},
  publisher={National Acad Sciences}
}

@article{jaeger2004harnessing,
  title={Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication},
  author={Jaeger, Herbert and Haas, Harald},
  journal={science},
  volume={304},
  number={5667},
  pages={78--80},
  year={2004},
  publisher={American Association for the Advancement of Science}
}

@article{pathak2017using,
  title={Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data},
  author={Pathak, Jaideep and Lu, Zhixin and Hunt, Brian R and Girvan, Michelle and Ott, Edward},
  journal={Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume={27},
  number={12},
  pages={121102},
  year={2017},
  publisher={AIP Publishing}
}

@article{pathak2018model,
  title={Model-free prediction of large spatiotemporally chaotic systems from data: A reservoir computing approach},
  author={Pathak, Jaideep and Hunt, Brian and Girvan, Michelle and Lu, Zhixin and Ott, Edward},
  journal={Physical review letters},
  volume={120},
  number={2},
  pages={024102},
  year={2018},
  publisher={APS}
}

@article{DBLP:journals/corr/abs-1803-01588,
  author    = {Risi Kondor},
  title     = {N-body Networks: a Covariant Hierarchical Neural Network Architecture
               for Learning Atomic Potentials},
  journal   = {CoRR},
  volume    = {abs/1803.01588},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.01588},
  archivePrefix = {arXiv},
  eprint    = {1803.01588},
  timestamp = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1803-01588},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%FInal refs

@article{Shen:2019ohi,
      author         = "Shen, Hongyu and George, Daniel and Huerta, E. A. and
                        Zhao, Zhizhen",
      title          = "{Denoising Gravitational Waves with Enhanced Deep
                        Recurrent Denoising Auto-Encoders}",
      year           = "2019",
      eprint         = "1903.03105",
      archivePrefix  = "arXiv",
      primaryClass   = "astro-ph.CO",
      SLACcitation   = "%%CITATION = ARXIV:1903.03105;%%"
}
@article{DiSipio:2019imz,
      author         = "Di Sipio, Riccardo and Faucci Giannelli, Michele and
                        Ketabchi Haghighat, Sana and Palazzo, Serena",
      title          = "{DijetGAN: A Generative-Adversarial Network Approach for
                        the Simulation of QCD Dijet Events at the LHC}",
      year           = "2019",
      eprint         = "1903.02433",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ex",
      SLACcitation   = "%%CITATION = ARXIV:1903.02433;%%"
}

@article{LaPlante:2018pst,
      author         = "La Plante, Paul and Ntampaka, Michelle",
      title          = "{Machine Learning Applied to the Reionization History of
                        the Universe}",
      year           = "2018",
      eprint         = "1810.08211",
      archivePrefix  = "arXiv",
      primaryClass   = "astro-ph.CO",
      SLACcitation   = "%%CITATION = ARXIV:1810.08211;%%"
}

@article{George:2016hay,
      author         = "George, Daniel and Huerta, E. A.",
      title          = "{Deep Neural Networks to Enable Real-time Multimessenger
                        Astrophysics}",
      journal        = "Phys. Rev.",
      volume         = "D97",
      year           = "2018",
      number         = "4",
      pages          = "044039",
      doi            = "10.1103/PhysRevD.97.044039",
      eprint         = "1701.00008",
      archivePrefix  = "arXiv",
      primaryClass   = "astro-ph.IM",
      SLACcitation   = "%%CITATION = ARXIV:1701.00008;%%"
}

% Another LFI
@article{Alsing:2019dvb,
      author         = "Alsing, Justin and Wandelt, Benjamin",
      title          = "{Nuisance hardened data compression for fast
                        likelihood-free inference}",
      year           = "2019",
      eprint         = "1903.01473",
      archivePrefix  = "arXiv",
      primaryClass   = "astro-ph.CO",
      SLACcitation   = "%%CITATION = ARXIV:1903.01473;%%"
}



@article{George:2016hay,
      author         = "George, Daniel and Huerta, E. A.",
      title          = "{Deep Neural Networks to Enable Real-time Multimessenger
                        Astrophysics}",
      journal        = "Phys. Rev.",
      volume         = "D97",
      year           = "2018",
      number         = "4",
      pages          = "044039",
      doi            = "10.1103/PhysRevD.97.044039",
      eprint         = "1701.00008",
      archivePrefix  = "arXiv",
      primaryClass   = "astro-ph.IM",
      SLACcitation   = "%%CITATION = ARXIV:1701.00008;%%"
}

@ARTICLE{2019arXiv190304057H,
       author = {{Hermans}, Joeri and {Begy}, Volodimir and {Louppe}, Gilles},
        title = "{Likelihood-free MCMC with Approximate Likelihood Ratios}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = "2019",
        month = "Mar",
archivePrefix = {arXiv},
       eprint = {1903.04057},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2019arXiv190304057H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018arXiv180507226P,
       author = {{Papamakarios}, George and {Sterratt}, David C. and {Murray}, Iain},
        title = "{Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = "2018",
        month = "May",
          eid = {arXiv:1805.07226},
        pages = {arXiv:1805.07226},
archivePrefix = {arXiv},
       eprint = {1805.07226},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2018arXiv180507226P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014A&A...569A..13R,
       author = {{Robin}, A.~C. and {Reyl{\'e}}, C. and {Fliri}, J. and {Czekaj}, M. and
         {Robert}, C.~P. and {Martins}, A.~M.~M.},
        title = "{Constraining the thick disc formation scenario of the Milky Way}",
      journal = {Astronomy and Astrophysics},
     keywords = {Galaxy: disk, Galaxy: evolution, Galaxy: structure, Galaxy: halo, Galaxy: formation, Galaxy: stellar content, Astrophysics - Astrophysics of Galaxies},
         year = "2014",
        month = "Sep",
       volume = {569},
          eid = {A13},
        pages = {A13},
          doi = {10.1051/0004-6361/201423415},
archivePrefix = {arXiv},
       eprint = {1406.5384},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2014A&A...569A..13R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}




@ARTICLE{2012MNRAS.425...44C,
       author = {{Cameron}, E. and {Pettitt}, A.~N.},
        title = "{Approximate Bayesian Computation for astronomical model analysis: a case study in galaxy demographics and morphological transformation at high redshift}",
      journal = {MNRAS},
     keywords = {methods: statistical, galaxies: evolution, galaxies: formation, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2012",
        month = "Sep",
       volume = {425},
        pages = {44-65},
          doi = {10.1111/j.1365-2966.2012.21371.x},
archivePrefix = {arXiv},
       eprint = {1202.1426},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2012MNRAS.425...44C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{2015AC....13....1I,
       author = {{Ishida}, E.~E.~O. and {Vitenti}, S.~D.~P. and {Penna-Lima}, M. and
         {Cisewski}, J. and {de Souza}, R.~S. and {Trindade}, A.~M.~M. and
         {Cameron}, E. and {Busti}, V.~C. and {COIN Collaboration}},
        title = "{COSMOABC: Likelihood-free inference via Population Monte Carlo Approximate Bayesian Computation}",
      journal = {Astronomy and Computing},
     keywords = {Galaxies: statistics, (cosmology:) large-scale structure of universe, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2015",
        month = "Nov",
       volume = {13},
        pages = {1-11},
          doi = {10.1016/j.ascom.2015.09.001},
archivePrefix = {arXiv},
       eprint = {1504.06129},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2015A&C....13....1I},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Ilten:2016csi,
      author         = "Ilten, Philip and Williams, Mike and Yang, Yunjie",
      title          = "{Event generator tuning using Bayesian optimization}",
      journal        = "JINST",
      volume         = "12",
      year           = "2017",
      number         = "04",
      pages          = "P04028",
      doi            = "10.1088/1748-0221/12/04/P04028",
      eprint         = "1610.08328",
      archivePrefix  = "arXiv",
      primaryClass   = "physics.data-an",
      SLACcitation   = "%%CITATION = ARXIV:1610.08328;%%"
}

%other VAE paper
@inproceedings{2014arXiv1401.4082J,
  title={Stochastic backpropagation and approximate inference in deep generative models},
  author={Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  booktitle={Proceedings of the 31st International Conference on International Conference on Machine Learning-Volume 32},
  pages={II--1278},
  year={2014},
  organization={JMLR. org}
}
     
@article{Bonnett:2015pww,
      author         = "Bonnett, C. and others",
      title          = "{Redshift distributions of galaxies in the Dark Energy
                        Survey Science Verification shear catalogue and
                        implications for weak lensing}",
      collaboration  = "DES",
      journal        = "Phys. Rev.",
      volume         = "D94",
      year           = "2016",
      number         = "4",
      pages          = "042005",
      doi            = "10.1103/PhysRevD.94.042005",
      eprint         = "1507.05909",
      archivePrefix  = "arXiv",
      primaryClass   = "astro-ph.CO",
      reportNumber   = "FERMILAB-PUB-15-306",
      SLACcitation   = "%%CITATION = ARXIV:1507.05909;%%"
}

@article{Tsaris:2018myg,
      author         = "Tsaris, Aristeidis and others",
      title          = "{The HEP.TrkX Project: Deep Learning for Particle
                        Tracking}",
      booktitle      = "{Proceedings, 18th International Workshop on Advanced
                        Computing and Analysis Techniques in Physics Research
                        (ACAT 2017): Seattle, WA, USA, August 21-25, 2017}",
      journal        = "J. Phys. Conf. Ser.",
      volume         = "1085",
      year           = "2018",
      number         = "4",
      pages          = "042023",
      doi            = "10.1088/1742-6596/1085/4/042023",
      reportNumber   = "FERMILAB-CONF-18-596-CD",
      SLACcitation   = "%%CITATION = 00462,1085,042023;%%"
}


@article{Aaij:2012me,
      author         = "Aaij, R and others",
      title          = "{The LHCb Trigger and its Performance in 2011}",
      journal        = "JINST",
      volume         = "8",
      year           = "2013",
      pages          = "P04022",
      doi            = "10.1088/1748-0221/8/04/P04022",
      eprint         = "1211.3055",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ex",
      SLACcitation   = "%%CITATION = ARXIV:1211.3055;%%"
}

@article{Gligorov:2012qt,
      author         = "Gligorov, V. V. and Williams, Mike",
      title          = "{Efficient, reliable and fast high-level triggering using
                        a bonsai boosted decision tree}",
      journal        = "JINST",
      volume         = "8",
      year           = "2013",
      pages          = "P02013",
      doi            = "10.1088/1748-0221/8/02/P02013",
      eprint         = "1210.6861",
      archivePrefix  = "arXiv",
      primaryClass   = "physics.ins-det",
      SLACcitation   = "%%CITATION = ARXIV:1210.6861;%%"
}

@article{Likhomanenko:2015aba,
      author         = "Likhomanenko, Tatiana and Ilten, Philip and Khairullin,
                        Egor and Rogozhnikov, Alex and Ustyuzhanin, Andrey and
                        Williams, Michael",
      title          = "{LHCb Topological Trigger Reoptimization}",
      booktitle      = "{Proceedings, 21st International Conference on Computing
                        in High Energy and Nuclear Physics (CHEP 2015): Okinawa,
                        Japan, April 13-17, 2015}",
      journal        = "J. Phys. Conf. Ser.",
      volume         = "664",
      year           = "2015",
      number         = "8",
      pages          = "082025",
      doi            = "10.1088/1742-6596/664/8/082025",
      eprint         = "1510.00572",
      archivePrefix  = "arXiv",
      primaryClass   = "physics.ins-det",
      SLACcitation   = "%%CITATION = ARXIV:1510.00572;%%"
}

@article{Ntampaka:2019udw,
      author         = "Ntampaka, Michelle and others",
      title          = "{The Role of Machine Learning in the Next Decade of
                        Cosmology}",
      year           = "2019",
      eprint         = "1902.10159",
      archivePrefix  = "arXiv",
      primaryClass   = "astro-ph.IM",
      reportNumber   = "FERMILAB-PUB-19-088-CD",
      SLACcitation   = "%%CITATION = ARXIV:1902.10159;%%"
}

@article{Ho:2019zap,
      author         = "Ho, Matthew and Rau, Markus Michael and Ntampaka,
                        Michelle and Farahi, Arya and Trac, Hy and Poczos,
                        Barnabas",
      title          = "{A Robust and Efficient Deep Learning Method for
                        Dynamical Mass Measurements of Galaxy Clusters}",
      year           = "2019",
      eprint         = "1902.05950",
      archivePrefix  = "arXiv",
      primaryClass   = "astro-ph.CO",
      SLACcitation   = "%%CITATION = ARXIV:1902.05950;%%"
}

@article{DBLP:journals/corr/Schmidhuber14,
  author    = {J{\"{u}}rgen Schmidhuber},
  title     = {Deep Learning in Neural Networks: An Overview},
  journal   = {CoRR},
  volume    = {abs/1404.7828},
  year      = {2014},
  url       = {http://arxiv.org/abs/1404.7828},
  archivePrefix = {arXiv},
  eprint    = {1404.7828},
  timestamp = {Mon, 13 Aug 2018 16:47:28 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/Schmidhuber14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{putzky2017recurrent,
  title={Recurrent inference machines for solving inverse problems},
  author={Putzky, Patrick and Welling, Max},
  journal={arXiv preprint arXiv:1706.04008},
  year={2017}
}


@article{schawinski_generative_2017,
	title = {Generative {Adversarial} {Networks} recover features in astrophysical images of galaxies beyond the deconvolution limit},
	issn = {1745-3925, 1745-3933},
	url = {http://arxiv.org/abs/1702.00403},
	doi = {10.1093/mnrasl/slx008},
	abstract = {Observations of astrophysical objects such as galaxies are limited by various sources of random and systematic noise from the sky background, the optical system of the telescope and the detector used to record the data. Conventional deconvolution techniques are limited in their ability to recover features in imaging data by the Shannon-Nyquist sampling theorem. Here we train a generative adversarial network (GAN) on a sample of \$4,550\$ images of nearby galaxies at \$0.01{\textless}z{\textless}0.02\$ from the Sloan Digital Sky Survey and conduct \$10{\textbackslash}times\$ cross validation to evaluate the results. We present a method using a GAN trained on galaxy images that can recover features from artificially degraded images with worse seeing and higher noise than the original with a performance which far exceeds simple deconvolution. The ability to better recover detailed features such as galaxy morphology from low-signal-to-noise and low angular resolution imaging data significantly increases our ability to study existing data sets of astrophysical objects as well as future observations with observatories such as the Large Synoptic Sky Telescope (LSST) and the Hubble and James Webb space telescopes.},
	urldate = {2019-03-22},
	journal = {Monthly Notices of the Royal Astronomical Society: Letters},
	author = {Schawinski, Kevin and Zhang, Ce and Zhang, Hantian and Fowler, Lucas and Santhanam, Gokula Krishnan},
	month = jan,
	year = {2017},
	note = {arXiv: 1702.00403},
	keywords = {Astrophysics - Astrophysics of Galaxies, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {slx008},
	annote = {Comment: Accepted for publication in MNRAS, for the full code and a virtual machine set up to run it, see http://space.ml/proj/GalaxyGAN.html},
	file = {arXiv\:1702.00403 PDF:/Users/giuscarl/Zotero/storage/C5WW97A9/Schawinski et al. - 2017 - Generative Adversarial Networks recover features i.pdf:application/pdf;arXiv.org Snapshot:/Users/giuscarl/Zotero/storage/DKVLCAXW/1702.html:text/html}
}

@article{marshall2009automated,
  title={Automated detection of galaxy-scale gravitational lenses in high-resolution imaging data},
  author={Marshall, Philip J and Hogg, David W and Moustakas, Leonidas A and Fassnacht, Christopher D and Brada{\v{c}}, Maru{\v{s}}a and Schrabback, Tim and Blandford, Roger D},
  journal={The Astrophysical Journal},
  volume={694},
  number={2},
  pages={924},
  year={2009},
  publisher={IOP Publishing}
}

@ARTICLE{2007ApJ...660.1176E,
   author = {{Estrada}, J. and {Annis}, J. and {Diehl}, H.~T. and {Hall}, P.~B. and 
	{Las}, T. and {Lin}, H. and {Makler}, M. and {Merritt}, K.~W. and 
	{Scarpine}, V. and {Allam}, S. and {Tucker}, D.},
    title = "{A Systematic Search for High Surface Brightness Giant Arcs in a Sloan Digital Sky Survey Cluster Sample}",
  journal = {\apj},
   eprint = {astro-ph/0701383},
 keywords = {Galaxies: Clusters: General, Cosmology: Gravitational Lensing, Surveys},
     year = 2007,
    month = may,
   volume = 660,
    pages = {1176-1185},
      doi = {10.1086/512599},
   adsurl = {http://adsabs.harvard.edu/abs/2007ApJ...660.1176E},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019arXiv190205965Z,
       author = {{Zhang}, Xinyue and {Wang}, Yanfang and {Zhang}, Wei and {Sun}, Yueqiu and
         {He}, Siyu and {Contardo}, Gabriella and
         {Villaescusa-Navarro}, Francisco and {Ho}, Shirley},
        title = "{From Dark Matter to Galaxies with Convolutional Networks}",
      journal = {arXiv e-prints},
     keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, Computer Science - Machine Learning},
         year = "2019",
        month = "Feb",
          eid = {arXiv:1902.05965},
        pages = {arXiv:1902.05965},
archivePrefix = {arXiv},
       eprint = {1902.05965},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2019arXiv190205965Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018arXiv180800011M,
       author = {{Morningstar}, Warren R. and {Hezaveh}, Yashar D. and
         {Perreault Levasseur}, Laurence and {Blandford}, Roger D. and
         {Marshall}, Philip J. and {Putzky}, Patrick and {Wechsler}, Risa H.},
        title = "{Analyzing interferometric observations of strong gravitational lenses with recurrent and convolutional neural networks}",
      journal = {arXiv e-prints},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2018",
        month = "Jul",
          eid = {arXiv:1808.00011},
        pages = {arXiv:1808.00011},
archivePrefix = {arXiv},
       eprint = {1808.00011},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2018arXiv180800011M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Hezaveh:2017sht,
      author         = "Hezaveh, Yashar D. and Perreault Levasseur, Laurence and
                        Marshall, Philip J.",
      title          = "{Fast Automated Analysis of Strong Gravitational Lenses
                        with Convolutional Neural Networks}",
      journal        = "Nature",
      volume         = "548",
      year           = "2017",
      pages          = "555-557",
      doi            = "10.1038/nature23463",
      eprint         = "1708.08842",
      archivePrefix  = "arXiv",
      primaryClass   = "astro-ph.IM",
      SLACcitation   = "%%CITATION = ARXIV:1708.08842;%%"
}

%%%%%%%
% END KYLE
%%%%%%%

@article{carleo_solving_2017,
	title = {Solving the quantum many-body problem with artificial neural networks},
	volume = {355},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/355/6325/602},
	doi = {10.1126/science.aag2302},
	abstract = {Machine learning and quantum physics
Elucidating the behavior of quantum interacting systems of many particles remains one of the biggest challenges in physics. Traditional numerical methods often work well, but some of the most interesting problems leave them stumped. Carleo and Troyer harnessed the power of machine learning to develop a variational approach to the quantum many-body problem (see the Perspective by Hush). The method performed at least as well as state-of-the-art approaches, setting a benchmark for a prototypical two-dimensional problem. With further development, it may well prove a valuable piece in the quantum toolbox.
Science, this issue p. 602; see also p. 580
The challenge posed by the many-body problem in quantum physics originates from the difficulty of describing the nontrivial correlations encoded in the exponential complexity of the many-body wave function. Here we demonstrate that systematic machine learning of the wave function can reduce this complexity to a tractable computational form for some notable cases of physical interest. We introduce a variational representation of quantum states based on artificial neural networks with a variable number of hidden neurons. A reinforcement-learning scheme we demonstrate is capable of both finding the ground state and describing the unitary time evolution of complex interacting quantum systems. Our approach achieves high accuracy in describing prototypical interacting spins models in one and two dimensions.
A machine-learning approach sets a computational benchmark for a prototypical two-dimensional problem.
A machine-learning approach sets a computational benchmark for a prototypical two-dimensional problem.},
	language = {en},
	number = {6325},
	urldate = {2017-02-13},
	journal = {Science},
	author = {Carleo, Giuseppe and Troyer, Matthias},
	month = feb,
	year = {2017},
	pmid = {28183973},
	pages = {602--606},
	file = {Snapshot:/Users/gcarleo/Zotero/storage/AWWFI2Z2/Carleo and Troyer - 2017 - Solving the quantum many-body problem with artific.html:text/html}
}

@article{kalantre2019machine,
  title={Machine learning techniques for state recognition and auto-tuning in quantum dots},
  author={Kalantre, Sandesh S and Zwolak, Justyna P and Ragole, Stephen and Wu, Xingyao and Zimmerman, Neil M and Stewart, MD and Taylor, Jacob M},
  journal={npj Quantum Information},
  volume={5},
  number={1},
  pages={6},
  year={2019},
  publisher={Nature Publishing Group}
}



@article{hartmann_neural-network_2019,
	title = {Neural-{Network} {Approach} to {Dissipative} {Quantum} {Many}-{Body} {Dynamics}},
	url = {http://arxiv.org/abs/1902.05131},
	journal = {arXiv:1902.05131 [cond-mat, physics:quant-ph]},
	author = {Hartmann, Michael J. and Carleo, Giuseppe},
	month = feb,
	year = {2019}
}


@article{vicentini_variational_2019,
	title = {Variational neural network ansatz for steady states in open quantum systems},
	url = {http://arxiv.org/abs/1902.10104},
	journal = {arXiv:1902.10104 [cond-mat, physics:quant-ph]},
	author = {Vicentini, Filippo and Biella, Alberto and Regnault, Nicolas and Ciuti, Cristiano},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.10104},
}

@article{nagy_variational_2019,
	title = {Variational quantum {Monte} {Carlo} with neural network ansatz for open quantum systems},
	url = {http://arxiv.org/abs/1902.09483},
	urldate = {2019-02-28},
	journal = {arXiv:1902.09483 [cond-mat, physics:quant-ph]},
	author = {Nagy, Alexandra and Savona, Vincenzo},
	month = feb,
	year = {2019},
}


@article{cheng_tree_2019,
	title = {Tree {Tensor} {Networks} for {Generative} {Modeling}},
	urldate = {2019-03-14},
	journal = {arXiv:1901.02217 [cond-mat, physics:quant-ph, stat]},
	author = {Cheng, Song and Wang, Lei and Xiang, Tao and Zhang, Pan},
	month = jan,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Statistical Mechanics, Quantum Physics, Statistics - Machine Learning},
	file = {arXiv\:1901.02217 PDF:/Users/gcarleo/Zotero/storage/4B332CFR/Cheng et al. - 2019 - Tree Tensor Networks for Generative Modeling.pdf:application/pdf;arXiv.org Snapshot:/Users/gcarleo/Zotero/storage/TWUCKVEL/1901.html:text/html}
}


@article{guo_matrix_2018,
	title = {Matrix product operators for sequence-to-sequence learning},
	volume = {98},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.98.042114},
	doi = {10.1103/PhysRevE.98.042114},
	abstract = {The method of choice to study one-dimensional strongly interacting many-body quantum systems is based on matrix product states and operators. Such a method allows one to explore the most relevant and numerically manageable portion of an exponentially large space. It also allows one to accurately describe correlations between distant parts of a system, an important ingredient to account for the context in machine learning tasks. Here we introduce a machine learning model in which matrix product operators are trained to implement sequence-to-sequence prediction, i.e., given a sequence at a time step, it allows one to predict the next sequence. We then apply our algorithm to cellular automata (for which we show exact analytical solutions in terms of matrix product operators) and to nonlinear coupled maps. We show advantages of the proposed algorithm when compared to conditional random fields and a bidirectional, long, short-term-memory neural network. To highlight the flexibility of the algorithm, we also show that it can readily perform classification tasks.},
	number = {4},
	urldate = {2019-03-14},
	journal = {Physical Review E},
	author = {Guo, Chu and Jie, Zhanming and Lu, Wei and Poletti, Dario},
	month = oct,
	year = {2018},
	pages = {042114},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/6C32UL4X/PhysRevE.98.html:text/html;Full Text PDF:/Users/gcarleo/Zotero/storage/W7Z2K3B2/Guo et al. - 2018 - Matrix product operators for sequence-to-sequence .pdf:application/pdf}
}


@article{liu_entanglement-guided_2018,
	title = {Entanglement-guided architectures of machine learning by quantum tensor network},
	url = {http://arxiv.org/abs/1803.09111},
	abstract = {It is a fundamental, but still elusive question whether the schemes based on quantum mechanics, in particular on quantum entanglement, can be used for classical information processing and machine learning. Even partial answer to this question would bring important insights to both fields of machine learning and quantum mechanics. In this work, we implement simple numerical experiments, related to pattern/images classification, in which we represent the classifiers by many-qubit quantum states written in the matrix product states (MPS). Classical machine learning algorithm is applied to these quantum states to learn the classical data. We explicitly show how quantum entanglement (i.e., single-site and bipartite entanglement) can emerge in such represented images. Entanglement characterizes here the importance of data, and such information are practically used to guide the architecture of MPS, and improve the efficiency. The number of needed qubits can be reduced to less than 1/10 of the original number, which is within the access of the state-of-the-art quantum computers. We expect such numerical experiments could open new paths in charactering classical machine learning algorithms, and at the same time shed lights on the generic quantum simulations/computations of machine learning tasks.},
	urldate = {2019-03-14},
	journal = {arXiv:1803.09111 [cond-mat, physics:quant-ph, stat]},
	author = {Liu, Yuhan and Zhang, Xiao and Lewenstein, Maciej and Ran, Shi-Ju},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.09111},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Strongly Correlated Electrons, Quantum Physics, Statistics - Machine Learning},
	annote = {Comment: 10 pages, 5 figures},
	file = {arXiv\:1803.09111 PDF:/Users/gcarleo/Zotero/storage/9WID79FN/Liu et al. - 2018 - Entanglement-guided architectures of machine learn.pdf:application/pdf;arXiv.org Snapshot:/Users/gcarleo/Zotero/storage/RXV9YGVF/1803.html:text/html}
}


@article{liu_machine_2017,
	title = {Machine {Learning} by {Unitary} {Tensor} {Network} of {Hierarchical} {Tree} {Structure}},
	url = {http://arxiv.org/abs/1710.04833},
	abstract = {The resemblance between the methods used in quantum-many body physics and in machine learning has drawn considerable attention. In particular, tensor networks (TNs) and deep learning architectures bear striking similarities to the extent that TNs can be used for machine learning. Previous results used one-dimensional TNs in image recognition, showing limited scalability and flexibilities. In this work, we train two-dimensional hierarchical TNs to solve image recognition problems, using a training algorithm derived from the multi-scale entanglement renormalization ansatz. This approach introduces mathematical connections among quantum many-body physics, quantum information theory, and machine learning. While keeping the TN unitary in the training phase, TN states are defined, which encode classes of images into quantum many-body states. We study the quantum features of the TN states, including quantum entanglement and fidelity. We find these quantities could be properties that characterize the image classes, as well as the machine learning tasks.},
	urldate = {2019-03-14},
	journal = {arXiv:1710.04833 [cond-mat, physics:physics, physics:quant-ph, stat]},
	author = {Liu, Ding and Ran, Shi-Ju and Wittek, Peter and Peng, Cheng and García, Raul Blázquez and Su, Gang and Lewenstein, Maciej},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.04833},
	keywords = {Condensed Matter - Strongly Correlated Electrons, Physics - Computational Physics, Quantum Physics, Statistics - Machine Learning},
	annote = {Comment: 6 pages, 4 figures},
	file = {arXiv\:1710.04833 PDF:/Users/gcarleo/Zotero/storage/TZNRIDX6/Liu et al. - 2017 - Machine Learning by Unitary Tensor Network of Hier.pdf:application/pdf;arXiv.org Snapshot:/Users/gcarleo/Zotero/storage/7U57VRCP/1710.html:text/html}
}


@article{shi_classical_2006,
	title = {Classical simulation of quantum many-body systems with a tree tensor network},
	volume = {74},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.74.022320},
	doi = {10.1103/PhysRevA.74.022320},
	abstract = {We show how to efficiently simulate a quantum many-body system with tree structure when its entanglement (Schmidt number) is small for any bipartite split along an edge of the tree. As an application, we show that any one-way quantum computation on a tree graph can be efficiently simulated with a classical computer.},
	number = {2},
	urldate = {2019-03-14},
	journal = {Physical Review A},
	author = {Shi, Y.-Y. and Duan, L.-M. and Vidal, G.},
	month = aug,
	year = {2006},
	pages = {022320},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/SU2DKNIY/PhysRevA.74.html:text/html;Full Text PDF:/Users/gcarleo/Zotero/storage/WXCPTDTE/Shi et al. - 2006 - Classical simulation of quantum many-body systems .pdf:application/pdf}
}


@article{hackbusch_new_2009,
	title = {A {New} {Scheme} for the {Tensor} {Representation}},
	volume = {15},
	issn = {1531-5851},
	url = {https://doi.org/10.1007/s00041-009-9094-9},
	doi = {10.1007/s00041-009-9094-9},
	abstract = {The paper presents a new scheme for the representation of tensors which is well-suited for high-order tensors. The construction is based on a hierarchy of tensor product subspaces spanned by orthonormal bases. The underlying binary tree structure makes it possible to apply standard Linear Algebra tools for performing arithmetical operations and for the computation of data-sparse approximations. In particular, a truncation algorithm can be implemented which is based on the standard matrix singular value decomposition (SVD) method.},
	language = {en},
	number = {5},
	urldate = {2019-03-14},
	journal = {Journal of Fourier Analysis and Applications},
	author = {Hackbusch, W. and Kühn, S.},
	month = oct,
	year = {2009},
	keywords = {15A69, Multilinear algebra, Singular value decomposition, Tensor representation},
	pages = {706--722},
	file = {Springer Full Text PDF:/Users/gcarleo/Zotero/storage/LCIEVYPJ/Hackbusch and Kühn - 2009 - A New Scheme for the Tensor Representation.pdf:application/pdf}
}

@article{venderley_machine_2018,
	title = {Machine {Learning} {Out}-of-{Equilibrium} {Phases} of {Matter}},
	volume = {120},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.120.257204},
	doi = {10.1103/PhysRevLett.120.257204},
	abstract = {Neural-network-based machine learning is emerging as a powerful tool for obtaining phase diagrams when traditional regression schemes using local equilibrium order parameters are not available, as in many-body localized (MBL) or topological phases. Nevertheless, instances of machine learning offering new insights have been rare up to now. Here we show that a single feed-forward neural network can decode the defining structures of two distinct MBL phases and a thermalizing phase, using entanglement spectra obtained from individual eigenstates. For this, we introduce a simplicial geometry-based method for extracting multipartite phase boundaries. We find that this method outperforms conventional metrics for identifying MBL phase transitions, revealing a sharper phase boundary and shedding new insight on the topology of the phase diagram. Furthermore, the phase diagram we acquire from a single disorder configuration confirms that the machine-learning-based approach we establish here can enable speedy exploration of large phase spaces that can assist with the discovery of new MBL phases. To our knowledge, this Letter represents the first example of a standard machine learning approach revealing new information on phase transitions.},
	number = {25},
	urldate = {2019-03-14},
	journal = {Physical Review Letters},
	author = {Venderley, Jordan and Khemani, Vedika and Kim, Eun-Ah},
	month = jun,
	year = {2018},
	pages = {257204},
}


@article{beach_machine_2018,
	title = {Machine learning vortices at the {Kosterlitz}-{Thouless} transition},
	volume = {97},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.97.045207},
	doi = {10.1103/PhysRevB.97.045207},
	number = {4},
	urldate = {2019-03-14},
	journal = {Physical Review B},
	author = {Beach, Matthew J. S. and Golubeva, Anna and Melko, Roger G.},
	month = jan,
	year = {2018},
	pages = {045207},
}
@article{alet_many-body_2018,
	series = {Quantum simulation / {Simulation} quantique},
	title = {Many-body localization: {An} introduction and selected topics},
	volume = {19},
	issn = {1631-0705},
	shorttitle = {Many-body localization},
	url = {http://www.sciencedirect.com/science/article/pii/S163107051830032X},
	doi = {10.1016/j.crhy.2018.03.003},
	number = {6},
	urldate = {2019-03-14},
	journal = {Comptes Rendus Physique},
	author = {Alet, Fabien and Laflorencie, Nicolas},
	month = sep,
	year = {2018},
	pages = {498--525}
}

@article{yoshioka_constructing_2019,
	title = {Constructing neural stationary states for open quantum many-body systems},
	url = {http://arxiv.org/abs/1902.07006},
	journal = {arXiv:1902.07006 [cond-mat, physics:quant-ph]},
	author = {Yoshioka, Nobuyuki and Hamazaki, Ryusuke},
	month = feb,
	year = {2019},
}


@article{dirac_note_1930,
	title = {Note on {Exchange} {Phenomena} in the {Thomas} {Atom}},
	volume = {26},
	issn = {1469-8064},
	doi = {10.1017/S0305004100016108},
	number = {03},
	journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
	author = {Dirac, Paul A. M.},
	month = jul,
	year = {1930},
	pages = {376--385},
}


@book{frenkel_wave_1934,
	series = {The {International} series of monographs on nuclear energy: {Reactor} design physics},
	title = {Wave {Mechanics}: {Advanced} {General} {Theory}},
	number = {v. 2},
	publisher = {The Clarendon Press},
	author = {Frenkel, Yakov I.},
	year = {1934},
	lccn = {lc34016678}
}


@article{aaronson_learnability_2007,
	title = {The learnability of quantum states},
	volume = {463},
	issn = {1364-5021, 1471-2946},
	url = {http://rspa.royalsocietypublishing.org/content/463/2088/3089},
	doi = {10.1098/rspa.2007.0113},
	language = {en},
	number = {2088},
	urldate = {2017-01-21},
	journal = {Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
	author = {Aaronson, Scott},
	month = dec,
	year = {2007},
	pages = {3089--3114},
}


@article{sharir_deep_2019,
	title = {Deep autoregressive models for the efficient variational simulation of many-body quantum systems},
	url = {http://arxiv.org/abs/1902.04057},
	journal = {arXiv:1902.04057 [cond-mat]},
	author = {Sharir, Or and Levine, Yoav and Wies, Noam and Carleo, Giuseppe and Shashua, Amnon},
	month = feb,
	year = {2019},
}


@article{sun_deep_2018,
	title = {Deep learning topological invariants of band insulators},
	volume = {98},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.98.085402},
	doi = {10.1103/PhysRevB.98.085402},
	number = {8},
	urldate = {2019-03-14},
	journal = {Physical Review B},
	author = {Sun, Ning and Yi, Jinmin and Zhang, Pengfei and Shen, Huitao and Zhai, Hui},
	month = aug,
	year = {2018},
	pages = {085402}
}


@article{ohtsuki_deep_2016,
	title = {Deep {Learning} the {Quantum} {Phase} {Transitions} in {Random} {Two}-{Dimensional} {Electron} {Systems}},
	volume = {85},
	issn = {0031-9015},
	url = {https://journals.jps.jp/doi/10.7566/JPSJ.85.123706},
	doi = {10.7566/JPSJ.85.123706},
	number = {12},
	urldate = {2019-03-14},
	journal = {Journal of the Physical Society of Japan},
	author = {Ohtsuki, Tomoki and Ohtsuki, Tomi},
	month = nov,
	year = {2016},
	pages = {123706},
}


@article{ohtsuki_deep_2017,
	title = {Deep {Learning} the {Quantum} {Phase} {Transitions} in {Random} {Electron} {Systems}: {Applications} to {Three} {Dimensions}},
	volume = {86},
	issn = {0031-9015},
	shorttitle = {Deep {Learning} the {Quantum} {Phase} {Transitions} in {Random} {Electron} {Systems}},
	url = {https://journals.jps.jp/doi/10.7566/JPSJ.86.044708},
	doi = {10.7566/JPSJ.86.044708},
	number = {4},
	urldate = {2019-03-14},
	journal = {Journal of the Physical Society of Japan},
	author = {Ohtsuki, Tomi and Ohtsuki, Tomoki},
	month = mar,
	year = {2017},
	pages = {044708},
}

@article{agresti2019pattern,
  title={Pattern recognition techniques for boson sampling validation},
  author={Agresti, Iris and Viggianiello, Niko and Flamini, Fulvio and Spagnolo, Nicol{\`o} and Crespi, Andrea and Osellame, Roberto and Wiebe, Nathan and Sciarrino, Fabio},
  journal={Physical Review X},
  volume={9},
  number={1},
  pages={011013},
  year={2019},
  publisher={APS}
}



@article{seif_machine_2018,
	title = {Machine learning assisted readout of trapped-ion qubits},
	volume = {51},
	issn = {0953-4075, 1361-6455},
	url = {http://arxiv.org/abs/1804.07718},
	doi = {10.1088/1361-6455/aad62b},
	abstract = {We reduce measurement errors in a quantum computer using machine learning techniques. We exploit a simple yet versatile neural network to classify multi-qubit quantum states, which is trained using experimental data. This flexible approach allows the incorporation of any number of features of the data with minimal modifications to the underlying network architecture. We experimentally illustrate this approach in the readout of trapped-ion qubits using additional spatial and temporal features in the data. Using this neural network classifier, we efficiently treat qubit readout crosstalk, resulting in a 30{\textbackslash}\% improvement in detection error over the conventional threshold method. Our approach does not depend on the specific details of the system and can be readily generalized to other quantum computing platforms.},
	number = {17},
	urldate = {2018-12-04},
	journal = {Journal of Physics B: Atomic, Molecular and Optical Physics},
	author = {Seif, Alireza and Landsman, Kevin A. and Linke, Norbert M. and Figgatt, Caroline and Monroe, C. and Hafezi, Mohammad},
	month = sep,
	year = {2018},
	note = {arXiv: 1804.07718},
	keywords = {Quantum Physics},
	pages = {174006},
	file = {arXiv\:1804.07718 PDF:/Users/gcarleo/Zotero/storage/XT3Z43JG/Seif et al. - 2018 - Machine learning assisted readout of trapped-ion q.pdf:application/pdf;arXiv.org Snapshot:/Users/gcarleo/Zotero/storage/J95AJY7A/1804.html:text/html}
}


@article{tubiana2017emergence,
  title={Emergence of compositional representations in restricted Boltzmann machines},
  author={Tubiana, J{\'e}r{\^o}me and Monasson, R{\'e}mi},
  journal={Physical review letters},
  volume={118},
  number={13},
  pages={138301},
  year={2017},
  publisher={APS}
}

@article{yedidia2003understanding,
  title={Understanding belief propagation and its generalizations},
  author={Yedidia, Jonathan S and Freeman, William T and Weiss, Yair},
  journal={Exploring artificial intelligence in the new millennium},
  volume={8},
  pages={236--239},
  year={2003}
}

@article{lesieur2017constrained,
  title={Constrained low-rank matrix estimation: Phase transitions, approximate message passing and applications},
  author={Lesieur, Thibault and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2017},
  number={7},
  pages={073403},
  year={2017},
  publisher={IOP Publishing}
}


@inproceedings{lesieur2015mmse,
  title={{MMSE} of probabilistic low-rank matrix estimation: Universality with respect to the output channel},
  author={Lesieur, Thibault and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  booktitle={Communication, Control, and Computing (Allerton), 2015 53rd Annual Allerton Conference on},
  pages={680--687},
  year={2015},
  organization={IEEE}
}


@article{von2007tutorial,
  title={A tutorial on spectral clustering},
  author={Von Luxburg, Ulrike},
  journal={Statistics and computing},
  volume={17},
  number={4},
  pages={395--416},
  year={2007},
  publisher={Springer}
}

@inproceedings{ng2002spectral,
  title={On spectral clustering: Analysis and an algorithm},
  author={Ng, Andrew Y and Jordan, Michael I and Weiss, Yair},
  booktitle={Advances in neural information processing systems},
  pages={849--856},
  year={2002}
}

@article{krzakala2013spectral,
  title={Spectral redemption in clustering sparse networks},
  author={Krzakala, Florent and Moore, Cristopher and Mossel, Elchanan and Neeman, Joe and Sly, Allan and Zdeborov{\'a}, Lenka and Zhang, Pan},
  journal={Proceedings of the National Academy of Sciences},
  volume={110},
  number={52},
  pages={20935--20940},
  year={2013},
  publisher={National Acad Sciences}
}

@article{decelle2011inference,
  title={Inference and phase transitions in the detection of modules in sparse networks},
  author={Decelle, Aurelien and Krzakala, Florent and Moore, Cristopher and Zdeborov{\'a}, Lenka},
  journal={Physical Review Letters},
  volume={107},
  number={6},
  pages={065701},
  year={2011},
  publisher={APS}
}

@article{decelle2011asymptotic,
  title={Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications},
  author={Decelle, Aurelien and Krzakala, Florent and Moore, Cristopher and Zdeborov{\'a}, Lenka},
  journal={Physical Review E},
  volume={84},
  number={6},
  pages={066106},
  year={2011},
  publisher={APS}
}

@article{fortunato2010community,
  title={Community detection in graphs},
  author={Fortunato, Santo},
  journal={Physics reports},
  volume={486},
  number={3-5},
  pages={75--174},
  year={2010},
  publisher={Elsevier}
}



@article{barkai1994statistical,
  title={Statistical mechanics of the maximum-likelihood density estimation},
  author={Barkai, N and Sompolinsky, Haim},
  journal={Physical Review E},
  volume={50},
  number={3},
  pages={1766},
  year={1994},
  publisher={APS}
}

@article{watkin1994optimal,
  title={Optimal unsupervised learning},
  author={Watkin, TLH and Nadal, J-P},
  journal={Journal of Physics A: Mathematical and General},
  volume={27},
  number={6},
  pages={1899},
  year={1994},
  publisher={IOP Publishing}
}

@article{biehl1993statistical,
  title={Statistical mechanics of unsupervised learning},
  author={Biehl, M and Mietzner, A},
  journal={EPL (Europhysics Letters)},
  volume={24},
  number={5},
  pages={421},
  year={1993},
  publisher={IOP Publishing}
}

@article{baldassi2016unreasonable,
  title={Unreasonable effectiveness of learning neural networks: From accessible states and robust ensembles to basic algorithmic schemes},
  author={Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer T and Ingrosso, Alessandro and Lucibello, Carlo and Saglietti, Luca and Zecchina, Riccardo},
  journal={Proceedings of the National Academy of Sciences},
  volume={113},
  number={48},
  pages={E7655--E7662},
  year={2016},
  publisher={National Acad Sciences}
}


@article{chaudhari2016entropy,
  title={Entropy-{SGD}: Biasing gradient descent into wide valleys},
  author={Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal={in ICLR 2017, arXiv preprint arXiv:1611.01838},
  year={2016}
}

@article{baity2018comparing,
  title={Comparing dynamics: Deep neural networks versus glassy systems},
  author={Baity-Jesi, Marco and Sagun, Levent and Geiger, Mario and Spigler, Stefano and Arous, G Ben and Cammarota, Chiara and LeCun, Yann and Wyart, Matthieu and Biroli, Giulio},
  journal={in ICML 2018, arXiv preprint arXiv:1803.06969},
  year={2018}
}

@article{advani2017high,
  title={High-dimensional dynamics of generalization error in neural networks},
  author={Advani, Madhu S and Saxe, Andrew M},
  journal={arXiv preprint arXiv:1710.03667},
  year={2017}
}

@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={in ICLR 2014, arXiv preprint arXiv:1312.6120},
  year={2013}
}

@book{mezard2009information,
  title={Information, physics, and computation},
  author={M\'ezard, Marc and Montanari, Andrea},
  year={2009},
  publisher={Oxford University Press}
}

@book{nishimori2001statistical,
  title={Statistical physics of spin glasses and information processing: an introduction},
  author={Nishimori, Hidetoshi},
  volume={111},
  year={2001},
  publisher={Clarendon Press}
}

@inproceedings{krzakala2013phase,
  title={Phase diagram and approximate message passing for blind calibration and dictionary learning},
  author={Krzakala, Florent and M{\'e}zard, Marc and Zdeborov{\'a}, Lenka},
  booktitle={Information Theory Proceedings (ISIT), 2013 IEEE International Symposium on},
  pages={659--663},
  year={2013},
  organization={IEEE}
}

@article{kabashima2016phase,
  title={Phase transitions and sample complexity in bayes-optimal matrix factorization},
  author={Kabashima, Yoshiyuki and Krzakala, Florent and M{\'e}zard, Marc and Sakata, Ayaka and Zdeborov{\'a}, Lenka},
  journal={IEEE Transactions on Information Theory},
  volume={62},
  number={7},
  pages={4228--4265},
  year={2016},
  publisher={IEEE}
}

@article{sakata2013statistical,
  title={Statistical mechanics of dictionary learning},
  author={Sakata, Ayaka and Kabashima, Yoshiyuki},
  journal={EPL (Europhysics Letters)},
  volume={103},
  number={2},
  pages={28008},
  year={2013},
  publisher={IOP Publishing}
}

@inproceedings{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}


@article{sompolinsky1990learning,
  title={Learning from examples in large neural networks},
  author={Sompolinsky, Haim and Tishby, Naftali and Seung, H Sebastian},
  journal={Physical Review Letters},
  volume={65},
  number={13},
  pages={1683},
  year={1990},
  publisher={APS}
}

@article{gyorgyitishby1990learning,
  title={Statistical Theory of Learning a Rule},
  author={Gy{\"o}rgyi, G{\'e}za and Tishby, Naftali},
  journal={in W.T. Theumann and R. Kobrele (Editors), Neural Networks and Spin Glasses},
  pages={3--36},
  year={1990},
  publisher={World Scientific}
}

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@article{mehta2018high,
  title={A high-bias, low-variance introduction to machine learning for physicists},
  author={Mehta, Pankaj and Bukov, Marin and Wang, Ching-Hao and Day, Alexandre GR and Richardson, Clint and Fisher, Charles K and Schwab, David J},
  journal={arXiv preprint arXiv:1803.08823},
  year={2018}
}

@unpublished{wang2018generative,
  title={Generative Models for Physicists},
  author={Wang, Lei},
  year={2018}
}

@article{wu2018solving,
  title={Solving Statistical Mechanics using Variational Autoregressive Networks},
  author={Wu, Dian and Wang, Lei and Zhang, Pan},
  journal={arXiv preprint arXiv:1809.10606},
  year={2018}
}

@article{mezard2017mean,
  title={Mean-field message-passing equations in the {H}opfield model and its generalizations},
  author={M{\'e}zard, Marc},
  journal={Physical Review E},
  volume={95},
  number={2},
  pages={022117},
  year={2017},
  publisher={APS}
}

@article{barra2018phase,
  title={Phase diagram of restricted Boltzmann machines and generalized {H}opfield networks with arbitrary priors},
  author={Barra, Adriano and Genovese, Giuseppe and Sollich, Peter and Tantari, Daniele},
  journal={Physical Review E},
  volume={97},
  number={2},
  pages={022310},
  year={2018},
  publisher={APS}
}

@article{schoenholz2017relationship,
  title={Relationship between local structure and relaxation in out-of-equilibrium glassy systems},
  author={Schoenholz, Samuel S and Cubuk, Ekin D and Kaxiras, Efthimios and Liu, Andrea J},
  journal={Proceedings of the National Academy of Sciences},
  volume={114},
  number={2},
  pages={263--267},
  year={2017},
  publisher={National Acad Sciences}
}

@article{cubuk2015identifying,
  title={Identifying structural flow defects in disordered solids using machine-learning methods},
  author={Cubuk, Ekin D and Schoenholz, Samuel Stern and Rieser, Jennifer M and Malone, Brad Dean and Rottler, Joerg and Durian, Douglas J and Kaxiras, Efthimios and Liu, Andrea J},
  journal={Physical review letters},
  volume={114},
  number={10},
  pages={108001},
  year={2015},
  publisher={APS}
}

@article{wetzel2017unsupervised,
  title={Unsupervised learning of phase transitions: From principal component analysis to variational autoencoders},
  author={Wetzel, Sebastian J},
  journal={Physical Review E},
  volume={96},
  number={2},
  pages={022140},
  year={2017},
  publisher={APS}
}


@article{nguyen2017inverse,
  title={Inverse statistical problems: from the inverse Ising problem to data science},
  author={Nguyen, H Chau and Zecchina, Riccardo and Berg, Johannes},
  journal={Advances in Physics},
  volume={66},
  number={3},
  pages={197--261},
  year={2017},
  publisher={Taylor \& Francis}
}


@article{uria_neural_2016,
	title = {Neural {Autoregressive} {Distribution} {Estimation}},
	volume = {17},
	url = {http://jmlr.org/papers/v17/16-272.html},
	number = {205},
	urldate = {2019-03-07},
	journal = {Journal of Machine Learning Research},
	author = {Uria, Benigno and Côté, Marc-Alexandre and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
	year = {2016},
	pages = {1--37},
	file = {Full Text PDF:/Users/giuscarl/Zotero/storage/F4Y3V4DL/Uria et al. - 2016 - Neural Autoregressive Distribution Estimation.pdf:application/pdf;Snapshot:/Users/giuscarl/Zotero/storage/2LRK22AI/16-272.html:text/html}
}
@article{van2017learning,
  title={Learning phase transitions by confusion},
  author={Van Nieuwenburg, Evert PL and Liu, Ye-Hua and Huber, Sebastian D},
  journal={Nature Physics},
  volume={13},
  number={5},
  pages={435},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{koch2018mutual,
  title={Mutual information, neural networks and the renormalization group},
  author={Koch-Janusz, Maciej and Ringel, Zohar},
  journal={Nature Physics},
  volume={14},
  number={6},
  pages={578},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{li2018neural,
  title = {Neural Network Renormalization Group},
  author = {Li, Shuo-Hui and Wang, Lei},
  journal = {Phys. Rev. Lett.},
  volume = {121},
  issue = {26},
  pages = {260601},
  numpages = {7},
  year = {2018}
}




@article{huang2017accelerated,
  title={Accelerated Monte Carlo simulations with restricted Boltzmann machines},
  author={Huang, Li and Wang, Lei},
  journal={Physical Review B},
  volume={95},
  number={3},
  pages={035105},
  year={2017},
  publisher={APS}
}

@article{gyorgyi1990first,
  title={First-order transition to perfect generalization in a neural network with binary synapses},
  author={Gy{\"o}rgyi, G{\'e}za},
  journal={Physical Review A},
  volume={41},
  number={12},
  pages={7097},
  year={1990},
  publisher={APS}
}

@article{carleo2018constructing,
  title={Constructing exact representations of quantum many-body systems with deep neural networks},
  author={Carleo, Giuseppe and Nomura, Yusuke and Imada, Masatoshi},
  journal={Nature communications},
  volume={9},
  number={1},
  pages={5322},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{beny2013deep,
  title={Deep learning and the renormalization group},
  author={B{\'e}ny, C{\'e}dric},
  journal={in ICLR 2013, arXiv preprint arXiv:1301.3124},
  year={2013}
}

@article{schwarze1993learning,
  title={Learning a rule in a multilayer neural network},
  author={Schwarze, Henry},
  journal={Journal of Physics A: Mathematical and General},
  volume={26},
  number={21},
  pages={5781},
  year={1993},
  publisher={IOP Publishing}
}

@inproceedings{choromanska2015loss,
  title={The loss surfaces of multilayer networks},
  author={Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  booktitle={Artificial Intelligence and Statistics},
  pages={192--204},
  year={2015}
}

@inproceedings{seung1992query,
  title={Query by committee},
  author={Seung, H Sebastian and Opper, Manfred and Sompolinsky, Haim},
  booktitle={Proceedings of the fifth annual workshop on Computational learning theory},
  pages={287--294},
  year={1992},
  organization={ACM}
}

@article{gabrie2018entropy,
  title={Entropy and mutual information in models of deep neural networks},
  author={Gabri{\'e}, Marylou and Manoel, Andre and Luneau, Cl{\'e}ment and Barbier, Jean and Macris, Nicolas and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  journal={in NeurIPS 2018, arXiv preprint arXiv:1805.09785},
  year={2018}
}

@article{saxe2018information,
  title={On the information bottleneck theory of deep learning},
  author={Saxe, Andrew Michael and Bansal, Yamini and Dapello, Joel and Advani, Madhu and Kolchinsky, Artemy and Tracey, Brendan Daniel and Cox, David Daniel},
  year={2018}
}

@article{shwartz2017opening,
  title={Opening the black box of deep neural networks via information},
  author={Shwartz-Ziv, Ravid and Tishby, Naftali},
  journal={arXiv preprint arXiv:1703.00810},
  year={2017}
}

@inproceedings{tishby2015deep,
  title={Deep learning and the information bottleneck principle},
  author={Tishby, Naftali and Zaslavsky, Noga},
  booktitle={Information Theory Workshop (ITW), 2015 IEEE},
  pages={1--5},
  year={2015},
  organization={IEEE}
}

@article{tishby2000information,
  title={The information bottleneck method},
  author={Tishby, Naftali and Pereira, Fernando C and Bialek, William},
  journal={arXiv preprint physics/0004057},
  year={2000}
}

@article{dean1996langevin,
  title={Langevin equation for the density of a system of interacting Langevin processes},
  author={Dean, David S},
  journal={Journal of Physics A: Mathematical and General},
  volume={29},
  number={24},
  pages={L613},
  year={1996},
  publisher={IOP Publishing}
}

@inproceedings{rotskoff2018parameters,
  title={Parameters as interacting particles: long time convergence and asymptotic error scaling of neural networks},
  author={Rotskoff, Grant and Vanden-Eijnden, Eric},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7146--7155},
  year={2018}
}

@article{mei2018mean,
  title={A Mean Field View of the Landscape of Two-Layers Neural Networks},
  author={Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
  journal={arXiv preprint arXiv:1804.06561},
  year={2018}
}

@article{carrasquilla2017machine,
  title={Machine learning phases of matter},
  author={Carrasquilla, Juan and Melko, Roger G},
  journal={Nature Physics},
  volume={13},
  number={5},
  pages={431},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{mehta2014exact,
  title={An exact mapping between the variational renormalization group and deep learning},
  author={Mehta, Pankaj and Schwab, David J},
  journal={arXiv preprint arXiv:1410.3831},
  year={2014}
}

@article{hinton2002training,
  title={Training products of experts by minimizing contrastive divergence},
  author={Hinton, Geoffrey E},
  journal={Neural computation},
  volume={14},
  number={8},
  pages={1771--1800},
  year={2002},
  publisher={MIT Press}
}

@inproceedings{saade2014spectral,
  title={Spectral clustering of graphs with the bethe hessian},
  author={Saade, Alaa and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  booktitle={Advances in Neural Information Processing Systems},
  pages={406--414},
  year={2014}
}


@inproceedings{matsushita2013low,
  title={Low-rank matrix reconstruction and clustering via approximate message passing},
  author={Matsushita, Ryosuke and Tanaka, Toshiyuki},
  booktitle={Advances in Neural Information Processing Systems},
  pages={917--925},
  year={2013}
}

@inproceedings{rangan2012iterative,
  title={Iterative estimation of constrained rank-one matrices in noise},
  author={Rangan, Sundeep and Fletcher, Alyson K},
  booktitle={Information Theory Proceedings (ISIT), 2012 IEEE International Symposium on},
  pages={1246--1250},
  year={2012},
  organization={IEEE}
}

@article{thouless1977solution,
  title={Solution of'solvable model of a spin glass'},
  author={Thouless, David J and Anderson, Philip W and Palmer, Robert G},
  journal={Philosophical Magazine},
  volume={35},
  number={3},
  pages={593--601},
  year={1977},
  publisher={Taylor \& Francis}
}

@article{tramel2018deterministic,
  title={Deterministic and Generalized Framework for Unsupervised Learning with Restricted Boltzmann Machines},
  author={Tramel, Eric W and Gabri{\'e}, Marylou and Manoel, Andre and Caltagirone, Francesco and Krzakala, Florent},
  journal={Physical Review X},
  volume={8},
  number={4},
  pages={041006},
  year={2018},
  publisher={APS}
}

@inproceedings{gabrie2015training,
  title={Training Restricted Boltzmann Machine via the￼ Thouless-Anderson-Palmer free energy},
  author={Gabri{\'e}, Marylou and Tramel, Eric W and Krzakala, Florent},
  booktitle={Advances in Neural Information Processing Systems},
  pages={640--648},
  year={2015}
}

@article{decelle2017spectral,
  title={Spectral dynamics of learning in restricted Boltzmann machines},
  author={Decelle, Aur{\'e}lien and Fissore, Giancarlo and Furtlehner, Cyril},
  journal={EPL (Europhysics Letters)},
  volume={119},
  number={6},
  pages={60001},
  year={2017},
  publisher={IOP Publishing}
}

@article{tubiana2018learning,
  title={Learning protein constitutive motifs from sequence data},
  author={Tubiana, J{\'e}r{\^o}me and Cocco, Simona and Monasson, R{\'e}mi},
  journal={arXiv preprint arXiv:1803.08718},
  year={2018}
}

@book{talagrand2003spin,
  title={Spin glasses: a challenge for mathematicians: cavity and mean field models},
  author={Talagrand, Michel},
  volume={46},
  year={2003},
  publisher={Springer Science \& Business Media}
}

@article{kim1998covering,
  title={Covering cubes by random half cubes, with applications to binary neural networks},
  author={Kim, Jeong Han and Roche, James R},
  journal={Journal of Computer and System Sciences},
  volume={56},
  number={2},
  pages={223--252},
  year={1998},
  publisher={Elsevier}
}


@article{franz2016simplest,
  title={The simplest model of jamming},
  author={Franz, Silvio and Parisi, Giorgio},
  journal={Journal of Physics A: Mathematical and Theoretical},
  volume={49},
  number={14},
  pages={145001},
  year={2016},
  publisher={IOP Publishing}
}

@article{stojnic2013another,
  title={Another look at the Gardner problem},
  author={Stojnic, Mihailo},
  journal={arXiv preprint arXiv:1306.3979},
  year={2013}
}

@article{baldassi2015subdominant,
  title={Subdominant dense clusters allow for simple learning and high computational performance in neural networks with discrete synapses},
  author={Baldassi, Carlo and Ingrosso, Alessandro and Lucibello, Carlo and Saglietti, Luca and Zecchina, Riccardo},
  journal={Physical review letters},
  volume={115},
  number={12},
  pages={128101},
  year={2015},
  publisher={APS}
}

@article{gardner1989three,
  title={Three unfinished works on the optimal storage capacity of networks},
  author={Gardner, Elizabeth and Derrida, Bernard},
  journal={Journal of Physics A: Mathematical and General},
  volume={22},
  number={12},
  pages={1983},
  year={1989},
  publisher={IOP Publishing}
}

@article{derrida1987exactly,
  title={An exactly solvable asymmetric neural network model},
  author={Derrida, Bernard and Gardner, Elizabeth and Zippelius, Anne},
  journal={EPL (Europhysics Letters)},
  volume={4},
  number={2},
  pages={167},
  year={1987},
  publisher={IOP Publishing}
}

@article{gardner1988space,
  title={The space of interactions in neural network models},
  author={Gardner, Elizabeth},
  journal={Journal of physics A: Mathematical and general},
  volume={21},
  number={1},
  pages={257},
  year={1988},
  publisher={IOP Publishing}
}

@article{valiant1984theory,
  title={A theory of the learnable},
  author={Valiant, Leslie G},
  journal={Communications of the ACM},
  volume={27},
  number={11},
  pages={1134--1142},
  year={1984},
  publisher={ACM}
}

@article{gardner1987maximum,
  title={Maximum storage capacity in neural networks},
  author={Gardner, Elizabeth},
  journal={EPL (Europhysics Letters)},
  volume={4},
  number={4},
  pages={481},
  year={1987},
  publisher={IOP Publishing}
}

@article{zdeborova2008locked,
  title={Locked constraint satisfaction problems},
  author={Zdeborov{\'a}, Lenka and M{\'e}zard, Marc},
  journal={Physical review letters},
  volume={101},
  number={7},
  pages={078702},
  year={2008},
  publisher={APS}
}

@article{martin2004frozen,
  title={Frozen glass phase in the multi-index matching problem},
  author={Martin, OC and M{\'e}zard, M and Rivoire, O},
  journal={Physical review letters},
  volume={93},
  number={21},
  pages={217205},
  year={2004},
  publisher={APS}
}

@article{shcherbina2003rigorous,
  title={Rigorous solution of the Gardner problem},
  author={Shcherbina, Mariya and Tirozzi, Brunello},
  journal={Communications in mathematical physics},
  volume={234},
  number={3},
  pages={383--422},
  year={2003},
  publisher={Springer}
}

@article{cover1965geometrical,
  title={Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition},
  author={Cover, Thomas M},
  journal={IEEE transactions on electronic computers},
  number={3},
  pages={326--334},
  year={1965},
  publisher={IEEE}
}



@article{stojnic2013negative,
  title={Negative spherical perceptron},
  author={Stojnic, Mihailo},
  journal={arXiv preprint arXiv:1306.3980},
  year={2013}
}

@article{stojnic2013discrete,
  title={Discrete perceptrons},
  author={Stojnic, Mihailo},
  journal={arXiv preprint arXiv:1306.4375},
  year={2013}
}


@article{talagrand2006parisi,
  title={The parisi formula},
  author={Talagrand, Michel},
  journal={Annals of mathematics},
  pages={221--263},
  year={2006},
  publisher={JSTOR}
}

@article{achlioptas2011solution,
  title={On the solution-space geometry of random constraint satisfaction problems},
  author={Achlioptas, Dimitris and Coja-Oghlan, Amin and Ricci-Tersenghi, Federico},
  journal={Random Structures \& Algorithms},
  volume={38},
  number={3},
  pages={251--268},
  year={2011},
  publisher={Wiley Online Library}
}

@article{panchenko2014parisi,
  title={The Parisi formula for mixed $ p $-spin models},
  author={Panchenko, Dmitry and others},
  journal={The Annals of Probability},
  volume={42},
  number={3},
  pages={946--958},
  year={2014},
  publisher={Institute of Mathematical Statistics}
}

@inproceedings{ding2015proof,
  title={Proof of the satisfiability conjecture for large k},
  author={Ding, Jian and Sly, Allan and Sun, Nike},
  booktitle={Proceedings of the forty-seventh annual ACM symposium on Theory of computing},
  pages={59--68},
  year={2015},
  organization={ACM}
}

@article{aubin2018committee,
  title={The committee machine: Computational to statistical gaps in learning a two-layers neural network},
  author={Aubin, Benjamin and Maillard, Antoine and Barbier, Jean and Krzakala, Florent and Macris, Nicolas and Zdeborov{\'a}, Lenka},
  journal={in NeurIPS 2018, arXiv preprint arXiv:1806.05451},
  year={2018}
}

@book{engel2001statistical,
  title={Statistical mechanics of learning},
  author={Engel, Andreas and Van den Broeck, Christian},
  year={2001},
  publisher={Cambridge University Press}
}

@article{watkin1993statistical,
  title={The statistical mechanics of learning a rule},
  author={Watkin, Timothy LH and Rau, Albrecht and Biehl, Michael},
  journal={Reviews of Modern Physics},
  volume={65},
  number={2},
  pages={499},
  year={1993},
  publisher={APS}
}

@article{cocco2018statistical,
  title={Statistical physics and representations in real and artificial neural networks},
  author={Cocco, Simona and Monasson, R{\'e}mi and Posani, Lorenzo and Rosay, Sophie and Tubiana, J{\'e}r{\^o}me},
  journal={Physica A: Statistical Mechanics and its Applications},
  volume={504},
  pages={45--76},
  year={2018},
  publisher={Elsevier}
}

@article{lelarge2016fundamental,
  title={Fundamental limits of symmetric low-rank matrix estimation},
  author={Lelarge, Marc and Miolane, L{\'e}o},
  journal={Probability Theory and Related Fields},
  pages={1--71},
  year={2016},
  publisher={Springer}
}

@article{amit1985spin,
  title={Spin-glass models of neural networks},
  author={Amit, Daniel J and Gutfreund, Hanoch and Sompolinsky, Haim},
  journal={Physical Review A},
  volume={32},
  number={2},
  pages={1007},
  year={1985},
  publisher={APS}
}


@inproceedings{deshpande2014information,
  title={Information-theoretically optimal sparse {PCA}},
  author={Deshpande, Yash and Montanari, Andrea},
  year={2014},
  booktitle={2014 IEEE International Symposium on Information Theory}
}


@article{saad1995line,
  title={On-line learning in soft committee machines},
  author={Saad, David and Solla, Sara A},
  journal={Physical Review E},
  volume={52},
  number={4},
  pages={4225},
  year={1995},
  publisher={APS}
}

@article{saad1995exact,
  title={Exact solution for on-line learning in multilayer neural networks},
  author={Saad, David and Solla, Sara A},
  journal={Physical Review Letters},
  volume={74},
  number={21},
  pages={4337},
  year={1995},
  publisher={APS}
}

@article{johnstone2009consistency,
  title={On consistency and sparsity for principal components analysis in high dimensions},
  author={Johnstone, Iain M and Lu, Arthur Yu},
  journal={Journal of the American Statistical Association},
  volume={104},
  number={486},
  pages={682--693},
  year={2009},
  publisher={Taylor \& Francis}
}

@article{hopfield1982neural,
  title={Neural networks and physical systems with emergent collective computational abilities},
  author={Hopfield, John J},
  journal={Proceedings of the national academy of sciences},
  volume={79},
  number={8},
  pages={2554--2558},
  year={1982},
  publisher={National Acad Sciences}
}

@article{wang2018solvable,
  title={A Solvable High-Dimensional Model of {GAN}},
  author={Wang, Chuang and Hu, Hong and Lu, Yue M},
  journal={arXiv preprint arxXiv:1805.08349},
  year={2018}
}

@article{martiniani2017quantifying,
  title = {Quantifying Hidden Order out of Equilibrium},
  author = {Martiniani, Stefano and Chaikin, Paul M. and Levine, Dov},
  journal = {Phys. Rev. X},
  volume = {9},
  issue = {1},
  pages = {011031},
  numpages = {13},
  year = {2019},
}

@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}

@book{minsky69perceptrons,
  added-at = {2008-05-16T13:57:01.000+0200},
  address = {Cambridge, MA, USA},
  author = {Minsky, Marvin and Papert, Seymour},
  publisher = {MIT Press},
  title = {Perceptrons: An Introduction to Computational Geometry},
  year = 1969
}

@article{coja2018information,
  title={Information-theoretic thresholds from the cavity method},
  author={Coja-Oghlan, Amin and Krzakala, Florent and Perkins, Will and Zdeborov\'{a}, Lenka},
  journal={Advances in Mathematics},
  volume={333},
  pages={694--795},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{barbier2016mutual,
  title={Mutual information for symmetric rank-one matrix estimation: A proof of the replica formula},
  author={Barbier, Jean and Dia, Mohamad and Macris, Nicolas and Krzakala, Florent and Lesieur, Thibault and Zdeborov{\'a}, Lenka},
  booktitle={Advances in Neural Information Processing Systems},
  pages={424--432},
  year={2016}
}

@article{javanmard2013state,
  title={State evolution for general approximate message passing algorithms, with applications to spatial coupling},
  author={Javanmard, Adel and Montanari, Andrea},
  journal={Information and Inference: A Journal of the IMA},
  volume={2},
  number={2},
  pages={115--144},
  year={2013},
  publisher={OUP}
}

@article{lee2017deep,
  title={Deep neural networks as gaussian processes},
  author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={ICRL 2018},
  eprint={arXiv:1711.00165},
  year={2018}
}

@article{schindler2017probing,
  title={Probing many-body localization with neural networks},
  author={Schindler, Frank and Regnault, Nicolas and Neupert, Titus},
  journal={Physical Review B},
  volume={95},
  number={24},
  pages={245134},
  year={2017},
  publisher={APS}
}


@inproceedings{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in neural information processing systems},
  pages={8580--8589},
  year={2018}
}

@article{morningstar2017deep,
  title={Deep Learning the Ising Model Near Criticality},
  author={Morningstar, Alan and Melko, Roger G},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={163},
  pages={1--17},
  year={2018}
}


@article{wang2018machine,
  title={Machine learning of frustrated classical spin models (II): Kernel principal component analysis},
  author={Wang, Ce and Zhai, Hui},
  journal={Frontiers of Physics},
  volume={13},
  number={5},
  pages={130507},
  year={2018},
  publisher={Springer}
}

@article{wang2017machine,
  title={Machine learning of frustrated classical spin models. I. Principal component analysis},
  author={Wang, Ce and Zhai, Hui},
  journal={Physical Review B},
  volume={96},
  number={14},
  pages={144432},
  year={2017},
  publisher={APS}
}

@article{bradde2017pca,
  title={PCA Meets RG},
  author={Bradde, Serena and Bialek, William},
  journal={Journal of Statistical Physics},
  volume={167},
  number={3-4},
  pages={462--475},
  year={2017},
  publisher={Springer}
}

@article{bolthausen2014iterative,
  title={An iterative construction of solutions of the TAP equations for the Sherrington--Kirkpatrick model},
  author={Bolthausen, Erwin},
  journal={Communications in Mathematical Physics},
  volume={325},
  number={1},
  pages={333--366},
  year={2014},
  publisher={Springer}
}

@article{barbier2017phase,
  title={Phase transitions, optimal errors and optimality of message-passing in generalized linear models},
  author={Barbier, Jean and Krzakala, Florent and Macris, Nicolas and Miolane, L{\'e}o and Zdeborov{\'a}, Lenka},
  journal={to appear in PNAS, arXiv preprint arXiv:1708.03395},
  year={2017}
}

@article{zdeborova2016statistical,
  title={Statistical physics of inference: Thresholds and algorithms},
  author={Zdeborov{\'a}, Lenka and Krzakala, Florent},
  journal={Advances in Physics},
  volume={65},
  number={5},
  pages={453--552},
  year={2016},
  publisher={Taylor \& Francis}
}

@inproceedings{achlioptas2002asymptotic,
  title={The asymptotic order of the random k-SAT threshold},
  author={Achlioptas, Dimitris and Moore, Cristopher},
  booktitle={Foundations of Computer Science, 2002. Proceedings. The 43rd Annual IEEE Symposium on},
  pages={779--788},
  year={2002},
  organization={IEEE}
}


@article{cristoforetti_towards_2017,
	title = {Towards meaningful physics from generative models},
	url = {http://arxiv.org/abs/1705.09524},
	abstract = {In several physical systems, important properties characterizing the system itself are theoretically related with specific degrees of freedom. Although standard Monte Carlo simulations provide an effective tool to accurately reconstruct the physical configurations of the system, they are unable to isolate the different contributions corresponding to different degrees of freedom. Here we show that unsupervised deep learning can become a valid support to MC simulation, coupling useful insights in the phases detection task with good reconstruction performance. As a testbed we consider the 2D XY model, showing that a deep neural network based on variational autoencoders can detect the continuous Kosterlitz-Thouless (KT) transitions, and that, if endowed with the appropriate constrains, they generate configurations with meaningful physical content.},
	urldate = {2019-03-14},
	journal = {arXiv:1705.09524 [cond-mat, physics:hep-lat]},
	author = {Cristoforetti, Marco and Jurman, Giuseppe and Nardelli, Andrea I. and Furlanello, Cesare},
	month = may,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Statistical Mechanics, High Energy Physics - Lattice},
	file = {arXiv\:1705.09524 PDF:/Users/gcarleo/Zotero/storage/XCF9TZZX/Cristoforetti et al. - 2017 - Towards meaningful physics from generative models.pdf:application/pdf;arXiv.org Snapshot:/Users/gcarleo/Zotero/storage/VVWA9NNG/1705.html:text/html}
}

@article{seung1992statistical,
  title={Statistical mechanics of learning from examples},
  author={Seung, HS and Sompolinsky, Haim and Tishby, N},
  journal={Physical Review A},
  volume={45},
  number={8},
  pages={6056},
  year={1992},
  publisher={APS}
}

@book{NishimoriBook01,
    author = "H. Nishimori",
    title = "Statistical Physics of Spin Glasses and Information
Processing:
           An Introduction",
    publisher = "Oxford University Press",
    year = 2001,
    address = "Oxford, UK"}

@article{wigley_fast_2016,
	title = {Fast machine-learning online optimization of ultra-cold-atom experiments},
	volume = {6},
	copyright = {2016 Nature Publishing Group},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/srep25890},
	doi = {10.1038/srep25890},
	abstract = {We apply an online optimization process based on machine learning to the production of Bose-Einstein condensates (BEC). BEC is typically created with an exponential evaporation ramp that is optimal for ergodic dynamics with two-body s-wave interactions and no other loss rates, but likely sub-optimal for real experiments. Through repeated machine-controlled scientific experimentation and observations our ‘learner’ discovers an optimal evaporation ramp for BEC production. In contrast to previous work, our learner uses a Gaussian process to develop a statistical model of the relationship between the parameters it controls and the quality of the BEC produced. We demonstrate that the Gaussian process machine learner is able to discover a ramp that produces high quality BECs in 10 times fewer iterations than a previously used online optimization technique. Furthermore, we show the internal model developed can be used to determine which parameters are essential in BEC creation and which are unimportant, providing insight into the optimization process of the system.},
	language = {en},
	urldate = {2018-12-04},
	journal = {Scientific Reports},
	author = {Wigley, P. B. and Everitt, P. J. and van den Hengel, A. and Bastian, J. W. and Sooriyabandara, M. A. and McDonald, G. D. and Hardman, K. S. and Quinlivan, C. D. and Manju, P. and Kuhn, C. C. N. and Petersen, I. R. and Luiten, A. N. and Hope, J. J. and Robins, N. P. and Hush, M. R.},
	month = may,
	year = {2016},
	pages = {25890},
	file = {Full Text PDF:/Users/gcarleo/Zotero/storage/WRCM9SFR/Wigley et al. - 2016 - Fast machine-learning online optimization of ultra.pdf:application/pdf;Snapshot:/Users/gcarleo/Zotero/storage/HLQKVBIZ/srep25890.html:text/html}
}


@article{kochkov_variational_2018,
	title = {Variational optimization in the {AI} era: {Computational} {Graph} {States} and {Supervised} {Wave}-function {Optimization}},
	shorttitle = {Variational optimization in the {AI} era},
	url = {http://arxiv.org/abs/1811.12423},
	urldate = {2019-03-14},
	journal = {arXiv:1811.12423 [cond-mat, physics:physics]},
	author = {Kochkov, Dmitrii and Clark, Bryan K.},
	month = nov,
	year = {2018}
}

@article{torlai_latent_2018,
	title = {Latent {Space} {Purification} via {Neural} {Density} {Operators}},
	volume = {120},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.120.240503},
	doi = {10.1103/PhysRevLett.120.240503},
	abstract = {Machine learning is actively being explored for its potential to design, validate, and even hybridize with near-term quantum devices. A central question is whether neural networks can provide a tractable representation of a given quantum state of interest. When true, stochastic neural networks can be employed for many unsupervised tasks, including generative modeling and state tomography. However, to be applicable for real experiments, such methods must be able to encode quantum mixed states. Here, we parametrize a density matrix based on a restricted Boltzmann machine that is capable of purifying a mixed state through auxiliary degrees of freedom embedded in the latent space of its hidden units. We implement the algorithm numerically and use it to perform tomography on some typical states of entangled photons, achieving fidelities competitive with standard techniques.},
	number = {24},
	urldate = {2018-12-04},
	journal = {Physical Review Letters},
	author = {Torlai, Giacomo and Melko, Roger G.},
	month = jun,
	year = {2018},
	pages = {240503},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/UBWXW8U9/PhysRevLett.120.html:text/html;Submitted Version:/Users/gcarleo/Zotero/storage/3M7GCPX3/Torlai and Melko - 2018 - Latent Space Purification via Neural Density Opera.pdf:application/pdf}
}

@article{carleo_localization_2012,
	title = {Localization and {Glassy} {Dynamics} {Of} {Many}-{Body} {Quantum} {Systems}},
	volume = {2},
	doi = {10.1038/srep00243},
	abstract = {When classical systems fail to explore their entire configurational space, intriguing macroscopic phenomena like aging and glass formation may emerge. Also closed quanto-mechanical systems may stop wandering freely around the whole Hilbert space, even if they are initially prepared into a macroscopically large combination of eigenstates. Here, we report numerical evidences that the dynamics of strongly interacting lattice bosons driven sufficiently far from equilibrium can be trapped into extremely long-lived inhomogeneous metastable states. The slowing down of incoherent density excitations above a threshold energy, much reminiscent of a dynamical arrest on the verge of a glass transition, is identified as the key feature of this phenomenon. We argue that the resulting long-lived inhomogeneities are responsible for the lack of thermalization observed in large systems. Such a rich phenomenology could be experimentally uncovered upon probing the out-of-equilibrium dynamics of conveniently prepared quantum states of trapped cold atoms which we hereby suggest.},
	journal = {Scientific Reports},
	author = {Carleo, Giuseppe and Becca, Federico and Schiro, Marco and Fabrizio, Michele},
	month = feb,
	year = {2012},
	pages = {243}
}

@book{becca_quantum_2017,
	address = {Cambridge, United Kingdom ; New York, NY},
	title = {Quantum {Monte} {Carlo} {Approaches} for {Correlated} {Systems}},
	isbn = {978-1-107-12993-1},
	abstract = {Over the past several decades, computational approaches to studying strongly-interacting systems have become increasingly varied and sophisticated. This book provides a comprehensive introduction to state-of-the-art quantum Monte Carlo techniques relevant for applications in correlated systems. Providing a clear overview of variational wave functions, and featuring a detailed presentation of stochastic samplings including Markov chains and Langevin dynamics, which are developed into a discussion of Monte Carlo methods. The variational technique is described, from foundations to a detailed description of its algorithms. Further topics discussed include optimisation techniques, real-time dynamics and projection methods, including Green's function, reptation and auxiliary-field Monte Carlo, from basic definitions to advanced algorithms for efficient codes, and the book concludes with recent developments on the continuum space. Quantum Monte Carlo Approaches for Correlated Systems provides an extensive reference for students and researchers working in condensed matter theory or those interested in advanced numerical methods for electronic simulation.},
	publisher = {Cambridge University Press},
	author = {Becca, Federico and Sorella, Sandro},
	month = nov,
	year = {2017}
}



@article{sorella_green_1998,
	title = {Green {Function} {Monte} {Carlo} with {Stochastic} {Reconfiguration}},
	volume = {80},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.80.4558},
	doi = {10.1103/PhysRevLett.80.4558},
	abstract = {A new method for the stabilization of the sign problem in the Green function Monte Carlo technique is proposed. The method is devised for real lattice Hamiltonians and is based on an iterative “stochastic reconfiguration” scheme which introduces some bias but allows a stable simulation with constant sign. The systematic reduction of this bias is possible in principle. The method is applied to the frustrated J1−J2 Heisenberg model, and tested against exact diagonalization data. Evidence of a finite spin gap for J2/J1{\textgreater}∼0.4 is found in the thermodynamic limit.},
	number = {20},
	urldate = {2018-12-04},
	journal = {Physical Review Letters},
	author = {Sorella, Sandro},
	month = may,
	year = {1998},
	pages = {4558--4561},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/SFU22SW6/PhysRevLett.80.html:text/html;Submitted Version:/Users/gcarleo/Zotero/storage/ZSLHCAHP/Sorella - 1998 - Green Function Monte Carlo with Stochastic Reconfi.pdf:application/pdf}
}


@article{amari_natural_1998,
	title = {Natural {Gradient} {Works} {Efficiently} in {Learning}},
	volume = {10},
	issn = {0899-7667},
	url = {http://www.mitpressjournals.org/doi/10.1162/089976698300017746},
	doi = {10.1162/089976698300017746},
	abstract = {When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural gradient does. Information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation), and the space of linear dynamical systems (for blind source deconvolution). The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters. This suggests that the plateau phenomenon, which appears in the backpropagation learning algorithm of multilayer perceptrons, might disappear or might not be so serious when the natural gradient is used. An adaptive method of updating the learning rate is proposed and analyzed.},
	number = {2},
	urldate = {2017-06-07},
	journal = {Neural Computation},
	author = {Amari, Shun-ichi},
	month = feb,
	year = {1998},
	pages = {251--276},
	file = {Snapshot:/Users/gcarleo/Zotero/storage/T63FZFDM/089976698300017746.html:text/html}
}

@incollection{Smolensky:1986:IPD:104279.104290,
 author = {Smolensky, P.},
 chapter = {Information Processing in Dynamical Systems: Foundations of Harmony Theory},
 title = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1},
 editor = {Rumelhart, David E. and McClelland, James L. and PDP Research Group, CORPORATE},
 year = {1986},
 isbn = {0-262-68053-X},
 pages = {194--281},
 numpages = {88},
 url = {http://dl.acm.org/citation.cfm?id=104279.104290},
 acmid = {104290},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 


@article{broecker_quantum_2017,
	title = {Quantum phase recognition via unsupervised machine learning},
	url = {http://arxiv.org/abs/1707.00663},
	abstract = {The application of state-of-the-art machine learning techniques to statistical physic problems has seen a surge of interest for their ability to discriminate phases of matter by extracting essential features in the many-body wavefunction or the ensemble of correlators sampled in Monte Carlo simulations. Here we introduce a gener- alization of supervised machine learning approaches that allows to accurately map out phase diagrams of inter- acting many-body systems without any prior knowledge, e.g. of their general topology or the number of distinct phases. To substantiate the versatility of this approach, which combines convolutional neural networks with quantum Monte Carlo sampling, we map out the phase diagrams of interacting boson and fermion models both at zero and finite temperatures and show that first-order, second-order, and Kosterlitz-Thouless phase transitions can all be identified. We explicitly demonstrate that our approach is capable of identifying the phase transition to non-trivial many-body phases such as superfluids or topologically ordered phases without supervision.},
	urldate = {2019-03-14},
	journal = {arXiv:1707.00663 [cond-mat]},
	author = {Broecker, Peter and Assaad, Fakher F. and Trebst, Simon},
	month = jul,
	year = {2017},

}

@article{broecker_machine_2017,
	title = {Machine learning quantum phases of matter beyond the fermion sign problem},
	volume = {7},
	copyright = {2017 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-017-09098-0},
	doi = {10.1038/s41598-017-09098-0},
	abstract = {State-of-the-art machine learning techniques promise to become a powerful tool in statistical mechanics via their capacity to distinguish different phases of matter in an automated way. Here we demonstrate that convolutional neural networks (CNN) can be optimized for quantum many-fermion systems such that they correctly identify and locate quantum phase transitions in such systems. Using auxiliary-field quantum Monte Carlo (QMC) simulations to sample the many-fermion system, we show that the Green’s function holds sufficient information to allow for the distinction of different fermionic phases via a CNN. We demonstrate that this QMC + machine learning approach works even for systems exhibiting a severe fermion sign problem where conventional approaches to extract information from the Green’s function, e.g. in the form of equal-time correlation functions, fail.},
	language = {En},
	number = {1},
	urldate = {2018-12-04},
	journal = {Scientific Reports},
	author = {Broecker, Peter and Carrasquilla, Juan and Melko, Roger G. and Trebst, Simon},
	month = aug,
	year = {2017},
	pages = {8823},
	file = {Full Text PDF:/Users/gcarleo/Zotero/storage/7ZCK8U3K/Broecker et al. - 2017 - Machine learning quantum phases of matter beyond t.pdf:application/pdf;Snapshot:/Users/gcarleo/Zotero/storage/DLZ9VVNZ/s41598-017-09098-0.html:text/html}
}

@article{mavadia_prediction_2017,
	title = {Prediction and real-time compensation of qubit decoherence via machine learning},
	volume = {8},
	copyright = {2017 Nature Publishing Group},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/ncomms14106},
	doi = {10.1038/ncomms14106},
	abstract = {The wide-ranging adoption of quantum technologies requires practical, high-performance advances in our ability to maintain quantum coherence while facing the challenge of state collapse under measurement. Here we use techniques from control theory and machine learning to predict the future evolution of a qubit’s state; we deploy this information to suppress stochastic, semiclassical decoherence, even when access to measurements is limited. First, we implement a time-division multiplexed approach, interleaving measurement periods with periods of unsupervised but stabilised operation during which qubits are available, for example, in quantum information experiments. Second, we employ predictive feedback during sequential but time delayed measurements to reduce the Dick effect as encountered in passive frequency standards. Both experiments demonstrate significant improvements in qubit-phase stability over ‘traditional’ measurement-based feedback approaches by exploiting time domain correlations in the noise processes. This technique requires no additional hardware and is applicable to all two-level quantum systems where projective measurements are possible.},
	language = {en},
	urldate = {2018-12-04},
	journal = {Nature Communications},
	author = {Mavadia, Sandeep and Frey, Virginia and Sastrawan, Jarrah and Dona, Stephen and Biercuk, Michael J.},
	month = jan,
	year = {2017},
	pages = {14106},
	file = {Full Text PDF:/Users/gcarleo/Zotero/storage/4PY7DQRE/Mavadia et al. - 2017 - Prediction and real-time compensation of qubit dec.pdf:application/pdf;Snapshot:/Users/gcarleo/Zotero/storage/3H262NVC/ncomms14106.html:text/html}
}

@article{torlai_neural-network_2018,
	title = {Neural-network quantum state tomography},
	volume = {14},
	copyright = {2018 The Author(s)},
	issn = {1745-2481},
	url = {https://www.nature.com/articles/s41567-018-0048-5},
	doi = {10.1038/s41567-018-0048-5},
	abstract = {Unsupervised machine learning techniques can efficiently perform quantum state tomography of large, highly entangled states with high accuracy, and allow the reconstruction of many-body quantities from simple experimentally accessible measurements.},
	language = {En},
	number = {5},
	urldate = {2018-11-14},
	journal = {Nature Physics},
	author = {Torlai, Giacomo and Mazzola, Guglielmo and Carrasquilla, Juan and Troyer, Matthias and Melko, Roger and Carleo, Giuseppe},
	month = may,
	year = {2018},
	pages = {447},
	file = {Full Text PDF:/Users/gcarleo/Zotero/storage/SC94ZG4H/Torlai et al. - 2018 - Neural-network quantum state tomography.pdf:application/pdf;Snapshot:/Users/gcarleo/Zotero/storage/V679X79A/s41567-018-0048-5.html:text/html}
}

@article{carrasquilla_reconstructing_2018,
  title={Reconstructing quantum states with generative models},
  author={Carrasquilla, Juan and Torlai, Giacomo and Melko, Roger G and Aolita, Leandro},
  journal={Nature Machine Intelligence},
  volume={1},
  number={3},
  pages={155},
  year={2019},
  publisher={Nature Publishing Group}
}


@article{gao_efficient_2017,
	title = {Efficient representation of quantum many-body states with deep neural networks},
	volume = {8},
	copyright = {2017 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-017-00705-2},
	doi = {10.1038/s41467-017-00705-2},
	abstract = {Part of the challenge for quantum many-body problems comes from the difficulty of representing large-scale quantum states, which in general requires an exponentially large number of parameters. Neural networks provide a powerful tool to represent quantum many-body states. An important open question is what characterizes the representational power of deep and shallow neural networks, which is of fundamental interest due to the popularity of deep learning methods. Here, we give a proof that, assuming a widely believed computational complexity conjecture, a deep neural network can efficiently represent most physical states, including the ground states of many-body Hamiltonians and states generated by quantum dynamics, while a shallow network representation with a restricted Boltzmann machine cannot efficiently represent some of those states. One of the challenges in studies of quantum many-body physics is finding an efficient way to record the large system wavefunctions. Here the authors present an analysis of the capabilities of recently-proposed neural network representations for storing physically accessible quantum states.},
	language = {En},
	number = {1},
	urldate = {2017-11-20},
	journal = {Nature Communications},
	author = {Gao, Xun and Duan, Lu-Ming},
	month = sep,
	year = {2017},
	pages = {662},
	file = {Full Text PDF:/Users/gcarleo/Zotero/storage/WJE4XP52/Gao and Duan - 2017 - Efficient representation of quantum many-body stat.pdf:application/pdf;Snapshot:/Users/gcarleo/Zotero/storage/DWCT87U9/s41467-017-00705-2.html:text/html}
}


@article{glasser_neural-network_2018,
	title = {Neural-{Network} {Quantum} {States}, {String}-{Bond} {States}, and {Chiral} {Topological} {States}},
	volume = {8},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.8.011006},
	doi = {10.1103/PhysRevX.8.011006},
	abstract = {Neural-network quantum states have recently been introduced as an Ansatz for describing the wave function of quantum many-body systems. We show that there are strong connections between neural-network quantum states in the form of restricted Boltzmann machines and some classes of tensor-network states in arbitrary dimensions. In particular, we demonstrate that short-range restricted Boltzmann machines are entangled plaquette states, while fully connected restricted Boltzmann machines are string-bond states with a nonlocal geometry and low bond dimension. These results shed light on the underlying architecture of restricted Boltzmann machines and their efficiency at representing many-body quantum states. String-bond states also provide a generic way of enhancing the power of neural-network quantum states and a natural generalization to systems with larger local Hilbert space. We compare the advantages and drawbacks of these different classes of states and present a method to combine them together. This allows us to benefit from both the entanglement structure of tensor networks and the efficiency of neural-network quantum states into a single Ansatz capable of targeting the wave function of strongly correlated systems. While it remains a challenge to describe states with chiral topological order using traditional tensor networks, we show that, because of their nonlocal geometry, neural-network quantum states and their string-bond-state extension can describe a lattice fractional quantum Hall state exactly. In addition, we provide numerical evidence that neural-network quantum states can approximate a chiral spin liquid with better accuracy than entangled plaquette states and local string-bond states. Our results demonstrate the efficiency of neural networks to describe complex quantum wave functions and pave the way towards the use of string-bond states as a tool in more traditional machine-learning applications.},
	number = {1},
	urldate = {2018-07-25},
	journal = {Physical Review X},
	author = {Glasser, Ivan and Pancotti, Nicola and August, Moritz and Rodriguez, Ivan D. and Cirac, J. Ignacio},
	month = jan,
	year = {2018},
	pages = {011006},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/BCKCLVH5/PhysRevX.8.html:text/html;Full Text PDF:/Users/gcarleo/Zotero/storage/ZQ7XQ8VE/Glasser et al. - 2018 - Neural-Network Quantum States, String-Bond States,.pdf:application/pdf}
}



@article{wang_discovering_2016,
	title = {Discovering phase transitions with unsupervised learning},
	volume = {94},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.94.195105},
	doi = {10.1103/PhysRevB.94.195105},
	abstract = {Unsupervised learning is a discipline of machine learning which aims at discovering patterns in large data sets or classifying the data into several categories without being trained explicitly. We show that unsupervised learning techniques can be readily used to identify phases and phases transitions of many-body systems. Starting with raw spin configurations of a prototypical Ising model, we use principal component analysis to extract relevant low-dimensional representations of the original data and use clustering analysis to identify distinct phases in the feature space. This approach successfully finds physical concepts such as the order parameter and structure factor to be indicators of a phase transition. We discuss the future prospects of discovering more complex phases and phase transitions using unsupervised learning techniques.},
	number = {19},
	urldate = {2018-11-14},
	journal = {Physical Review B},
	author = {Wang, Lei},
	month = nov,
	year = {2016},
	pages = {195105},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/7FDCC3UP/PhysRevB.94.html:text/html}
}

@article{chen_equivalence_2018,
	title = {Equivalence of restricted {Boltzmann} machines and tensor network states},
	volume = {97},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.97.085104},
	doi = {10.1103/PhysRevB.97.085104},
	abstract = {The restricted Boltzmann machine (RBM) is one of the fundamental building blocks of deep learning. RBM finds wide applications in dimensional reduction, feature extraction, and recommender systems via modeling the probability distributions of a variety of input data including natural images, speech signals, and customer ratings, etc. We build a bridge between RBM and tensor network states (TNS) widely used in quantum many-body physics research. We devise efficient algorithms to translate an RBM into the commonly used TNS. Conversely, we give sufficient and necessary conditions to determine whether a TNS can be transformed into an RBM of given architectures. Revealing these general and constructive connections can cross fertilize both deep learning and quantum many-body physics. Notably, by exploiting the entanglement entropy bound of TNS, we can rigorously quantify the expressive power of RBM on complex data sets. Insights into TNS and its entanglement capacity can guide the design of more powerful deep learning architectures. On the other hand, RBM can represent quantum many-body states with fewer parameters compared to TNS, which may allow more efficient classical simulations.},
	number = {8},
	urldate = {2018-02-07},
	journal = {Physical Review B},
	author = {Chen, Jing and Cheng, Song and Xie, Haidong and Wang, Lei and Xiang, Tao},
	month = feb,
	year = {2018},
	pages = {085104},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/7GWKLXCW/PhysRevB.97.html:text/html}
}


@article{choo_symmetries_2018,
	title = {Symmetries and {Many}-{Body} {Excitations} with {Neural}-{Network} {Quantum} {States}},
	volume = {121},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.121.167204},
	doi = {10.1103/PhysRevLett.121.167204},
	abstract = {Artificial neural networks have been recently introduced as a general ansatz to represent many-body wave functions. In conjunction with variational Monte Carlo calculations, this ansatz has been applied to find Hamiltonian ground states and their energies. Here, we provide extensions of this method to study excited states, a central task in several many-body quantum calculations. First, we give a prescription that allows us to target eigenstates of a (nonlocal) symmetry of the Hamiltonian. Second, we give an algorithm to compute low-lying excited states without symmetries. We demonstrate our approach with both restricted Boltzmann machines and feed-forward neural networks. Results are shown for the one-dimensional spin-1/2 Heisenberg model, and for the one-dimensional Bose-Hubbard model. When comparing to exact results, we obtain good agreement for a large range of excited-states energies. Interestingly, we find that deep networks typically outperform shallow architectures for high-energy states.},
	number = {16},
	urldate = {2018-11-14},
	journal = {Physical Review Letters},
	author = {Choo, Kenny and Carleo, Giuseppe and Regnault, Nicolas and Neupert, Titus},
	month = oct,
	year = {2018},
	pages = {167204},
}


@article{zheng_restricted_2018,
	title = {Restricted {Boltzmann} {Machines} and {Matrix} {Product} {States} of 1D {Translational} {Invariant} {Stabilizer} {Codes}},
	url = {http://arxiv.org/abs/1812.08171},
	abstract = {We discuss the relations between restricted Boltzmann machine (RBM) states and the matrix product states (MPS) for the ground states of 1D translational invariant stabilizer codes. A generic translational invariant and finitely connected RBM state can be expressed as an MPS, and the matrices of the resulting MPS are of rank 1. We dub such an MPS as an RBM-MPS. This provides a necessary condition for exactly realizing a quantum state as an RBM state, if the quantum state can be written as an MPS. For generic 1D stabilizer codes having a non-degenerate ground state with periodic boundary condition, we obtain an expression for the lower bound of their MPS bond dimension, and an upper bound for the rank of their MPS matrices. In terms of RBM, we provide an algorithm to derive the RBM for the cocycle Hamiltonians whose MPS matrices are proved to be of rank 1. Moreover, the RBM-MPS produced by our algorithm has the minimal bond dimension. A family of examples is provided to explain the algorithm. We finally conjecture that these features hold true for all the 1D stabilizer codes having a non-degenerate ground state with periodic boundary condition, as long as their MPS matrices are of rank 1.},
	journal = {arXiv:1812.08171},
	author = {Zheng, Yunqin and He, Huan and Regnault, Nicolas and Bernevig, B. Andrei},
	year = {2018},
	keywords = {Condensed Matter - Strongly Correlated Electrons, Quantum Physics},
}


@article{wetzel_unsupervised_2017,
	title = {Unsupervised learning of phase transitions: {From} principal component analysis to variational autoencoders},
	volume = {96},
	shorttitle = {Unsupervised learning of phase transitions},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.96.022140},
	doi = {10.1103/PhysRevE.96.022140},
	abstract = {We examine unsupervised machine learning techniques to learn features that best describe configurations of the two-dimensional Ising model and the three-dimensional XY model. The methods range from principal component analysis over manifold and clustering methods to artificial neural-network-based variational autoencoders. They are applied to Monte Carlo–sampled configurations and have, a priori, no knowledge about the Hamiltonian or the order parameter. We find that the most promising algorithms are principal component analysis and variational autoencoders. Their predicted latent parameters correspond to the known order parameters. The latent representations of the models in question are clustered, which makes it possible to identify phases without prior knowledge of their existence. Furthermore, we find that the reconstruction loss function can be used as a universal identifier for phase transitions.},
	number = {2},
	urldate = {2018-11-14},
	journal = {Physical Review E},
	author = {Wetzel, Sebastian J.},
	month = aug,
	year = {2017},
	pages = {022140},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/BBVUUX3E/PhysRevE.96.html:text/html}
}


@article{deng_machine_2017,
	title = {Machine learning topological states},
	volume = {96},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.96.195145},
	doi = {10.1103/PhysRevB.96.195145},
	abstract = {Artificial neural networks and machine learning have now reached a new era after several decades of improvement where applications are to explode in many fields of science, industry, and technology. Here, we use artificial neural networks to study an intriguing phenomenon in quantum physics—the topological phases of matter. We find that certain topological states, either symmetry-protected or with intrinsic topological order, can be represented with classical artificial neural networks. This is demonstrated by using three concrete spin systems, the one-dimensional (1D) symmetry-protected topological cluster state and the 2D and 3D toric code states with intrinsic topological orders. For all three cases, we show rigorously that the topological ground states can be represented by short-range neural networks in an exact and efficient fashion—the required number of hidden neurons is as small as the number of physical spins and the number of parameters scales only linearly with the system size. For the 2D toric-code model, we find that the proposed short-range neural networks can describe the excited states with Abelian anyons and their nontrivial mutual statistics as well. In addition, by using reinforcement learning we show that neural networks are capable of finding the topological ground states of nonintegrable Hamiltonians with strong interactions and studying their topological phase transitions. Our results demonstrate explicitly the exceptional power of neural networks in describing topological quantum states, and at the same time provide valuable guidance to machine learning of topological phases in generic lattice models.},
	number = {19},
	urldate = {2018-02-12},
	journal = {Physical Review B},
	author = {Deng, Dong-Ling and Li, Xiaopeng and Das Sarma, S.},
	month = nov,
	year = {2017},
	pages = {195145},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/9VFZ3UTH/Deng et al. - 2017 - Machine learning topological states.html:text/html}
}


@article{deng_quantum_2017,
	title = {Quantum {Entanglement} in {Neural} {Network} {States}},
	volume = {7},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.7.021021},
	doi = {10.1103/PhysRevX.7.021021},
	abstract = {Machine learning, one of today’s most rapidly growing interdisciplinary fields, promises an unprecedented perspective for solving intricate quantum many-body problems. Understanding the physical aspects of the representative artificial neural-network states has recently become highly desirable in the applications of machine-learning techniques to quantum many-body physics. In this paper, we explore the data structures that encode the physical features in the network states by studying the quantum entanglement properties, with a focus on the restricted-Boltzmann-machine (RBM) architecture. We prove that the entanglement entropy of all short-range RBM states satisfies an area law for arbitrary dimensions and bipartition geometry. For long-range RBM states, we show by using an exact construction that such states could exhibit volume-law entanglement, implying a notable capability of RBM in representing quantum states with massive entanglement. Strikingly, the neural-network representation for these states is remarkably efficient, in the sense that the number of nonzero parameters scales only linearly with the system size. We further examine the entanglement properties of generic RBM states by randomly sampling the weight parameters of the RBM. We find that their averaged entanglement entropy obeys volume-law scaling, and the meantime strongly deviates from the Page entropy of the completely random pure states. We show that their entanglement spectrum has no universal part associated with random matrix theory and bears a Poisson-type level statistics. Using reinforcement learning, we demonstrate that RBM is capable of finding the ground state (with power-law entanglement) of a model Hamiltonian with a long-range interaction. In addition, we show, through a concrete example of the one-dimensional symmetry-protected topological cluster states, that the RBM representation may also be used as a tool to analytically compute the entanglement spectrum. Our results uncover the unparalleled power of artificial neural networks in representing quantum many-body states regardless of how much entanglement they possess, which paves a novel way to bridge computer-science-based machine-learning techniques to outstanding quantum condensed-matter physics problems.},
	number = {2},
	urldate = {2017-08-23},
	journal = {Physical Review X},
	author = {Deng, Dong-Ling and Li, Xiaopeng and Das Sarma, S.},
	month = may,
	year = {2017},
	pages = {021021},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/YSFW9FHW/PhysRevX.7.html:text/html;Full Text PDF:/Users/gcarleo/Zotero/storage/F3F9GMLB/Deng et al. - 2017 - Quantum Entanglement in Neural Network States.pdf:application/pdf}
}


@article{cai_approximating_2018,
	title = {Approximating quantum many-body wave functions using artificial neural networks},
	volume = {97},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.97.035116},
	doi = {10.1103/PhysRevB.97.035116},
	abstract = {In this paper, we demonstrate the expressibility of artificial neural networks (ANNs) in quantum many-body physics by showing that a feed-forward neural network with a small number of hidden layers can be trained to approximate with high precision the ground states of some notable quantum many-body systems. We consider the one-dimensional free bosons and fermions, spinless fermions on a square lattice away from half-filling, as well as frustrated quantum magnetism with a rapidly oscillating ground-state characteristic function. In the latter case, an ANN with a standard architecture fails, while that with a slightly modified one successfully learns the frustration-induced complex sign rule in the ground state and approximates the ground states with high precisions. As an example of practical use of our method, we also perform the variational method to explore the ground state of an antiferromagnetic J1−J2 Heisenberg model.},
	number = {3},
	urldate = {2018-11-14},
	journal = {Physical Review B},
	author = {Cai, Zi and Liu, Jinguo},
	month = jan,
	year = {2018},
	pages = {035116},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/UH76TTJW/PhysRevB.97.html:text/html}
}


@article{hornik_approximation_1991,
	title = {Approximation capabilities of multilayer feedforward networks},
	volume = {4},
	issn = {0893-6080},
	doi = {10.1016/0893-6080(91)90009-T},
	abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.},
	number = {2},
	urldate = {2016-04-18},
	journal = {Neural Networks},
	author = {Hornik, Kurt},
	year = {1991},
	keywords = {Activation function, Input environment measure, Lp(μ) approximation, Multilayer feedforward networks, Smooth approximation, Sobolev spaces, Uniform approximation, Universal approximation capabilities},
	pages = {251--257},
	file = {ScienceDirect Snapshot:/Users/gcarleo/Zotero/storage/XCXQZGEI/Hornik - 1991 - Approximation capabilities of multilayer feedforwa.html:text/html}
}

@article{luo_backflow_2018,
	title = {Backflow {Transformations} via {Neural} {Networks} for {Quantum} {Many}-{Body} {Wave}-{Functions}},
	url = {http://arxiv.org/abs/1807.10770},
	abstract = {Obtaining an accurate ground state wave function is one of the great challenges in the quantum many-body problem. In this paper, we propose a new class of wave functions, neural network backflow (NNB). The backflow approach, pioneered originally by Feynman, adds correlation to a mean-field ground state by transforming the single-particle orbitals in a configuration-dependent way. NNB uses a feed-forward neural network to find the optimal transformation. NNB directly dresses a mean-field state, can be systematically improved and directly alters the sign structure of the wave-function. It generalizes the standard backflow which we show how to explicitly represent as a NNB. We benchmark the NNB on a Hubbard model at intermediate doping finding that it significantly decreases the relative error, restores the symmetry of both observables and single-particle orbitals, and decreases the double-occupancy density. Finally, we illustrate interesting patterns in the weights and bias of the optimized neural network.},
	urldate = {2018-09-14},
	journal = {arXiv:1807.10770 [cond-mat, physics:physics]},
	author = {Luo, Di and Clark, Bryan K.},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.10770},
	keywords = {Condensed Matter - Strongly Correlated Electrons, Condensed Matter - Disordered Systems and Neural Networks, Physics - Computational Physics},
	file = {arXiv\:1807.10770 PDF:/Users/gcarleo/Zotero/storage/9XK7Z8TQ/Luo e Clark - 2018 - Backflow Transformations via Neural Networks for Q.pdf:application/pdf;arXiv.org Snapshot:/Users/gcarleo/Zotero/storage/XTVXD92C/1807.html:text/html}
}


@article{nomura_restricted_2017,
	title = {Restricted {Boltzmann} machine learning for solving strongly correlated quantum systems},
	volume = {96},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.96.205152},
	doi = {10.1103/PhysRevB.96.205152},
	abstract = {We develop a machine learning method to construct accurate ground-state wave functions of strongly interacting and entangled quantum spin as well as fermionic models on lattices. A restricted Boltzmann machine algorithm in the form of an artificial neural network is combined with a conventional variational Monte Carlo method with pair product (geminal) wave functions and quantum number projections. The combination allows an application of the machine learning scheme to interacting fermionic systems. The combined method substantially improves the accuracy beyond that ever achieved by each method separately, in the Heisenberg as well as Hubbard models on square lattices, thus proving its power as a highly accurate quantum many-body solver.},
	number = {20},
	urldate = {2018-11-14},
	journal = {Physical Review B},
	author = {Nomura, Yusuke and Darmawan, Andrew S. and Yamaji, Youhei and Imada, Masatoshi},
	month = nov,
	year = {2017},
	pages = {205152},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/X2RF8GN4/PhysRevB.96.html:text/html;Submitted Version:/Users/gcarleo/Zotero/storage/LQHMGESM/Nomura et al. - 2017 - Restricted Boltzmann machine learning for solving .pdf:application/pdf}
}


@article{han_solving_2018,
	title = {Solving {Many}-{Electron} {Schr}{\textbackslash}"odinger {Equation} {Using} {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1807.07014},
	abstract = {We introduce a new family of trial wave-functions based on deep neural networks to solve the many-electron Sch{\textbackslash}"rondinger equation. The Pauli exclusion principle is dealt with explicitly to ensure that the trial wave-functions are physical. The optimal trial wave-function is obtained through variational Monte Carlo and the computational cost scales quadratically with the number of electrons. The algorithm does not make use of any prior knowledge such as atomic orbitals. Yet it is able to represent accurately the ground-states of the tested systems, including He, H2, Be, B, LiH, and a chain of 10 hydrogen atoms. This opens up new possibilities for solving large-scale many-electron Schr{\textbackslash}"ondinger equation.},
	urldate = {2018-09-14},
	journal = {arXiv:1807.07014 [physics]},
	author = {Han, Jiequn and Zhang, Linfeng and E, Weinan},
	month = jul,
	year = {2018},
	keywords = {Physics - Chemical Physics, Physics - Computational Physics},
	file = {arXiv\:1807.07014 PDF:/Users/gcarleo/Zotero/storage/JPHYEAW7/Han et al. - 2018 - Solving Many-Electron Schrodinger Equation Using.pdf:application/pdf;arXiv.org Snapshot:/Users/gcarleo/Zotero/storage/WNDVBWFS/1807.html:text/html}
}


@article{saito_solving_2017,
	title = {Solving the {Bose}–{Hubbard} {Model} with {Machine} {Learning}},
	volume = {86},
	issn = {0031-9015},
	url = {http://journals.jps.jp/doi/10.7566/JPSJ.86.093001},
	doi = {10.7566/JPSJ.86.093001},
	abstract = {Motivated by the recent successful application of artificial neural networks to quantum many-body problems [G. Carleo and M. Troyer, Science 355, 602 (2017)], a method to calculate the ground state of the Bose–Hubbard model using a feedforward neural network is proposed. The results are in good agreement with those obtained by exact diagonalization and the Gutzwiller approximation. The method of neural-network quantum states is promising for solving quantum many-body problems of ultracold atoms in optical lattices.},
	number = {9},
	urldate = {2017-08-21},
	journal = {Journal of the Physical Society of Japan},
	author = {Saito, Hiroki},
	month = jul,
	year = {2017},
	pages = {093001},
	file = {Snapshot:/Users/gcarleo/Zotero/storage/ZCKNZ6GQ/Saito - 2017 - Solving the Bose–Hubbard Model with Machine Learni.html:text/html}
}


@article{saito_machine_2017,
	title = {Machine {Learning} {Technique} to {Find} {Quantum} {Many}-{Body} {Ground} {States} of {Bosons} on a {Lattice}},
	volume = {87},
	issn = {0031-9015},
	url = {http://journals.jps.jp/doi/10.7566/JPSJ.87.014001},
	doi = {10.7566/JPSJ.87.014001},
	abstract = {We have developed a variational method to obtain many-body ground states of the Bose–Hubbard model using feedforward artificial neural networks. A fully connected network with a single hidden layer works better than a fully connected network with multiple hidden layers, and a multilayer convolutional network is more efficient than a fully connected network. AdaGrad and Adam are optimization methods that work well. Moreover, we show that many-body ground states with different numbers of particles can be generated by a single network.},
	number = {1},
	urldate = {2018-02-12},
	journal = {Journal of the Physical Society of Japan},
	author = {Saito, Hiroki and Kato, Masaya},
	month = dec,
	year = {2017},
	pages = {014001},
	file = {Full Text PDF:/Users/gcarleo/Zotero/storage/P7IMSLXL/Saito and Kato - 2017 - Machine Learning Technique to Find Quantum Many-Bo.pdf:application/pdf;Snapshot:/Users/gcarleo/Zotero/storage/VR7FL7ZR/JPSJ.87.html:text/html}
}


@article{borin_approximating_2019,
	title = {Approximating power of machine-learning ansatz for quantum many-body states},
	url = {http://arxiv.org/abs/1901.08615},
	journal = {arXiv:1901.08615 [cond-mat, physics:quant-ph]},
	author = {Borin, Artem and Abanin, Dmitry A.},
	month = jan,
	year = {2019},
}


@article{teng_machine-learning_2018,
	title = {Machine-learning quantum mechanics: {Solving} quantum mechanics problems using radial basis function networks},
	volume = {98},
	shorttitle = {Machine-learning quantum mechanics},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.98.033305},
	doi = {10.1103/PhysRevE.98.033305},
	abstract = {In this article, machine-learning methods are used to solve quantum mechanics problems. The radial basis function network in a discrete basis is used as the variational wave function for the ground state of a quantum system. Variational Monte Carlo (VMC) calculations are carried out for some simple Hamiltonians. The results are in good agreement with theoretical values. The smallest eigenvalue of a Hermitian matrix can also be acquired using VMC calculations. Results are provided to demonstrate that machine-learning techniques are capable of solving quantum mechanical problems.},
	number = {3},
	urldate = {2018-11-14},
	journal = {Physical Review E},
	author = {Teng, Peiyuan},
	month = sep,
	year = {2018},
	pages = {033305},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/QYGDAWFH/PhysRevE.98.html:text/html;Submitted Version:/Users/gcarleo/Zotero/storage/HIRHD9IC/Teng - 2018 - Machine-learning quantum mechanics Solving quantu.pdf:application/pdf}
}


@article{czischek_quenches_2018,
	title = {Quenches near {Ising} quantum criticality as a challenge for artificial neural networks},
	volume = {98},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.98.024311},
	doi = {10.1103/PhysRevB.98.024311},
	abstract = {The near-critical unitary dynamics of quantum Ising spin chains in transversal and longitudinal magnetic fields is studied using an artificial neural network representation of the wave function. A focus is set on strong spatial correlations which build up in the system following a quench into the vicinity of the quantum critical point. We compare correlations obtained by optimizing the parameters of the network states with analytical solutions in integrable cases and time-dependent density matrix renormalization group (tDMRG) simulations, as well as with predictions from a semiclassical discrete truncated Wigner analysis. While the semiclassical approach yields quantitatively correct results only at very short times and near zero transverse fields, the neural-network representation is applicable in a much wider regime. However, for quenches close to the quantum critical point the representation becomes inefficient. For nonintegrable models we show that in regimes where tDMRG is limited to short times due to extensive entanglement growth, also the neural-network parametrization converges only at short times.},
	number = {2},
	urldate = {2018-11-14},
	journal = {Physical Review B},
	author = {Czischek, Stefanie and Gärttner, Martin and Gasenzer, Thomas},
	month = jul,
	year = {2018},
	pages = {024311},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/5BDQDCY3/PhysRevB.98.html:text/html}
}

@article{ciliberto2018quantum,
 title={Quantum machine learning: a classical perspective},
 author={Ciliberto, Carlo and Herbster, Mark and Ialongo, Alessandro Davide and Pontil, Massimiliano and Rocchetto, Andrea and Severini, Simone and Wossnig, Leonard},
 journal={Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
 volume={474},
 number={2209},
 pages={20170551},
 year={2018},
 publisher={The Royal Society Publishing}
}

@article{haah2017sample,
 title={Sample-optimal tomography of quantum states},
 author={Haah, Jeongwan and Harrow, Aram W and Ji, Zhengfeng and Wu, Xiaodi and Yu, Nengkun},
 journal={IEEE Transactions on Information Theory},
 volume={63},
 number={9},
 pages={5628--5641},
 year={2017},
 publisher={IEEE}
}

@inproceedings{o2016efficient,
 title={Efficient quantum tomography},
 author={O'Donnell, Ryan and Wright, John},
 booktitle={Proceedings of the forty-eighth annual ACM symposium on Theory of Computing},
 pages={899--912},
 year={2016},
 organization={ACM}
}

@article{aaronson2007learnability,
 title={The learnability of quantum states},
 author={Aaronson, Scott},
 journal={Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
 volume={463},
 number={2088},
 pages={3089--3114},
 year={2007},
 publisher={The Royal Society London}
}


@article{rocchetto2018stabiliser,
 title={Stabiliser states are efficiently {PAC}-learnable},
 author={Rocchetto, Andrea},
 journal={Quantum Information and Computation},
 volume={18},
 number={7\&8},
 year={2018}
}

@article{lu_efficient_2018,
	title = {Efficient {Representation} of {Topologically} {Ordered} {States} with {Restricted} {Boltzmann} {Machines}},
	url = {http://arxiv.org/abs/1810.02352},
	abstract = {Representation by neural networks, in particular by restricted Boltzmann machines (RBM), has provided a powerful computational tool to solve quantum many-body problems. An important open question is how to characterize which class of quantum states can be efficiently represented with the RBM. Here, we show that the RBM can efficiently represent a wide class of many-body entangled states with rich exotic topological orders. This includes: (1) ground states of double semion and twisted quantum double models with intrinsic topological orders; (2) states of the AKLT model and 2D CZX model with symmetry protected topological order; (3) states of Haah code model with fracton topological order; (4) generalized stabilizer states and hypergraph states that are important for quantum information protocols. One twisted quantum double model state considered here harbors non-abelian anyon excitations. Our result shows that it is possible to study a variety of quantum models with exotic topological orders and rich physics using the RBM computational toolbox.},
	urldate = {2018-11-14},
	journal = {arXiv:1810.02352 [cond-mat, physics:quant-ph]},
	author = {Lu, Sirui and Gao, X. and Duan, L.-M.},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.02352},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Strongly Correlated Electrons, Quantum Physics},
	annote = {Comment: 10 pages, 4 figures},
	file = {arXiv\:1810.02352 PDF:/Users/gcarleo/Zotero/storage/TVU6B848/Lu et al. - 2018 - Efficient Representation of Topologically Ordered .pdf:application/pdf;arXiv.org Snapshot:/Users/gcarleo/Zotero/storage/SLWHE4MQ/1810.html:text/html}
}


@article{inack_projective_2018,
	title = {Projective quantum {Monte} {Carlo} simulations guided by unrestricted neural network states},
	url = {http://arxiv.org/abs/1809.03562},
	abstract = {We investigate the use of variational wave-functions that mimic stochastic recurrent neural networks, specifically, unrestricted Boltzmann machines, as guiding functions in projective quantum Monte Carlo (PQMC) simulations of quantum spin models. As a preliminary step, we investigate the accuracy of such unrestricted neural network states as variational Ans{\textbackslash}"atze for the ground state of the ferromagnetic quantum Ising chain. We find that by optimizing just three variational parameters, independently on the system size, accurate ground-state energies are obtained, comparable to those previously obtained using restricted Boltzmann machines with few variational parameters per spin. Chiefly, we show that if one uses optimized unrestricted neural network states as guiding functions for importance sampling the efficiency of the PQMC algorithms is greatly enhanced, drastically reducing the most relevant systematic bias, namely that due to the finite random-walker population. The scaling of the computational cost with the system size changes from the exponential scaling characteristic of PQMC simulations performed without importance sampling, to a polynomial scaling, even at the ferromagnetic quantum critical point. The important role of the protocol chosen to sample hidden-spins configurations, in particular at the critical point, is analyzed. We discuss the implications of these findings for what concerns the problem of simulating adiabatic quantum optimization using stochastic algorithms on classical computers.},
	urldate = {2018-11-14},
	journal = {arXiv:1809.03562 [cond-mat, physics:physics]},
	author = {Inack, E. M. and Santoro, G. E. and Dell'Anna, L. and Pilati, S.},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.03562},
	keywords = {Condensed Matter - Statistical Mechanics, Physics - Computational Physics},
	annote = {Comment: 10 pages, 6 figures},
	file = {arXiv\:1809.03562 PDF:/Users/gcarleo/Zotero/storage/BJXMDIZY/Inack et al. - 2018 - Projective quantum Monte Carlo simulations guided .pdf:application/pdf;arXiv.org Snapshot:/Users/gcarleo/Zotero/storage/N3RX5YS7/1809.html:text/html}
}

@article{zhang_machine_2018,
	title = {Machine {Learning} {Topological} {Invariants} with {Neural} {Networks}},
	volume = {120},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.120.066401},
	doi = {10.1103/PhysRevLett.120.066401},
	number = {6},
	urldate = {2019-03-14},
	journal = {Physical Review Letters},
	author = {Zhang, Pengfei and Shen, Huitao and Zhai, Hui},
	month = feb,
	year = {2018},
	pages = {066401},
}


@article{zhang_machine_2017,
	title = {Machine learning \$\{{\textbackslash}mathbb\{{Z}\}\}\_\{2\}\$ quantum spin liquids with quasiparticle statistics},
	volume = {96},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.96.245119},
	doi = {10.1103/PhysRevB.96.245119},
	abstract = {After decades of progress and effort, obtaining a phase diagram for a strongly correlated topological system still remains a challenge. Although in principle one could turn to Wilson loops and long-range entanglement, evaluating these nonlocal observables at many points in phase space can be prohibitively costly. With growing excitement over topological quantum computation comes the need for an efficient approach for obtaining topological phase diagrams. Here we turn to machine learning using quantum loop topography (QLT), a notion we have recently introduced. Specifically, we propose a construction of QLT that is sensitive to quasiparticle statistics. We then use mutual statistics between the spinons and visons to detect a Z2 quantum spin liquid in a multiparameter phase space. We successfully obtain the quantum phase boundary between the topological and trivial phases using a simple feed-forward neural network. Furthermore, we demonstrate advantages of our approach for the evaluation of phase diagrams relating to speed and storage. Such statistics-based machine learning of topological phases opens new efficient routes to studying topological phase diagrams in strongly correlated systems.},
	number = {24},
	urldate = {2019-03-14},
	journal = {Physical Review B},
	author = {Zhang, Yi and Melko, Roger G. and Kim, Eun-Ah},
	month = dec,
	year = {2017},
	pages = {245119},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/2NEHPVQQ/PhysRevB.96.html:text/html;Full Text PDF:/Users/gcarleo/Zotero/storage/EXEKBG2I/Zhang et al. - 2017 - Machine learning \$ mathbb Z _ 2 \$ quantum spin l.pdf:application/pdf}
}

@article{van_nieuwenburg_learning_2018,
	title = {Learning phase transitions from dynamics},
	volume = {98},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.98.060301},
	doi = {10.1103/PhysRevB.98.060301},
	number = {6},
	urldate = {2019-03-14},
	journal = {Physical Review B},
	author = {van Nieuwenburg, Evert and Bairey, Eyal and Refael, Gil},
	month = aug,
	year = {2018},
	pages = {060301}
}
@article{rem_identifying_2018,
	title = {Identifying {Quantum} {Phase} {Transitions} using {Artificial} {Neural} {Networks} on {Experimental} {Data}},
	url = {http://arxiv.org/abs/1809.05519},
	abstract = {Machine learning techniques such as artificial neural networks are currently revolutionizing many technological areas and have also proven successful in quantum physics applications. Here we employ an artificial neural network and deep learning techniques to identify quantum phase transitions from single-shot experimental momentum-space density images of ultracold quantum gases and obtain results, which were not feasible with conventional methods. We map out the complete two-dimensional topological phase diagram of the Haldane model and provide an accurate characterization of the superfluid-to-Mott-insulator transition in an inhomogeneous Bose-Hubbard system. Our work points the way to unravel complex phase diagrams of general experimental systems, where the Hamiltonian and the order parameters might not be known.},
	urldate = {2018-11-14},
	journal = {arXiv:1809.05519 [cond-mat, physics:quant-ph]},
	author = {Rem, Benno S. and Käming, Niklas and Tarnowski, Matthias and Asteria, Luca and Fläschner, Nick and Becker, Christoph and Sengstock, Klaus and Weitenberg, Christof},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.05519},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Mesoscale and Nanoscale Physics, Condensed Matter - Quantum Gases, Quantum Physics},
	annote = {Comment: 20 pages, 10 figures},
	file = {arXiv\:1809.05519 PDF:/Users/gcarleo/Zotero/storage/J7RDMJC6/Rem et al. - 2018 - Identifying Quantum Phase Transitions using Artifi.pdf:application/pdf;arXiv.org Snapshot:/Users/gcarleo/Zotero/storage/Y827JYLI/1809.html:text/html}
}


@article{courtin_analysis_2000,
	title = {Analysis of magnetic domain patterns by a perceptron neural network},
	volume = {50},
	issn = {0295-5075},
	url = {http://iopscience.iop.org/article/10.1209/epl/i2000-00240-x/meta},
	doi = {10.1209/epl/i2000-00240-x},
	language = {en},
	number = {1},
	urldate = {2018-11-14},
	journal = {EPL (Europhysics Letters)},
	author = {Courtin, S. and Padovani, S.},
	month = apr,
	year = {2000},
	pages = {94},
	file = {Snapshot:/Users/gcarleo/Zotero/storage/WLRNA992/meta.html:text/html}
}


@article{huang_accelerated_2017,
	title = {Accelerated {Monte} {Carlo} simulations with restricted {Boltzmann} machines},
	volume = {95},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.95.035105},
	doi = {10.1103/PhysRevB.95.035105},
	abstract = {Despite their exceptional flexibility and popularity, Monte Carlo methods often suffer from slow mixing times for challenging statistical physics problems. We present a general strategy to overcome this difficulty by adopting ideas and techniques from the machine learning community. We fit the unnormalized probability of the physical model to a feed-forward neural network and reinterpret the architecture as a restricted Boltzmann machine. Then, exploiting its feature detection ability, we utilize the restricted Boltzmann machine to propose efficient Monte Carlo updates to speed up the simulation of the original physical system. We implement these ideas for the Falicov-Kimball model and demonstrate an improved acceptance ratio and autocorrelation time near the phase transition point.},
	number = {3},
	urldate = {2018-11-14},
	journal = {Physical Review B},
	author = {Huang, Li and Wang, Lei},
	month = jan,
	year = {2017},
	pages = {035105},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/4G3K8GSX/PhysRevB.95.html:text/html;Submitted Version:/Users/gcarleo/Zotero/storage/94ILIXPL/Huang and Wang - 2017 - Accelerated Monte Carlo simulations with restricte.pdf:application/pdf}
}


@article{carrasquilla_machine_2017,
	title = {Machine learning phases of matter},
	volume = {13},
	copyright = {2017 Nature Publishing Group},
	issn = {1745-2481},
	url = {https://www.nature.com/articles/nphys4035},
	doi = {10.1038/nphys4035},
	abstract = {Condensed-matter physics is the study of the collective behaviour of infinitely complex assemblies of electrons, nuclei, magnetic moments, atoms or qubits1. This complexity is reflected in the size of the state space, which grows exponentially with the number of particles, reminiscent of the ‘curse of dimensionality’ commonly encountered in machine learning2. Despite this curse, the machine learning community has developed techniques with remarkable abilities to recognize, classify, and characterize complex sets of data. Here, we show that modern machine learning architectures, such as fully connected and convolutional neural networks3, can identify phases and phase transitions in a variety of condensed-matter Hamiltonians. Readily programmable through modern software libraries4,5, neural networks can be trained to detect multiple types of order parameter, as well as highly non-trivial states with no conventional order, directly from raw state configurations sampled with Monte Carlo6,7.},
	language = {en},
	number = {5},
	urldate = {2018-11-14},
	journal = {Nature Physics},
	author = {Carrasquilla, Juan and Melko, Roger G.},
	month = may,
	year = {2017},
	pages = {431--434},
	file = {Full Text PDF:/Users/gcarleo/Zotero/storage/SR5VCIB4/Carrasquilla and Melko - 2017 - Machine learning phases of matter.pdf:application/pdf;Snapshot:/Users/gcarleo/Zotero/storage/UX7A5DAQ/nphys4035.html:text/html}
}


@article{torlai_learning_2016,
	title = {Learning thermodynamics with {Boltzmann} machines},
	volume = {94},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.94.165134},
	doi = {10.1103/PhysRevB.94.165134},
	abstract = {A Boltzmann machine is a stochastic neural network that has been extensively used in the layers of deep architectures for modern machine learning applications. In this paper, we develop a Boltzmann machine that is capable of modeling thermodynamic observables for physical systems in thermal equilibrium. Through unsupervised learning, we train the Boltzmann machine on data sets constructed with spin configurations importance sampled from the partition function of an Ising Hamiltonian at different temperatures using Monte Carlo (MC) methods. The trained Boltzmann machine is then used to generate spin states, for which we compare thermodynamic observables to those computed by direct MC sampling. We demonstrate that the Boltzmann machine can faithfully reproduce the observables of the physical system. Further, we observe that the number of neurons required to obtain accurate results increases as the system is brought close to criticality.},
	number = {16},
	urldate = {2018-11-14},
	journal = {Physical Review B},
	author = {Torlai, Giacomo and Melko, Roger G.},
	month = oct,
	year = {2016},
	pages = {165134},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/4CSP56VU/PhysRevB.94.html:text/html;Submitted Version:/Users/gcarleo/Zotero/storage/58WTZTQK/Torlai and Melko - 2016 - Learning thermodynamics with Boltzmann machines.pdf:application/pdf}
}


@article{liu_self-learning_2017,
	title = {Self-learning {Monte} {Carlo} method},
	volume = {95},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.95.041101},
	doi = {10.1103/PhysRevB.95.041101},
	abstract = {Monte Carlo simulation is an unbiased numerical tool for studying classical and quantum many-body systems. One of its bottlenecks is the lack of a general and efficient update algorithm for large size systems close to the phase transition, for which local updates perform badly. In this Rapid Communication, we propose a general-purpose Monte Carlo method, dubbed self-learning Monte Carlo (SLMC), in which an efficient update algorithm is first learned from the training data generated in trial simulations and then used to speed up the actual simulation. We demonstrate the efficiency of SLMC in a spin model at the phase transition point, achieving a 10–20 times speedup.},
	number = {4},
	urldate = {2018-11-14},
	journal = {Physical Review B},
	author = {Liu, Junwei and Qi, Yang and Meng, Zi Yang and Fu, Liang},
	month = jan,
	year = {2017},
	pages = {041101}
}


@article{zhang_interpretable_2019,
	title = {Interpretable machine learning study of the many-body localization transition in disordered quantum {Ising} spin chains},
	volume = {99},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.99.054208},
	doi = {10.1103/PhysRevB.99.054208},
	number = {5},
	urldate = {2019-03-14},
	journal = {Physical Review B},
	author = {Zhang, Wei and Wang, Lei and Wang, Ziqiang},
	month = feb,
	year = {2019},
	pages = {054208},
}

@article{hsu_machine_2018,
	title = {Machine {Learning} {Many}-{Body} {Localization}: {Search} for the {Elusive} {Nonergodic} {Metal}},
	volume = {121},
	shorttitle = {Machine {Learning} {Many}-{Body} {Localization}},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.121.245701},
	doi = {10.1103/PhysRevLett.121.245701},
	number = {24},
	urldate = {2019-03-14},
	journal = {Physical Review Letters},
	author = {Hsu, Yi-Ting and Li, Xiao and Deng, Dong-Ling and Das Sarma, S.},
	year = {2018},
	pages = {245701},
}


@article{rodriguez-nieva_identifying_2018,
	title = {Identifying topological order via unsupervised machine learning},
	url = {http://arxiv.org/abs/1805.05961},
	abstract = {Machine learning of topological phase transitions has proven to be challenging due to their inherent non-local nature. We propose an unsupervised approach based on diffusion maps that learns topological phase transitions from raw data without the need of manual feature engineering. Using bare spin configurations as input, the approach is shown to be capable of classifying samples of the two-dimensional XY model by winding number and capture the Berezinskii-Kosterlitz-Thouless transition. We also discuss a connection between the output of diffusion maps and the eigenstates of a quantum-well Hamiltonian.},
	urldate = {2019-03-14},
	journal = {arXiv:1805.05961 [cond-mat]},
	author = {Rodriguez-Nieva, Joaquin F. and Scheurer, Mathias S.},
	month = may,
	year = {2018},
	note = {arXiv: 1805.05961},
	keywords = {Condensed Matter - Statistical Mechanics},
}


@article{gorodetsky_2018,
  title={A continuous analogue of the tensor-train decomposition},
  author={Gorodetsky, Alex and Karaman, Sertac and Marzouk, Youssef},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={347},
  pages={59--84},
  year={2019},
  publisher={Elsevier}
}

@article{izmailov_2017,
	title = {Scalable {Gaussian} {Processes} with {Billions} of {Inducing} {Inputs} via {Tensor} {Train} {Decomposition}},
	url = {http://arxiv.org/abs/1710.07324},
	abstract = {We propose a method (TT-GP) for approximate inference in Gaussian Process (GP) models. We build on previous scalable GP research including stochastic variational inference based on inducing inputs, kernel interpolation, and structure exploiting algebra. The key idea of our method is to use Tensor Train decomposition for variational parameters, which allows us to train GPs with billions of inducing inputs and achieve state-of-the-art results on several benchmarks. Further, our approach allows for training kernels based on deep neural networks without any modifications to the underlying GP model. A neural network learns a multidimensional embedding for the data, which is used by the GP to make the final prediction. We train GP and neural network parameters end-to-end without pretraining, through maximization of GP marginal likelihood. We show the efficiency of the proposed approach on several regression and classification benchmark datasets including MNIST, CIFAR-10, and Airline.},
	urldate = {2019-03-14},
	journal = {arXiv:1710.07324 [cs, stat]},
	author = {Izmailov, Pavel and Novikov, Alexander and Kropotov, Dmitry},
	month = oct,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1710.07324 PDF:/Users/gcarleo/Zotero/storage/MN924BZ8/Izmailov et al. - 2017 - Scalable Gaussian Processes with Billions of Induc.pdf:application/pdf;arXiv.org Snapshot:/Users/gcarleo/Zotero/storage/KK5CVZRM/1710.html:text/html}
}

@article{doggen_many-body_2018,
	title = {Many-body localization and delocalization in large quantum chains},
	volume = {98},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.98.174202},
	doi = {10.1103/PhysRevB.98.174202},
	number = {17},
	journal = {Physical Review B},
	author = {Doggen, Elmer V. H. and Schindler, Frank and Tikhonov, Konstantin S. and Mirlin, Alexander D. and Neupert, Titus and Polyakov, Dmitry G. and Gornyi, Igor V.},
	year = {2018},
	pages = {174202},
}
@article{van_nieuwenburg_learning_2017,
	title = {Learning phase transitions by confusion},
	volume = {13},
	copyright = {2017 Nature Publishing Group},
	issn = {1745-2481},
	url = {https://www.nature.com/articles/nphys4037},
	doi = {10.1038/nphys4037},
	abstract = {Classifying phases of matter is key to our understanding of many problems in physics. For quantum-mechanical systems in particular, the task can be daunting due to the exponentially large Hilbert space. With modern computing power and access to ever-larger data sets, classification problems are now routinely solved using machine-learning techniques1. Here, we propose a neural-network approach to finding phase transitions, based on the performance of a neural network after it is trained with data that are deliberately labelled incorrectly. We demonstrate the success of this method on the topological phase transition in the Kitaev chain2, the thermal phase transition in the classical Ising model3, and the many-body-localization transition in a disordered quantum spin chain4. Our method does not depend on order parameters, knowledge of the topological content of the phases, or any other specifics of the transition at hand. It therefore paves the way to the development of a generic tool for identifying unexplored phase transitions.},
	language = {en},
	number = {5},
	urldate = {2018-11-14},
	journal = {Nature Physics},
	author = {van Nieuwenburg, Evert P. L. and Liu, Ye-Hua and Huber, Sebastian D.},
	month = may,
	year = {2017},
	pages = {435--439},
	file = {Full Text PDF:/Users/gcarleo/Zotero/storage/TWA3LWJC/van Nieuwenburg et al. - 2017 - Learning phase transitions by confusion.pdf:application/pdf;Snapshot:/Users/gcarleo/Zotero/storage/SHJUGDGZ/nphys4037.html:text/html}
}


@article{zhang_quantum_2017,
	title = {Quantum {Loop} {Topography} for {Machine} {Learning}},
	volume = {118},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.118.216401},
	doi = {10.1103/PhysRevLett.118.216401},
	abstract = {Despite rapidly growing interest in harnessing machine learning in the study of quantum many-body systems, training neural networks to identify quantum phases is a nontrivial challenge. The key challenge is in efficiently extracting essential information from the many-body Hamiltonian or wave function and turning the information into an image that can be fed into a neural network. When targeting topological phases, this task becomes particularly challenging as topological phases are defined in terms of nonlocal properties. Here, we introduce quantum loop topography (QLT): a procedure of constructing a multidimensional image from the “sample” Hamiltonian or wave function by evaluating two-point operators that form loops at independent Monte Carlo steps. The loop configuration is guided by the characteristic response for defining the phase, which is Hall conductivity for the cases at hand. Feeding QLT to a fully connected neural network with a single hidden layer, we demonstrate that the architecture can be effectively trained to distinguish the Chern insulator and the fractional Chern insulator from trivial insulators with high fidelity. In addition to establishing the first case of obtaining a phase diagram with a topological quantum phase transition with machine learning, the perspective of bridging traditional condensed matter theory with machine learning will be broadly valuable.},
	number = {21},
	urldate = {2018-11-14},
	journal = {Physical Review Letters},
	author = {Zhang, Yi and Kim, Eun-Ah},
	month = may,
	year = {2017},
	pages = {216401},
	file = {Accepted Version:/Users/gcarleo/Zotero/storage/89LG4L5W/Zhang and Kim - 2017 - Quantum Loop Topography for Machine Learning.pdf:application/pdf;APS Snapshot:/Users/gcarleo/Zotero/storage/5WISUDF8/PhysRevLett.118.html:text/html}
}


@article{chen_symmetry-enforced_2018,
	title = {Symmetry-enforced self-learning {Monte} {Carlo} method applied to the {Holstein} model},
	volume = {98},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.98.041102},
	doi = {10.1103/PhysRevB.98.041102},
	abstract = {The self-learning Monte Carlo method (SLMC), using a trained effective model to guide Monte Carlo sampling processes, is a powerful general-purpose numerical method recently introduced to speed up simulations in (quantum) many-body systems. In this Rapid Communication, we further improve the efficiency of SLMC by enforcing physical symmetries on the effective model. We demonstrate its effectiveness in the Holstein Hamiltonian, one of the most fundamental many-body descriptions of electron-phonon coupling. Simulations of the Holstein model are notoriously difficult due to a combination of the typical cubic scaling of fermionic Monte Carlo and the presence of extremely long autocorrelation times. Our method addresses both bottlenecks. This enables simulations on large lattices in the most difficult parameter regions, and an evaluation of the critical point for the charge density wave transition at half filling with high precision. We argue that our work opens a research area of quantum Monte Carlo, providing a general procedure to deal with ergodicity in situations involving Hamiltonians with multiple, distinct low-energy states.},
	number = {4},
	journal = {Physical Review B},
	author = {Chen, Chuang and Xu, Xiao Yan and Liu, Junwei and Batrouni, George and Scalettar, Richard and Meng, Zi Yang},
	month = jul,
	year = {2018},
	pages = {041102},
	file = {APS Snapshot:/Users/giuscarl/Zotero/storage/NCURA8SW/PhysRevB.98.html:text/html;Submitted Version:/Users/giuscarl/Zotero/storage/NLBPPDAI/Chen et al. - 2018 - Symmetry-enforced self-learning Monte Carlo method.pdf:application/pdf}
}

@article{liu_self-learning_fermions_2017,
	title = {Self-learning {Monte} {Carlo} method and cumulative update in fermion systems},
	volume = {95},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.95.241104},
	doi = {10.1103/PhysRevB.95.241104},
	abstract = {We develop the self-learning Monte Carlo (SLMC) method, a general-purpose numerical method recently introduced to simulate many-body systems, for studying interacting fermion systems. Our method uses a highly efficient update algorithm, which we design and dub “cumulative update”, to generate new candidate configurations in the Markov chain based on a self-learned bosonic effective model. From a general analysis and a numerical study of the double exchange model as an example, we find that the SLMC with cumulative update drastically reduces the computational cost of the simulation, while remaining statistically exact. Remarkably, its computational complexity is far less than the conventional algorithm with local updates.},
	number = {24},
	urldate = {2018-11-14},
	journal = {Physical Review B},
	author = {Liu, Junwei and Shen, Huitao and Qi, Yang and Meng, Zi Yang and Fu, Liang},
	month = jun,
	year = {2017},
	pages = {241104},
	file = {Accepted Version:/Users/gcarleo/Zotero/storage/2RDW637U/Liu et al. - 2017 - Self-learning Monte Carlo method and cumulative up.pdf:application/pdf;APS Snapshot:/Users/gcarleo/Zotero/storage/AF5R43XY/PhysRevB.95.html:text/html}
}


@article{huang_neural_2017,
	title = {Neural network representation of tensor network and chiral states},
	url = {http://arxiv.org/abs/1701.06246},
	abstract = {We study the representational power of a Boltzmann machine (a type of neural network) in quantum many-body systems. We prove that any (local) tensor network state has a (local) neural network representation. The construction is almost optimal in the sense that the number of parameters in the neural network representation is almost linear in the number of nonzero parameters in the tensor network representation. Despite the difficulty of representing (gapped) chiral topological states with local tensor networks, we construct a quasi-local neural network representation for a chiral p-wave superconductor. This demonstrates the power of Boltzmann machines.},
	urldate = {2018-11-14},
	journal = {arXiv:1701.06246 [cond-mat]},
	author = {Huang, Yichen and Moore, Joel E.},
	month = jan,
	year = {2017},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks},
	file = {arXiv\:1701.06246 PDF:/Users/gcarleo/Zotero/storage/T44TPBP5/Huang and Moore - 2017 - Neural network representation of tensor network an.pdf:application/pdf;arXiv.org Snapshot:/Users/gcarleo/Zotero/storage/HDBMA2H2/1701.html:text/html}
}


@article{hu_discovering_2017,
	title = {Discovering phases, phase transitions, and crossovers through unsupervised machine learning: {A} critical examination},
	volume = {95},
	shorttitle = {Discovering phases, phase transitions, and crossovers through unsupervised machine learning},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.95.062122},
	doi = {10.1103/PhysRevE.95.062122},
	abstract = {We apply unsupervised machine learning techniques, mainly principal component analysis (PCA), to compare and contrast the phase behavior and phase transitions in several classical spin models—the square- and triangular-lattice Ising models, the Blume-Capel model, a highly degenerate biquadratic-exchange spin-1 Ising (BSI) model, and the two-dimensional XY model—and we examine critically what machine learning is teaching us. We find that quantified principal components from PCA not only allow the exploration of different phases and symmetry-breaking, but they can distinguish phase-transition types and locate critical points. We show that the corresponding weight vectors have a clear physical interpretation, which is particularly interesting in the frustrated models such as the triangular antiferromagnet, where they can point to incipient orders. Unlike the other well-studied models, the properties of the BSI model are less well known. Using both PCA and conventional Monte Carlo analysis, we demonstrate that the BSI model shows an absence of phase transition and macroscopic ground-state degeneracy. The failure to capture the “charge” correlations (vorticity) in the BSI model (XY model) from raw spin configurations points to some of the limitations of PCA. Finally, we employ a nonlinear unsupervised machine learning procedure, the “autoencoder method,” and we demonstrate that it too can be trained to capture phase transitions and critical points.},
	number = {6},
	urldate = {2018-11-14},
	journal = {Physical Review E},
	author = {Hu, Wenjian and Singh, Rajiv R. P. and Scalettar, Richard T.},
	month = jun,
	year = {2017},
	pages = {062122},
	file = {Accepted Version:/Users/gcarleo/Zotero/storage/8KESR7PI/Hu et al. - 2017 - Discovering phases, phase transitions, and crossov.pdf:application/pdf;APS Snapshot:/Users/gcarleo/Zotero/storage/I6UGT49L/PhysRevE.95.html:text/html}
}


@article{costa_principal_2017,
	title = {Principal component analysis for fermionic critical points},
	volume = {96},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.96.195138},
	doi = {10.1103/PhysRevB.96.195138},
	abstract = {We use determinant quantum Monte Carlo (DQMC), in combination with the principal component analysis (PCA) approach to unsupervised learning, to extract information about phase transitions in several of the most fundamental Hamiltonians describing strongly correlated materials. We first explore the zero-temperature antiferromagnet to singlet transition in the periodic Anderson model, the Mott insulating transition in the Hubbard model on a honeycomb lattice, and the magnetic transition in the 1/6-filled Lieb lattice. We then discuss the prospects for learning finite temperature superconducting transitions in the attractive Hubbard model, for which there is no sign problem. Finally, we investigate finite temperature charge density wave (CDW) transitions in the Holstein model, where the electrons are coupled to phonon degrees of freedom, and carry out a finite size scaling analysis to determine Tc. We examine the different behaviors associated with Hubbard-Stratonovich auxiliary field configurations on both the entire space-time lattice and on a single imaginary time slice, or other quantities, such as equal-time Green's and pair-pair correlation functions.},
	number = {19},
	urldate = {2019-03-07},
	journal = {Physical Review B},
	author = {Costa, Natanael C. and Hu, Wenjian and Bai, Z. J. and Scalettar, Richard T. and Singh, Rajiv R. P.},
	month = nov,
	year = {2017},
	pages = {195138},
	file = {Accepted Version:/Users/giuscarl/Zotero/storage/HDNTQY8E/Costa et al. - 2017 - Principal component analysis for fermionic critica.pdf:application/pdf;APS Snapshot:/Users/giuscarl/Zotero/storage/KVI2Z8CJ/PhysRevB.96.html:text/html}
}

@article{schindler_probing_2017,
	title = {Probing many-body localization with neural networks},
	volume = {95},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.95.245134},
	doi = {10.1103/PhysRevB.95.245134},
	abstract = {We show that a simple artificial neural network trained on entanglement spectra of individual states of a many-body quantum system can be used to determine the transition between a many-body localized and a thermalizing regime. Specifically, we study the Heisenberg spin-1/2 chain in a random external field. We employ a multilayer perceptron with a single hidden layer, which is trained on labeled entanglement spectra pertaining to the fully localized and fully thermal regimes. We then apply this network to classify spectra belonging to states in the transition region. For training, we use a cost function that contains, in addition to the usual error and regularization parts, a term that favors a confident classification of the transition region states. The resulting phase diagram is in good agreement with the one obtained by more conventional methods and can be computed for small systems. In particular, the neural network outperforms conventional methods in classifying individual eigenstates pertaining to a single disorder realization. It allows us to map out the structure of these eigenstates across the transition with spatial resolution. Furthermore, we analyze the network operation using the dreaming technique to show that the neural network correctly learns by itself the power-law structure of the entanglement spectra in the many-body localized regime.},
	number = {24},
	urldate = {2018-11-14},
	journal = {Physical Review B},
	author = {Schindler, Frank and Regnault, Nicolas and Neupert, Titus},
	month = jun,
	year = {2017},
	pages = {245134},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/ED99QWJ8/PhysRevB.95.html:text/html;Full Text:/Users/gcarleo/Zotero/storage/8S4T77EN/Schindler et al. - 2017 - Probing many-body localization with neural network.pdf:application/pdf}
}


@article{bukov_reinforcement_2018,
	title = {Reinforcement {Learning} in {Different} {Phases} of {Quantum} {Control}},
	volume = {8},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.8.031086},
	doi = {10.1103/PhysRevX.8.031086},
	abstract = {The ability to prepare a physical system in a desired quantum state is central to many areas of physics such as nuclear magnetic resonance, cold atoms, and quantum computing. Yet, preparing states quickly and with high fidelity remains a formidable challenge. In this work, we implement cutting-edge reinforcement learning (RL) techniques and show that their performance is comparable to optimal control methods in the task of finding short, high-fidelity driving protocol from an initial to a target state in nonintegrable many-body quantum systems of interacting qubits. RL methods learn about the underlying physical system solely through a single scalar reward (the fidelity of the resulting state) calculated from numerical simulations of the physical system. We further show that quantum-state manipulation viewed as an optimization problem exhibits a spin-glass-like phase transition in the space of protocols as a function of the protocol duration. Our RL-aided approach helps identify variational protocols with nearly optimal fidelity, even in the glassy phase, where optimal state manipulation is exponentially hard. This study highlights the potential usefulness of RL for applications in out-of-equilibrium quantum physics.},
	number = {3},
	urldate = {2018-11-14},
	journal = {Physical Review X},
	author = {Bukov, Marin and Day, Alexandre G. R. and Sels, Dries and Weinberg, Phillip and Polkovnikov, Anatoli and Mehta, Pankaj},
	month = sep,
	year = {2018},
	pages = {031086},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/ZN8TUXIH/PhysRevX.8.html:text/html;Full Text PDF:/Users/gcarleo/Zotero/storage/W6Z7CUP8/Bukov et al. - 2018 - Reinforcement Learning in Different Phases of Quan.pdf:application/pdf}
}


@article{torlai_neural_2017,
	title = {Neural {Decoder} for {Topological} {Codes}},
	volume = {119},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.119.030501},
	doi = {10.1103/PhysRevLett.119.030501},
	abstract = {We present an algorithm for error correction in topological codes that exploits modern machine learning techniques. Our decoder is constructed from a stochastic neural network called a Boltzmann machine, of the type extensively used in deep learning. We provide a general prescription for the training of the network and a decoding strategy that is applicable to a wide variety of stabilizer codes with very little specialization. We demonstrate the neural decoder numerically on the well-known two-dimensional toric code with phase-flip errors.},
	number = {3},
	journal = {Physical Review Letters},
	author = {Torlai, Giacomo and Melko, Roger G.},
	month = jul,
	year = {2017},
	pages = {030501},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/E56VU4HK/PhysRevLett.119.html:text/html}
}


@article{fosel_reinforcement_2018,
	title = {Reinforcement {Learning} with {Neural} {Networks} for {Quantum} {Feedback}},
	volume = {8},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.8.031084},
	doi = {10.1103/PhysRevX.8.031084},
	abstract = {Machine learning with artificial neural networks is revolutionizing science. The most advanced challenges require discovering answers autonomously. In the domain of reinforcement learning, control strategies are improved according to a reward function. The power of neural-network-based reinforcement learning has been highlighted by spectacular recent successes such as playing Go, but its benefits for physics are yet to be demonstrated. Here, we show how a network-based “agent” can discover complete quantum-error-correction strategies, protecting a collection of qubits against noise. These strategies require feedback adapted to measurement outcomes. Finding them from scratch without human guidance and tailored to different hardware resources is a formidable challenge due to the combinatorially large search space. To solve this challenge, we develop two ideas: two-stage learning with teacher and student networks and a reward quantifying the capability to recover the quantum information stored in a multiqubit system. Beyond its immediate impact on quantum computation, our work more generally demonstrates the promise of neural-network-based reinforcement learning in physics.},
	number = {3},
	urldate = {2018-11-14},
	journal = {Physical Review X},
	author = {Fösel, Thomas and Tighineanu, Petru and Weiss, Talitha and Marquardt, Florian},
	month = sep,
	year = {2018},
	pages = {031084},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/CVWYP435/PhysRevX.8.html:text/html;Full Text PDF:/Users/gcarleo/Zotero/storage/G98FGW6X/Fösel et al. - 2018 - Reinforcement Learning with Neural Networks for Qu.pdf:application/pdf}
}

@article{nagai_self-learning_2017,
	title = {Self-learning {Monte} {Carlo} method: {Continuous}-time algorithm},
	volume = {96},
	shorttitle = {Self-learning {Monte} {Carlo} method},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.96.161102},
	doi = {10.1103/PhysRevB.96.161102},
	abstract = {The recently introduced self-learning Monte Carlo method is a general-purpose numerical method that speeds up Monte Carlo simulations by training an effective model to propose uncorrelated configurations in the Markov chain. We implement this method in the framework of a continuous-time Monte Carlo method with an auxiliary field in quantum impurity models. We introduce and train a diagram generating function (DGF) to model the probability distribution of auxiliary field configurations in continuous imaginary time, at all orders of diagrammatic expansion. By using DGF to propose global moves in configuration space, we show that the self-learning continuous-time Monte Carlo method can significantly reduce the computational complexity of the simulation.},
	number = {16},
	urldate = {2018-11-14},
	journal = {Physical Review B},
	author = {Nagai, Yuki and Shen, Huitao and Qi, Yang and Liu, Junwei and Fu, Liang},
	month = oct,
	year = {2017},
	pages = {161102},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/IVNW33SP/PhysRevB.96.html:text/html;Full Text PDF:/Users/gcarleo/Zotero/storage/8WDU5MR5/Nagai et al. - 2017 - Self-learning Monte Carlo method Continuous-time .pdf:application/pdf}
}


@article{baireuther_2018,
	title = {Machine-learning-assisted correction of correlated qubit errors in a topological code},
	volume = {2},
	url = {https://quantum-journal.org/papers/q-2018-01-29-48/},
	doi = {10.22331/q-2018-01-29-48},
	abstract = {Paul Baireuther, Thomas E. O'Brien, Brian Tarasinski, and Carlo W. J. Beenakker,
Quantum 2, 48 (2018). https://doi.org/10.22331/q-2018-01-29-48
A fault-tolerant quantum computation requires an efficient means to detect and correct errors that accumulate in encoded quantum information. In the context of machine learning, neural netwo…},
	language = {en-GB},
	urldate = {2018-11-14},
	journal = {Quantum},
	author = {Baireuther, Paul and O'Brien, Thomas E. and Tarasinski, Brian and Beenakker, Carlo W. J.},
	month = jan,
	year = {2018},
	pages = {48},
	file = {Full Text:/Users/gcarleo/Zotero/storage/G5NKMQST/Baireuther et al. - 2018 - Machine-learning-assisted correction of correlated.pdf:application/pdf;Snapshot:/Users/gcarleo/Zotero/storage/HYDTBJL4/q-2018-01-29-48.html:text/html}
}


@article{liu_discriminative_2018,
	title = {Discriminative {Cooperative} {Networks} for {Detecting} {Phase} {Transitions}},
	volume = {120},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.120.176401},
	doi = {10.1103/PhysRevLett.120.176401},
	abstract = {The classification of states of matter and their corresponding phase transitions is a special kind of machine-learning task, where physical data allow for the analysis of new algorithms, which have not been considered in the general computer-science setting so far. Here we introduce an unsupervised machine-learning scheme for detecting phase transitions with a pair of discriminative cooperative networks (DCNs). In this scheme, a guesser network and a learner network cooperate to detect phase transitions from fully unlabeled data. The new scheme is efficient enough for dealing with phase diagrams in two-dimensional parameter spaces, where we can utilize an active contour model—the snake—from computer vision to host the two networks. The snake, with a DCN “brain,” moves and learns actively in the parameter space, and locates phase boundaries automatically.},
	number = {17},
	urldate = {2018-11-14},
	journal = {Physical Review Letters},
	author = {Liu, Ye-Hua and van Nieuwenburg, Evert P. L.},
	month = apr,
	year = {2018},
	pages = {176401},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/UIMMUUVV/PhysRevLett.120.html:text/html;Full Text:/Users/gcarleo/Zotero/storage/G2TCTD89/Liu and van Nieuwenburg - 2018 - Discriminative Cooperative Networks for Detecting .pdf:application/pdf}
}


@article{schmitt_quantum_2018,
	title = {Quantum dynamics in transverse-field {Ising} models from classical networks},
	volume = {4},
	issn = {2542-4653},
	url = {https://scipost.org/10.21468/SciPostPhys.4.2.013},
	doi = {10.21468/SciPostPhys.4.2.013},
	language = {en},
	number = {2},
	urldate = {2018-11-14},
	journal = {SciPost Physics},
	author = {Schmitt, Markus and Heyl, Markus},
	month = feb,
	year = {2018},
	pages = {013},
	file = {Full Text PDF:/Users/gcarleo/Zotero/storage/62UBATAM/Schmitt and Heyl - 2018 - Quantum dynamics in transverse-field Ising models .pdf:application/pdf;Snapshot:/Users/gcarleo/Zotero/storage/IM74VXUM/SciPostPhys.4.2.html:text/html}
}

@article{huembeli_automated_2018,
	title = {Automated discovery of characteristic features of phase transitions in many-body localization},
	url = {http://arxiv.org/abs/1806.00419},
	abstract = {We identify a new "order parameter" for the disorder driven many-body localization (MBL) transition by leveraging artificial intelligence. This allows us to pin down the transition, as the point at which the physics changes qualitatively, from vastly fewer disorder realizations and in an objective and cleaner way than is possible with the existing zoo of quantities. Contrary to previous studies, our method is almost entirely unsupervised. A game theoretic process between neural networks defines an adversarial setup with conflicting objectives to identify what characteristic features to base efficient predictions on. This reduces the numerical effort for mapping out the phase diagram by a factor of {\textasciitilde}100x. This approach of automated discovery is applicable specifically to poorly understood phase transitions and exemplifies the potential of machine learning assisted research in physics.},
	urldate = {2018-11-14},
	journal = {arXiv:1806.00419 [cond-mat, physics:quant-ph]},
	author = {Huembeli, Patrick and Dauphin, Alexandre and Wittek, Peter and Gogolin, Christian},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.00419},
	keywords = {Quantum Physics, Condensed Matter - Disordered Systems and Neural Networks},
	annote = {Comment: 3 pages + 3 pages appendix, 4 figures, first submitted to a journal on 2018-05-17}
}

@article{fournier_artificial_2018,
	title = {An {Artificial} {Neural} {Network} {Approach} to the {Analytic} {Continuation} {Problem}},
	url = {http://arxiv.org/abs/1810.00913},
	urldate = {2018-11-14},
	journal = {arXiv:1810.00913 [cond-mat, physics:physics]},
	author = {Fournier, Romain and Wang, Lei and Yazyev, Oleg V. and Wu, QuanSheng},
	month = oct,
	year = {2018},
	keywords = {Condensed Matter - Strongly Correlated Electrons, Physics - Computational Physics},
	annote = {Comment: 5 pages, 4 figures, submitted}
}

@article{arsenault_projected_2017,
	title = {Projected regression method for solving {Fredholm} integral equations arising in the analytic continuation problem of quantum physics},
	volume = {33},
	issn = {0266-5611},
	url = {http://stacks.iop.org/0266-5611/33/i=11/a=115007},
	doi = {10.1088/1361-6420/aa8d93},
	abstract = {We present a supervised machine learning approach to the inversion of Fredholm integrals of the first kind as they arise, for example, in the analytic continuation problem of quantum many-body physics. The approach provides a natural regularization for the ill-conditioned inverse of the Fredholm kernel, as well as an efficient and stable treatment of constraints. The key observation is that the stability of the forward problem permits the construction of a large database of outputs for physically meaningful inputs. Applying machine learning to this database generates a regression function of controlled complexity, which returns approximate solutions for previously unseen inputs; the approximate solutions are then projected onto the subspace of functions satisfying relevant constraints. Under standard error metrics the method performs as well or better than the Maximum Entropy method for low input noise and is substantially more robust to increased input noise. We suggest that the methodology will be similarly effective for other problems involving a formally ill-conditioned inversion of an integral operator, provided that the forward problem can be efficiently solved.},
	language = {en},
	number = {11},
	urldate = {2018-11-14},
	journal = {Inverse Problems},
	author = {Arsenault, Louis-François and Neuberg, Richard and Hannah, Lauren A. and Millis, Andrew J.},
	year = {2017},
	pages = {115007},
	file = {Accepted Version:/Users/gcarleo/Zotero/storage/R2CBMX33/Arsenault et al. - 2017 - Projected regression method for solving Fredholm i.pdf:application/pdf}
}


@article{ma_transforming_2018,
	title = {Transforming {Bell}’s inequalities into state classifiers with machine learning},
	volume = {4},
	copyright = {2018 The Author(s)},
	issn = {2056-6387},
	url = {https://www.nature.com/articles/s41534-018-0081-3},
	doi = {10.1038/s41534-018-0081-3},
	abstract = {A new approach combines machine-learning techniques with Bell’s inequality for efficient entanglement detection. Classifying entangled states is a computationally demanding task. Of particular relevance for quantum information processing tasks is the distinction between entangled and separable states. Although Bell’s inequality can be violated only by entangled states, it isn’t a reliable entanglement witness. In fact, for each form of the inequality some entangled states won’t lead to any violation. Yue-Chi Ma and Man-Hong Yung from China’s Tsinghua University have now shown that a suitably trained artificial neural network can find the optimal form of Bell’s inequality to efficiently determine if an unknown bipartite or tripartite state is entangled or separable. Beyond entanglement detection, this optimisation approach could be used to construct optimal operators for other quantum state classification tasks.},
	language = {En},
	number = {1},
	urldate = {2018-11-14},
	journal = {npj Quantum Information},
	author = {Ma, Yue-Chi and Yung, Man-Hong},
	month = jul,
	year = {2018},
	pages = {34},
	file = {Full Text PDF:/Users/gcarleo/Zotero/storage/BJLICTE6/Ma and Yung - 2018 - Transforming Bell’s inequalities into state classi.pdf:application/pdf;Snapshot:/Users/gcarleo/Zotero/storage/NUKQV8CZ/s41534-018-0081-3.html:text/html}
}


@article{weinstein_learning_2017,
	title = {Learning the {Einstein}-{Podolsky}-{Rosen} correlations on a {Restricted} {Boltzmann} {Machine}},
	url = {http://arxiv.org/abs/1707.03114},
	abstract = {We construct a hidden variable model for the EPR correlations using a Restricted Boltzmann Machine. The model reproduces the expected correlations and thus violates the Bell inequality, as required by Bell's theorem. Unlike most hidden-variable models, this model does not violate the \$locality\$ assumption in Bell's argument. Rather, it violates \$measurement\$ \$independence\$, albeit in a decidedly non-conspiratorial way.},
	urldate = {2018-11-14},
	journal = {arXiv:1707.03114 [cond-mat, physics:quant-ph]},
	author = {Weinstein, Steven},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.03114},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics, Quantum Physics},
	annote = {Comment: 5 pages, 3 figures. Comments welcome. References added in v2},
	file = {arXiv\:1707.03114 PDF:/Users/gcarleo/Zotero/storage/SRMKLRHA/Weinstein - 2017 - Learning the Einstein-Podolsky-Rosen correlations .pdf:application/pdf;arXiv.org Snapshot:/Users/gcarleo/Zotero/storage/7EG5FMH3/1707.html:text/html}
}


@incollection{stoudenmire_supervised_2016,
	title = {Supervised {Learning} with {Tensor} {Networks}},
	url = {http://papers.nips.cc/paper/6211-supervised-learning-with-tensor-networks.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Stoudenmire, Edwin and Schwab, David J},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {4799--4807}
}


@article{changlani_approximating_2009,
	title = {Approximating strongly correlated wave functions with correlator product states},
	volume = {80},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.80.245116},
	doi = {10.1103/PhysRevB.80.245116},
	abstract = {We describe correlator product states, a class of numerically efficient many-body wave functions to describe strongly correlated wave functions in any dimension. Correlator product states introduce direct correlations between physical degrees of freedom in a simple way, yet provide the flexibility to describe a wide variety of systems. We show that many interesting wave functions can be mapped exactly onto correlator product states, including Laughlin’s quantum Hall wave function, Kitaev’s toric code states, and Huse and Elser’s frustrated spin states. We also outline the relationship between correlator product states and other common families of variational wave functions such as matrix product states, tensor product states, and resonating valence-bond states. Variational calculations for the Heisenberg and spinless Hubbard models demonstrate the promise of correlator product states for describing both two-dimensional and fermion correlations. Even in one-dimensional systems, correlator product states are competitive with matrix product states for a fixed number of variational parameters.},
	number = {24},
	urldate = {2019-03-13},
	journal = {Physical Review B},
	author = {Changlani, Hitesh J. and Kinder, Jesse M. and Umrigar, C. J. and Chan, Garnet Kin-Lic},
	month = dec,
	year = {2009},
	pages = {245116},
	file = {Full Text PDF:/Users/gcarleo/Zotero/storage/FH9YYLGU/Changlani et al. - 2009 - Approximating strongly correlated wave functions w.pdf:application/pdf}
}

@article{gendiar_latent_2002,
	title = {Latent heat calculation of the three-dimensional \$q=3,\$ 4, and 5 {Potts} models by the tensor product variational approach},
	volume = {65},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.65.046702},
	doi = {10.1103/PhysRevE.65.046702},
	abstract = {Three-dimensional (3D) q-state Potts models (q=3, 4, and 5) are studied by the tensor product variational approach, which is a recently developed variational method for 3D classical lattice models. The variational state is given by a 2D product of local factors, and is improved by way of self-consistent calculations assisted by the corner transfer matrix renormalization group. It should be noted that no a priori condition is imposed for the local factor. Transition temperatures and latent heats are calculated from the observations of thermodynamic functions in both ordered and disordered phases.},
	number = {4},
	urldate = {2019-03-13},
	journal = {Physical Review E},
	author = {Gendiar, A. and Nishino, T.},
	month = apr,
	year = {2002},
	pages = {046702},
	file = {Full Text PDF:/Users/gcarleo/Zotero/storage/8EVYBDVI/Gendiar and Nishino - 2002 - Latent heat calculation of the three-dimensional \$.pdf:application/pdf}
}

@article{oseledets_tensor-train_2011,
	title = {Tensor-{Train} {Decomposition}},
	volume = {33},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/10.1137/090752286},
	doi = {10.1137/090752286},
	abstract = {A simple nonrecursive form of the tensor decomposition in d dimensions is presented. It does not inherently suffer from the curse of dimensionality, it has asymptotically the same number of parameters as the canonical decomposition, but it is stable and its computation is based on low-rank approximation of auxiliary unfolding matrices. The new form gives a clear and convenient way to implement all basic operations efficiently. A fast rounding procedure is presented, as well as basic linear algebra operations. Examples showing the benefits of the decomposition are given, and the efficiency is demonstrated by the computation of the smallest eigenvalue of a 19-dimensional operator.},
	number = {5},
	urldate = {2019-03-13},
	journal = {SIAM Journal on Scientific Computing},
	author = {Oseledets, I.},
	month = jan,
	year = {2011},
	pages = {2295--2317},
	file = {Full Text PDF:/Users/gcarleo/Zotero/storage/I2UMEXST/Oseledets - 2011 - Tensor-Train Decomposition.pdf:application/pdf;Snapshot:/Users/gcarleo/Zotero/storage/V3FXDS99/090752286.html:text/html}
}

@article{schuch_simulation_2008,
	title = {Simulation of {Quantum} {Many}-{Body} {Systems} with {Strings} of {Operators} and {Monte} {Carlo} {Tensor} {Contractions}},
	volume = {100},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.100.040501},
	doi = {10.1103/PhysRevLett.100.040501},
	abstract = {We introduce string-bond states, a class of states obtained by placing strings of operators on a lattice, which encompasses the relevant states in quantum information. For string-bond states, expectation values of local observables can be computed efficiently using Monte Carlo sampling, making them suitable for a variational algorithm which extends the density matrix renormalization group to higher dimensional and irregular systems. Numerical results demonstrate the applicability of these states to the simulation of many-body systems.},
	number = {4},
	urldate = {2019-03-13},
	journal = {Physical Review Letters},
	author = {Schuch, Norbert and Wolf, Michael M. and Verstraete, Frank and Cirac, J. Ignacio},
	month = jan,
	year = {2008},
	pages = {040501},
	file = {Full Text PDF:/Users/gcarleo/Zotero/storage/GTHDV6HH/Schuch et al. - 2008 - Simulation of Quantum Many-Body Systems with Strin.pdf:application/pdf}
}

@article{mezzacapo_ground-state_2009,
	title = {Ground-state properties of quantum many-body systems: entangled-plaquette states and variational {Monte} {Carlo}},
	volume = {11},
	issn = {1367-2630},
	shorttitle = {Ground-state properties of quantum many-body systems},
	doi = {10.1088/1367-2630/11/8/083026},
	abstract = {We propose a new ansatz for the ground-state wave function of quantum many-body systems on a lattice. The key idea is to cover the lattice with plaquettes and obtain a state whose configurational weights can be optimized by means of a variational Monte Carlo algorithm. Such a scheme applies to any dimension, without any 'sign' instability. We show results for various two-dimensional spin models (including frustrated ones). A detailed comparison with available exact results, as well as with variational methods based on different ansatze, is offered. In particular, our numerical estimates are in quite good agreement with exact ones for unfrustrated systems, and compare favorably to other methods for frustrated ones.},
	language = {en},
	number = {8},
	urldate = {2016-05-31},
	journal = {New Journal of Physics},
	author = {Mezzacapo, F. and Schuch, N. and Boninsegni, M. and Cirac, J. I.},
	year = {2009},
	pages = {083026},
	file = {IOP Full Text PDF:/Users/gcarleo/Zotero/storage/G385IHRW/Mezzacapo et al. - 2009 - Ground-state properties of quantum many-body syste.pdf:application/pdf}
}

@article{vidal_class_2008,
	title = {Class of {Quantum} {Many}-{Body} {States} {That} {Can} {Be} {Efficiently} {Simulated}},
	volume = {101},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.101.110501},
	doi = {10.1103/PhysRevLett.101.110501},
	abstract = {We introduce the multiscale entanglement renormalization ansatz, a class of quantum many-body states on a D-dimensional lattice that can be efficiently simulated with a classical computer, in that the expectation value of local observables can be computed exactly and efficiently. The multiscale entanglement renormalization ansatz is equivalent to a quantum circuit of logarithmic depth that has a very characteristic causal structure. It is also the ansatz underlying entanglement renormalization, a novel coarse-graining scheme for many-body quantum systems on a lattice.},
	number = {11},
	urldate = {2019-03-13},
	journal = {Physical Review Letters},
	author = {Vidal, G.},
	month = sep,
	year = {2008},
	pages = {110501},
	file = {Full Text PDF:/Users/gcarleo/Zotero/storage/CQUG5ZYC/Vidal - 2008 - Class of Quantum Many-Body States That Can Be Effi.pdf:application/pdf}
}

@article{vidal_entanglement_2007,
	title = {Entanglement {Renormalization}},
	volume = {99},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.99.220405},
	doi = {10.1103/PhysRevLett.99.220405},
	abstract = {We propose a real-space renormalization group (RG) transformation for quantum systems on a D-dimensional lattice. The transformation partially disentangles a block of sites before coarse-graining it into an effective site. Numerical simulations with the ground state of a 1D lattice at criticality show that the resulting coarse-grained sites require a Hilbert space dimension that does not grow with successive RG transformations. As a result we can address, in a quasi-exact way, tens of thousands of quantum spins with a computational effort that scales logarithmically in the system’s size. The calculations unveil that ground state entanglement in extended quantum systems is organized in layers corresponding to different length scales. At a quantum critical point, each relevant length scale makes an equivalent contribution to the entanglement of a block.},
	number = {22},
	urldate = {2019-03-13},
	journal = {Physical Review Letters},
	author = {Vidal, G.},
	month = nov,
	year = {2007},
	pages = {220405},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/8W8P2QAV/PhysRevLett.99.html:text/html;Full Text PDF:/Users/gcarleo/Zotero/storage/IUULG67G/Vidal - 2007 - Entanglement Renormalization.pdf:application/pdf}
}

@article{white_density_1992,
	title = {Density matrix formulation for quantum renormalization groups},
	volume = {69},
	doi = {10.1103/PhysRevLett.69.2863},
	abstract = {A generalization of the numerical renormalization-group procedure used first by Wilson for the Kondo problem is presented. It is shown that this formulation is optimal in a certain sense. As a demonstration of the effectiveness of this approach, results from numerical real-space renormalization-group calculations for Heisenberg chains are presented., This article appears in the following collection:},
	number = {19},
	urldate = {2015-11-23},
	journal = {Physical Review Letters},
	author = {White, Steven R.},
	month = nov,
	year = {1992},
	pages = {2863--2866},
}

@article{verstraete_matrix_2008,
	title = {Matrix product states, projected entangled pair states, and variational renormalization group methods for quantum spin systems},
	volume = {57},
	issn = {0001-8732},
	url = {https://doi.org/10.1080/14789940801912366},
	doi = {10.1080/14789940801912366},
	abstract = {This article reviews recent developments in the theoretical understanding and the numerical implementation of variational renormalization group methods using matrix product states and projected entangled pair states.},
	number = {2},
	urldate = {2019-02-03},
	journal = {Advances in Physics},
	author = {Verstraete, F. and Murg, V. and Cirac, J. I.},
	month = mar,
	year = {2008},
	keywords = {density matrix renormalization group, entanglement, matrix product states, numerical renormalization group, projected entangled pair states, quantum spin chains, strongly correlated quantum systems},
	pages = {143--224},
	file = {Full Text PDF:/Users/gcarleo/Zotero/storage/T2PSQCCP/Verstraete et al. - 2008 - Matrix product states, projected entangled pair st.pdf:application/pdf;Snapshot:/Users/gcarleo/Zotero/storage/8UELKBPC/14789940801912366.html:text/html}
}

@article{levine_deep_2017,
	title = {Deep {Learning} and {Quantum} {Entanglement}: {Fundamental} {Connections} with {Implications} to {Network} {Design}},
	shorttitle = {Deep {Learning} and {Quantum} {Entanglement}},
	url = {http://arxiv.org/abs/1704.01552},
	abstract = {Deep convolutional networks have witnessed unprecedented success in various machine learning applications. Formal understanding on what makes these networks so successful is gradually unfolding, but for the most part there are still significant mysteries to unravel. The inductive bias, which reflects prior knowledge embedded in the network architecture, is one of them. In this work, we establish a fundamental connection between the fields of quantum physics and deep learning. We use this connection for asserting novel theoretical observations regarding the role that the number of channels in each layer of the convolutional network fulfills in the overall inductive bias. Specifically, we show an equivalence between the function realized by a deep convolutional arithmetic circuit (ConvAC) and a quantum many-body wave function, which relies on their common underlying tensorial structure. This facilitates the use of quantum entanglement measures as well-defined quantifiers of a deep network's expressive ability to model intricate correlation structures of its inputs. Most importantly, the construction of a deep ConvAC in terms of a Tensor Network is made available. This description enables us to carry a graph-theoretic analysis of a convolutional network, with which we demonstrate a direct control over the inductive bias of the deep network via its channel numbers, that are related to the min-cut in the underlying graph. This result is relevant to any practitioner designing a network for a specific task. We theoretically analyze ConvACs, and empirically validate our findings on more common ConvNets which involve ReLU activations and max pooling. Beyond the results described above, the description of a deep convolutional network in well-defined graph-theoretic tools and the formal connection to quantum entanglement, are two interdisciplinary bridges that are brought forth by this work.},
	urldate = {2018-11-14},
	journal = {ICLR 2018},
	author = {Levine, Yoav and Yakira, David and Cohen, Nadav and Shashua, Amnon},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.01552},
	keywords = {Quantum Physics, Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning}
}




@article{levine_quantum_2019,
	title = {Quantum {Entanglement} in {Deep} {Learning} {Architectures}},
	volume = {122},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.122.065301},
	doi = {10.1103/PhysRevLett.122.065301},
	abstract = {Modern deep learning has enabled unprecedented achievements in various domains. Nonetheless, employment of machine learning for wave function representations is focused on more traditional architectures such as restricted Boltzmann machines (RBMs) and fully connected neural networks. In this Letter, we establish that contemporary deep learning architectures, in the form of deep convolutional and recurrent networks, can efficiently represent highly entangled quantum systems. By constructing tensor network equivalents of these architectures, we identify an inherent reuse of information in the network operation as a key trait which distinguishes them from standard tensor network-based representations, and which enhances their entanglement capacity. Our results show that such architectures can support volume-law entanglement scaling, polynomially more efficiently than presently employed RBMs. Thus, beyond a quantification of the entanglement capacity of leading deep learning architectures, our analysis formally motivates a shift of trending neural-network-based wave function representations closer to the state-of-the-art in machine learning.},
	number = {6},
	urldate = {2019-03-13},
	journal = {Physical Review Letters},
	author = {Levine, Yoav and Sharir, Or and Cohen, Nadav and Shashua, Amnon},
	month = feb,
	year = {2019},
	pages = {065301},
	file = {Full Text PDF:/Users/gcarleo/Zotero/storage/C38NQSXJ/Levine et al. - 2019 - Quantum Entanglement in Deep Learning Architecture.pdf:application/pdf}
}


@article{han_unsupervised_2018,
	title = {Unsupervised {Generative} {Modeling} {Using} {Matrix} {Product} {States}},
	volume = {8},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.8.031012},
	doi = {10.1103/PhysRevX.8.031012},
	abstract = {Generative modeling, which learns joint probability distribution from data and generates samples according to it, is an important task in machine learning and artificial intelligence. Inspired by probabilistic interpretation of quantum physics, we propose a generative model using matrix product states, which is a tensor network originally proposed for describing (particularly one-dimensional) entangled quantum states. Our model enjoys efficient learning analogous to the density matrix renormalization group method, which allows dynamically adjusting dimensions of the tensors and offers an efficient direct sampling approach for generative tasks. We apply our method to generative modeling of several standard data sets including the Bars and Stripes random binary patterns and the MNIST handwritten digits to illustrate the abilities, features, and drawbacks of our model over popular generative models such as the Hopfield model, Boltzmann machines, and generative adversarial networks. Our work sheds light on many interesting directions of future exploration in the development of quantum-inspired algorithms for unsupervised machine learning, which are promisingly possible to realize on quantum devices.},
	number = {3},
	urldate = {2018-11-14},
	journal = {Physical Review X},
	author = {Han, Zhao-Yu and Wang, Jun and Fan, Heng and Wang, Lei and Zhang, Pan},
	month = jul,
	year = {2018},
	pages = {031012},
}


@article{stoudenmire_learning_2018,
	title = {Learning relevant features of data with multi-scale tensor networks},
	volume = {3},
	issn = {2058-9565},
	url = {http://stacks.iop.org/2058-9565/3/i=3/a=034003},
	doi = {10.1088/2058-9565/aaba1a},
	language = {en},
	number = {3},
	urldate = {2018-11-14},
	journal = {Quantum Science and Technology},
	author = {Stoudenmire, E. Miles},
	year = {2018},
	pages = {034003}
}

@article{chung2018classification,
  title={Classification and geometry of general perceptual manifolds},
  author={Chung, SueYeon and Lee, Daniel D and Sompolinsky, Haim},
  journal={Physical Review X},
  volume={8},
  number={3},
  pages={031003},
  year={2018},
  publisher={APS}
}

@article{kadmon2015transition,
  title={Transition to chaos in random neuronal networks},
  author={Kadmon, Jonathan and Sompolinsky, Haim},
  journal={Physical Review X},
  volume={5},
  number={4},
  pages={041030},
  year={2015},
  publisher={APS}
}


@article{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2016}
}




@article{levine_bridging_2018,
	title = {Bridging {Many}-{Body} {Quantum} {Physics} and {Deep} {Learning} via {Tensor} {Networks}},
	url = {http://arxiv.org/abs/1803.09780},
	abstract = {The harnessing of modern computational abilities for many-body wave-function representations is naturally placed as a prominent avenue in contemporary condensed matter physics. Specifically, highly expressive computational schemes that are able to efficiently represent the entanglement properties of many-particle systems are of interest. In the seemingly unrelated field of machine learning, deep network architectures have exhibited an unprecedented ability to tractably encompass the dependencies characterizing hard learning tasks such as image classification. However, key questions regarding deep learning architecture design still have no adequate theoretical answers. In this paper, we establish a Tensor Network (TN) based common language between the two disciplines, which allows us to offer bidirectional contributions. By showing that many-body wave-functions are structurally equivalent to mappings of ConvACs and RACs, we construct their TN equivalents, and suggest quantum entanglement measures as natural quantifiers of dependencies in such networks. Accordingly, we propose a novel entanglement based deep learning design scheme. In the other direction, we identify that an inherent re-use of information in state-of-the-art deep learning architectures is a key trait that distinguishes them from standard TNs. Therefore, we employ a TN manifestation of information re-use and construct TNs corresponding to powerful architectures such as deep recurrent and overlapping convolutional networks. This allows us to demonstrate that the entanglement scaling supported by state-of-the-art deep learning architectures matches that of MERA TN in 1D, and that they support volume law entanglement in 2D polynomially more efficiently than RBMs. We thus provide theoretical motivation to shift trending neural-network based wave-function representations closer to state-of-the-art deep learning architectures.},
	urldate = {2018-11-14},
	journal = {arXiv:1803.09780 [quant-ph]},
	author = {Levine, Yoav and Sharir, Or and Cohen, Nadav and Shashua, Amnon},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.09780},
	keywords = {Quantum Physics, Computer Science - Machine Learning}
}


@article{glasser_supervised_2018,
  title={Supervised learning with generalized tensor networks},
  author={Glasser, Ivan and Pancotti, Nicola and Cirac, J Ignacio},
  journal={arXiv preprint arXiv:1806.05964},
  year={2018}
}
	
@article{anandkumar_tensor_2014,
	title = {Tensor {Decompositions} for {Learning} {Latent} {Variable} {Models}},
	volume = {15},
	url = {http://jmlr.org/papers/v15/anandkumar14b.html},
	journal = {Journal of Machine Learning Research},
	author = {Anandkumar, Animashree and Ge, Rong and Hsu, Daniel and Kakade, Sham M. and Telgarsky, Matus},
	year = {2014},
	pages = {2773--2832}
}


@article{acar_unsupervised_2009,
	title = {Unsupervised {Multiway} {Data} {Analysis}: {A} {Literature} {Survey}},
	volume = {21},
	issn = {1041-4347},
	shorttitle = {Unsupervised {Multiway} {Data} {Analysis}},
	doi = {10.1109/TKDE.2008.112},
	abstract = {Two-way arrays or matrices are often not enough to represent all the information in the data and standard two-way analysis techniques commonly applied on matrices may fail to find the underlying structures in multi-modal datasets. Multiway data analysis has recently become popular as an exploratory analysis tool in discovering the structures in higher-order datasets, where data have more than two modes. We provide a review of significant contributions in the literature on multiway models, algorithms as well as their applications in diverse disciplines including chemometrics, neuroscience, social network analysis, text mining and computer vision.},
	number = {1},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Acar, E. and Yener, B.},
	month = jan,
	year = {2009},
	keywords = {Chemical analysis, computer vision, Context modeling, data analysis, Data analysis, Data mining, Electroencephalography, exploratory analysis tool, Frequency domain analysis, higher-order singular value decomposition, Information analysis, Introductory and Survey, Mining methods and algorithms, Models, multimodal datasets, Neuroscience, singular value decomposition, Singular value decomposition, social network analysis, Social network services, Tensile stress, text mining, two-way analysis techniques, unsupervised multiway data analysis},
	pages = {6--20},
	file = {IEEE Xplore Abstract Record:/Users/gcarleo/Zotero/storage/LZS2EIB8/4538221.html:text/html}
}

@InProceedings{pmlr-v49-cohen16,
  title = 	 {On the Expressive Power of Deep Learning: A Tensor Analysis},
  author = 	 {Nadav Cohen and Or Sharir and Amnon Shashua},
  booktitle = 	 {29th Annual Conference on Learning Theory},
  pages = 	 {698--728},
  year = 	 {2016},
  editor = 	 {Vitaly Feldman and Alexander Rakhlin and Ohad Shamir},
  volume = 	 {49},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Columbia University, New York, New York, USA},
  month = 	 {23--26 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v49/cohen16.pdf},
  url = 	 {http://proceedings.mlr.press/v49/cohen16.html},
  abstract = 	 {It has long been conjectured that hypotheses spaces suitable for data that is compositional in nature, such as text or images, may be more efficiently represented with deep hierarchical networks than with shallow ones. Despite the vast empirical evidence supporting this belief, theoretical justifications to date are limited. In particular, they do not account for the locality, sharing and pooling constructs of convolutional networks, the most successful deep learning architecture to date. In this work we derive a deep network architecture based on arithmetic circuits that inherently employs locality, sharing and pooling. An equivalence between the networks and hierarchical tensor factorizations is established. We show that a shallow network corresponds to CP (rank-1) decomposition, whereas a deep network corresponds to Hierarchical Tucker decomposition. Using tools from measure theory and matrix algebra, we prove that besides a negligible set, all functions that can be implemented by a deep network of polynomial size, require exponential size in order to be realized (or even approximated) by a shallow network. Since log-space computation transforms our networks into SimNets, the result applies directly to a deep learning architecture demonstrating promising empirical performance. The construction and theory developed in this paper shed new light on various practices and ideas employed by the deep learning community.}
}




@article{bukov_floquet_2018,
  title={Reinforcement learning for autonomous preparation of Floquet-engineered states: Inverting the quantum Kapitza oscillator},
  author={Bukov, Marin},
  journal={Physical Review B},
  volume={98},
  number={22},
  pages={224305},
  year={2018},
  publisher={APS}
}

@article{liang_solving_2018,
	title = {Solving frustrated quantum many-particle models with convolutional neural networks},
	volume = {98},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.98.104426},
	doi = {10.1103/PhysRevB.98.104426},
	abstract = {Recently, there has been significant progress in solving quantum many-particle problems via machine learning based on the restricted Boltzmann machine. However, it is still highly challenging to solve frustrated models via machine learning, which has not been demonstrated so far. In this paper, we design a brand new convolutional neural network (CNN) to solve such quantum many-particle problems. We demonstrate, for the first time, solving the highly frustrated spin-1/2 J1−J2 antiferromagnetic Heisenberg model on square lattices via CNN. The energy per site achieved by the CNN is even better than previous string-bond-state calculations. Our work therefore opens up a new routine to solve challenging frustrated quantum many-particle problems using machine learning.},
	number = {10},
	urldate = {2018-11-14},
	journal = {Physical Review B},
	author = {Liang, Xiao and Liu, Wen-Yuan and Lin, Pei-Ze and Guo, Guang-Can and Zhang, Yong-Sheng and He, Lixin},
	month = sep,
	year = {2018},
	pages = {104426},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/AXDJVPIF/PhysRevB.98.html:text/html;Submitted Version:/Users/gcarleo/Zotero/storage/J69SQPAT/Liang et al. - 2018 - Solving frustrated quantum many-particle models wi.pdf:application/pdf}
}


@article{xin_local-measurement-based_2018,
	title = {Local-measurement-based quantum state tomography via neural networks},
	url = {http://arxiv.org/abs/1807.07445},
	abstract = {Quantum state tomography is a daunting challenge of experimental quantum computing even in moderate system size. One way to boost the efficiency of state tomography is via local measurements on reduced density matrices, but the reconstruction of the full state thereafter is hard. Here, we present a machine learning method to recover the full quantum state from its local information, where a fully-connected neural network is built to fulfill the task with up to seven qubits. In particular, we test the neural network model with a practical dataset, that in a 4-qubit nuclear magnetic resonance system our method yields global states via 2-local information with high accuracy. Our work paves the way towards scalable state tomography in large quantum systems.},
	urldate = {2018-11-14},
	journal = {arXiv:1807.07445 [quant-ph]},
	author = {Xin, Tao and Lu, Sirui and Cao, Ningping and Anikeeva, Galit and Lu, Dawei and Li, Jun and Long, Guilu and Zeng, Bei},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.07445},
	keywords = {Quantum Physics},
	annote = {Comment: 5 pages, 4 figures},
	file = {arXiv\:1807.07445 PDF:/Users/gcarleo/Zotero/storage/FH99K7DN/Xin et al. - 2018 - Local-measurement-based quantum state tomography v.pdf:application/pdf;arXiv.org Snapshot:/Users/gcarleo/Zotero/storage/BJ6FC78H/1807.html:text/html}
}

@article{saito_method_2018,
  title={Method to solve quantum few-body problems with artificial neural networks},
  author={Saito, Hiroki},
  journal={Journal of the Physical Society of Japan},
  volume={87},
  number={7},
  pages={074002},
  year={2018},
  publisher={The Physical Society of Japan}
}
	


@article{rocchetto_learning_2018,
	title = {Learning hard quantum distributions with variational autoencoders},
	volume = {4},
	copyright = {2018 The Author(s)},
	issn = {2056-6387},
	url = {https://www.nature.com/articles/s41534-018-0077-z},
	doi = {10.1038/s41534-018-0077-z},
	abstract = {Artificial neural networks are able to learn how to efficiently represent complex quantum many-body states. An international team lead by Andrea Rocchetto and Edward Grant from University of Oxford and University College London have tested the capabilities of their neural network on quantum states of different complexity and showed that depth influences the representational capability of the model. Their network is able to efficiently represent states for which an efficient classical description is known, and compress the representation of states which can only be generated efficiently by a quantum computer. Increasing the “depth” of the network, i.e. the number of intermediate layers the computation goes through, improves performances in both cases, but not for states which are hard also for quantum computers. This suggests that neural networks are able to learn correlations that arise specifically in quantum processes and are not easily reproducible by a classical system.},
	number = {1},
	urldate = {2018-07-05},
	journal = {npj Quantum Information},
	author = {Rocchetto, Andrea and Grant, Edward and Strelchuk, Sergii and Carleo, Giuseppe and Severini, Simone},
	month = jun,
	year = {2018},
	pages = {28}}





  @article{jonsson_neural-network_2018,
  	title = {Neural-network states for the classical simulation of quantum computing},
  	url = {http://arxiv.org/abs/1808.05232},
  	abstract = {Simulating quantum algorithms with classical resources generally requires exponential resources. However, heuristic classical approaches are often very efficient in approximately simulating special circuit structures, for example with limited entanglement, or based on one-dimensional geometries. Here we introduce a classical approach to the simulation of general quantum circuits based on neural-network quantum states (NQS) representations. Considering a set of universal quantum gates, we derive rules for exactly applying single-qubit and two-qubit Z rotations to NQS, whereas we provide a learning scheme to approximate the action of Hadamard gates. Results are shown for the Hadamard and Fourier transform of entangled initial states for systems sizes and total circuit depths exceeding what can be currently simulated with state-of-the-art brute-force techniques. The overall accuracy obtained by the neural-network states based on Restricted Boltzmann machines is satisfactory, and offers a classical route to simulating highly-entangled circuits. In the test cases considered, we find that our classical simulations are comparable to quantum simulations affected by an incoherent noise level in the hardware of about \$10{\textasciicircum}\{-3\}\$ per gate.},
  	urldate = {2018-09-14},
  	journal = {arXiv:1808.05232 [cond-mat, physics:physics, physics:quant-ph]},
  	author = {Jónsson, Bjarni and Bauer, Bela and Carleo, Giuseppe},
  	month = aug,
  	year = {2018},
  	keywords = {Condensed Matter - Quantum Gases, Quantum Physics, Condensed Matter - Disordered Systems and Neural Networks, Physics - Computational Physics},
  }



@article{zhang_using_2018,
	journal = {arXiv:1808.00479 [cond-mat, physics:physics]},
	author = {Zhang, Yi and Mesaros, A. and Fujita, K. and Edkins, S. D. and Hamidian, M. H. and Ch'ng, K. and Eisaki, H. and Uchida, S. and Davis, J. C. Séamus and Khatami, E. and Kim, Eun-Ah},
	month = aug,
	year = {2018},
}

@article{bohrdt_classifying_2018,
	title = {Classifying {Snapshots} of the {Doped} {Hubbard} {Model} with {Machine} {Learning}},
	url = {http://arxiv.org/abs/1811.12425},
	journal = {arXiv:1811.12425 [cond-mat]},
	author = {Bohrdt, Annabelle and Chiu, Christie S. and Ji, Geoffrey and Xu, Muqing and Greif, Daniel and Greiner, Markus and Demler, Eugene and Grusdt, Fabian and Knap, Michael},
	month = nov,
	year = {2018},
	keywords = {Condensed Matter - Quantum Gases, Condensed Matter - Strongly Correlated Electrons, Condensed Matter - Disordered Systems and Neural Networks},

}
@article{kaubruegger_chiral_2018,
	title = {Chiral topological phases from artificial neural networks},
	volume = {97},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.97.195136},
	doi = {10.1103/PhysRevB.97.195136},
	abstract = {Motivated by recent progress in applying techniques from the field of artificial neural networks (ANNs) to quantum many-body physics, we investigate to what extent the flexibility of ANNs can be used to efficiently study systems that host chiral topological phases such as fractional quantum Hall (FQH) phases. With benchmark examples, we demonstrate that training ANNs of restricted Boltzmann machine type in the framework of variational Monte Carlo can numerically solve FQH problems to good approximation. Furthermore, we show by explicit construction how n-body correlations can be kept at an exact level with ANN wave functions exhibiting polynomial scaling with power n in system size. Using this construction, we analytically represent the paradigmatic Laughlin wave function as an ANN state.},
	number = {19},
	urldate = {2018-07-25},
	journal = {Physical Review B},
	author = {Kaubruegger, Raphael and Pastori, Lorenzo and Budich, Jan Carl},
	month = may,
	year = {2018},
	pages = {195136},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/ZDHEH5QM/PhysRevB.97.html:text/html}
}


@article{pastori_generalized_2018,
	title = {Generalized {Transfer} {Matrix} {States} from {Artificial} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1808.02069},
	abstract = {Identifying variational wavefunctions that efficiently parametrize the physically relevant states in the exponentially large Hilbert space is one of the key tasks towards solving the quantum many-body problem. Powerful tools in this context such as tensor network states have recently been complemented by states derived from artificial neural networks (ANNs). Here, we propose and investigate a new family of quantum states, coined generalized transfer matrix states (GTMS), which bridges between the two mentioned approaches in the framework of deep ANNs. In particular, we show by means of a constructive embedding that the class of GTMS contains generic matrix product states while at the same time being capable of capturing more long-ranged quantum correlations that go beyond the area-law entanglement properties of tensor networks. While generic deep ANNs are hard to contract, meaning that the corresponding state amplitude can not be exactly evaluated, the GTMS network is shown to be analytically contractible using transfer matrix methods. With numerical simulations, we demonstrate how the GTMS network learns random matrix product states in a supervised learning scheme, and how augmenting the network by long-ranged couplings leads to the onset of volume-law entanglement scaling. We argue that this capability of capturing long-range quantum correlations makes GTMS a promising candidate for the study of critical and dynamical quantum many-body systems.},
	urldate = {2018-09-14},
	journal = {arXiv:1808.02069 [cond-mat, physics:physics, physics:quant-ph]},
	author = {Pastori, Lorenzo and Kaubruegger, Raphael and Budich, Jan Carl},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.02069},
	keywords = {Condensed Matter - Strongly Correlated Electrons, Quantum Physics, Physics - Computational Physics},
	file = {arXiv\:1808.02069 PDF:/Users/gcarleo/Zotero/storage/LKTNL26V/Pastori et al. - 2018 - Generalized Transfer Matrix States from Artificial.pdf:application/pdf;arXiv.org Snapshot:/Users/gcarleo/Zotero/storage/GZ8WIQ8B/1808.html:text/html}
}



@article{clark_unifying_2018,
	title = {Unifying neural-network quantum states and correlator product states via tensor networks},
	volume = {51},
	issn = {1751-8121},
	url = {http://stacks.iop.org/1751-8121/51/i=13/a=135301},
	doi = {10.1088/1751-8121/aaaaf2},
	abstract = {Correlator product states (CPS) are a powerful and very broad class of states for quantum lattice systems whose (unnormalised) amplitudes in a fixed basis can be sampled exactly and efficiently. They work by gluing together states of overlapping clusters of sites on the lattice, called correlators. Recently Carleo and Troyer (2017 Science 355 602) introduced a new type sampleable ansatz called neural-network quantum states (NQS) that are inspired by the restricted Boltzmann model used in machine learning. By employing the formalism of tensor networks we show that NQS are a special form of CPS with novel properties. Diagramatically a number of simple observations become transparent. Namely, that NQS are CPS built from extensively sized GHZ-form correlators making them uniquely unbiased geometrically. The appearance of GHZ correlators also relates NQS to canonical polyadic decompositions of tensors. Another immediate implication of the NQS equivalence to CPS is that we are able to formulate exact NQS representations for a wide range of paradigmatic states, including superpositions of weighed-graph states, the Laughlin state, toric code states, and the resonating valence bond state. These examples reveal the potential of using higher dimensional hidden units and a second hidden layer in NQS. The major outlook of this study is the elevation of NQS to correlator operators allowing them to enhance conventional well-established variational Monte Carlo approaches for strongly correlated fermions.},
	language = {en},
	number = {13},
	urldate = {2018-11-14},
	journal = {Journal of Physics A: Mathematical and Theoretical},
	author = {Clark, Stephen R.},
	year = {2018},
	pages = {135301},
}


@article{rocchetto_experimental_2017,
	title = {Experimental learning of quantum states},
	url = {http://arxiv.org/abs/1712.00127},
	abstract = {The number of parameters describing a quantum state is well known to grow exponentially with the number of particles. This scaling clearly limits our ability to do tomography to systems with no more than a few qubits and has been used to argue against the universal validity of quantum mechanics itself. However, from a computational learning theory perspective, it can be shown that, in a probabilistic setting, quantum states can be approximately learned using only a linear number of measurements. Here we experimentally demonstrate this linear scaling in optical systems with up to 6 qubits. Our results highlight the power of computational learning theory to investigate quantum information, provide the first experimental demonstration that quantum states can be "probably approximately learned" with access to a number of copies of the state that scales linearly with the number of qubits, and pave the way to probing quantum states at new, larger scales.},
	urldate = {2019-03-15},
	journal = {arXiv:1712.00127 [quant-ph]},
	author = {Rocchetto, Andrea and Aaronson, Scott and Severini, Simone and Carvacho, Gonzalo and Poderini, Davide and Agresti, Iris and Bentivegna, Marco and Sciarrino, Fabio},
	month = nov,
	year = {2017},
	note = {arXiv: 1712.00127},
	keywords = {Computer Science - Machine Learning, Quantum Physics},
	annote = {Comment: 11 pages, 6 figures},
	file = {arXiv\:1712.00127 PDF:/Users/gcarleo/Zotero/storage/Y4BSLPD8/Rocchetto et al. - 2017 - Experimental learning of quantum states.pdf:application/pdf;arXiv.org Snapshot:/Users/gcarleo/Zotero/storage/Y4ATLXYU/1712.html:text/html}
}


@article{riofrio_experimental_2017,
	title = {Experimental quantum compressed sensing for a seven-qubit system},
	volume = {8},
	copyright = {2017 Nature Publishing Group},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/ncomms15305},
	doi = {10.1038/ncomms15305},
	abstract = {Well-controlled quantum devices with their increasing system size face a new roadblock hindering further development of quantum technologies. The effort of quantum tomography—the reconstruction of states and processes of a quantum device—scales unfavourably: state-of-the-art systems can no longer be characterized. Quantum compressed sensing mitigates this problem by reconstructing states from incomplete data. Here we present an experimental implementation of compressed tomography of a seven-qubit system—a topological colour code prepared in a trapped ion architecture. We are in the highly incomplete—127 Pauli basis measurement settings—and highly noisy—100 repetitions each—regime. Originally, compressed sensing was advocated for states with few non-zero eigenvalues. We argue that low-rank estimates are appropriate in general since statistical noise enables reliable reconstruction of only the leading eigenvectors. The remaining eigenvectors behave consistently with a random-matrix model that carries no information about the true state.},
	language = {en},
	urldate = {2019-03-15},
	journal = {Nature Communications},
	author = {Riofrío, C. A. and Gross, D. and Flammia, S. T. and Monz, T. and Nigg, D. and Blatt, R. and Eisert, J.},
	month = may,
	year = {2017},
	pages = {15305},
	file = {Full Text PDF:/Users/gcarleo/Zotero/storage/Z39T92FG/Riofrío et al. - 2017 - Experimental quantum compressed sensing for a seve.pdf:application/pdf;Snapshot:/Users/gcarleo/Zotero/storage/YGKIWA4L/ncomms15305.html:text/html}
}

@article{lanyon_efficient_2017,
	title = {Efficient tomography of a quantum many-body system},
	volume = {advance online publication},
	copyright = {© 2017 Nature Publishing Group},
	issn = {1745-2473},
	url = {http://www.nature.com/nphys/journal/vaop/ncurrent/full/nphys4244.html?foxtrotcallback=true},
	doi = {10.1038/nphys4244},
	abstract = {Quantum state tomography is the standard technique for estimating the quantum state of small systems. But its application to larger systems soon becomes impractical as the required resources scale exponentially with the size. Therefore, considerable effort is dedicated to the development of new characterization tools for quantum many-body states. Here we demonstrate matrix product state tomography, which is theoretically proven to allow for the efficient and accurate estimation of a broad class of quantum states. We use this technique to reconstruct the dynamical state of a trapped-ion quantum simulator comprising up to 14 entangled and individually controlled spins: a size far beyond the practical limits of quantum state tomography. Our results reveal the dynamical growth of entanglement and describe its complexity as correlations spread out during a quench: a necessary condition for future demonstrations of better-than-classical performance. Matrix product state tomography should therefore find widespread use in the study of large quantum many-body systems and the benchmarking and verification of quantum simulators and computers.},
	language = {en},
	urldate = {2017-10-06},
	journal = {Nature Physics},
	author = {Lanyon, B. P. and Maier, C. and Holzäpfel, M. and Baumgratz, T. and Hempel, C. and Jurcevic, P. and Dhand, I. and Buyskikh, A. S. and Daley, A. J. and Cramer, M. and Plenio, M. B. and Blatt, R. and Roos, C. F.},
	month = sep,
	year = {2017},
	keywords = {Quantum information, Quantum simulation},
	file = {Full Text PDF:/Users/gcarleo/Zotero/storage/PGSXW2D8/Lanyon et al. - 2017 - Efficient tomography of a quantum many-body system.pdf:application/pdf;Snapshot:/Users/gcarleo/Zotero/storage/68JRPU99/nphys4244.html:text/html}
}
@article{gross_quantum_2010,
	title = {Quantum {State} {Tomography} via {Compressed} {Sensing}},
	volume = {105},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.105.150401},
	doi = {10.1103/PhysRevLett.105.150401},
	abstract = {We establish methods for quantum state tomography based on compressed sensing. These methods are specialized for quantum states that are fairly pure, and they offer a significant performance improvement on large quantum systems. In particular, they are able to reconstruct an unknown density matrix of dimension d and rank r using O(rdlog 2d) measurement settings, compared to standard methods that require d2 settings. Our methods have several features that make them amenable to experimental implementation: they require only simple Pauli measurements, use fast convex optimization, are stable against noise, and can be applied to states that are only approximately low rank. The acquired data can be used to certify that the state is indeed close to pure, so no a priori assumptions are needed.},
	number = {15},
	urldate = {2019-03-15},
	journal = {Physical Review Letters},
	author = {Gross, David and Liu, Yi-Kai and Flammia, Steven T. and Becker, Stephen and Eisert, Jens},
	month = oct,
	year = {2010},
	pages = {150401},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/WPHK2JVS/PhysRevLett.105.html:text/html;Full Text PDF:/Users/gcarleo/Zotero/storage/YPPE36D9/Gross et al. - 2010 - Quantum State Tomography via Compressed Sensing.pdf:application/pdf}
}
@article{xu_neural_2018,
	title = {Neural network state estimation for full quantum state tomography},
	url = {http://arxiv.org/abs/1811.06654},
	abstract = {An efficient state estimation model, neural network estimation (NNE), empowered by machine learning techniques, is presented for full quantum state tomography (FQST). A parameterized function based on neural network is applied to map the measurement outcomes to the estimated quantum states. Parameters are updated with supervised learning procedures. From the computational complexity perspective our algorithm is the most efficient one among existing state estimation algorithms for full quantum state tomography. We perform numerical tests to prove both the accuracy and scalability of our model.},
	urldate = {2019-03-15},
	journal = {arXiv:1811.06654 [quant-ph]},
	author = {Xu, Qian and Xu, Shuqi},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.06654},
	keywords = {Computer Science - Artificial Intelligence, Quantum Physics},
	file = {arXiv\:1811.06654 PDF:/Users/gcarleo/Zotero/storage/JHIE7FUI/Xu and Xu - 2018 - Neural network state estimation for full quantum s.pdf:application/pdf;arXiv.org Snapshot:/Users/gcarleo/Zotero/storage/DP6YIY42/1811.html:text/html}
}

@book{paris_quantum_2004,
	address = {Berlin Heidelberg},
	series = {Lecture {Notes} in {Physics}},
	title = {Quantum {State} {Estimation}},
	isbn = {978-3-540-22329-0},
	url = {https://www.springer.com/us/book/9783540223290},
	abstract = {This book is a comprehensive survey of most of the theoretical and experimental achievements in the field of quantum estimation of states and operations. Albeit still quite young, this field has already been recognized as a necessary tool for research in quantum optics and quantum information, beyond being a fascinating subject on its own as it touches upon the very conceptual foundations of quantum mechanics. The books consists of twelve extensive lectures that are essentially self-contained and modular allowing to combine various chapters as a basis for advanced courses and seminars on theoretical or experimental aspects. The last two chapters, for instance, form a self-consistent exposition on quantum discrimination problems. The book will benefit graduate students and newcomers to the field as high-level but accessible textbook, lecturers in search for advanced course material and researcher wishing to consult a modern and authoritative source of reference.},
	publisher = {Springer-Verlag},
	editor = {Paris, Matteo and Rehacek, Jaroslav},
	year = {2004},
}

@article{quek_adaptive_2018,
	title = {Adaptive {Quantum} {State} {Tomography} with {Neural} {Networks}},
	url = {http://arxiv.org/abs/1812.06693},
	abstract = {Quantum State Tomography is the task of determining an unknown quantum state by making measurements on identical copies of the state. Current algorithms are costly both on the experimental front -- requiring vast numbers of measurements -- as well as in terms of the computational time to analyze those measurements. In this paper, we address the problem of analysis speed and flexibility, introducing {\textbackslash}textit\{Neural Adaptive Quantum State Tomography\} (NA-QST), a machine learning based algorithm for quantum state tomography that adapts measurements and provides orders of magnitude faster processing while retaining state-of-the-art reconstruction accuracy. Our algorithm is inspired by particle swarm optimization and Bayesian particle-filter based adaptive methods, which we extend and enhance using neural networks. The resampling step, in which a bank of candidate solutions -- particles -- is refined, is in our case learned directly from data, removing the computational bottleneck of standard methods. We successfully replace the Bayesian calculation that requires computational time of \$O({\textbackslash}mathrm\{poly\}(n))\$ with a learned heuristic whose time complexity empirically scales as \$O({\textbackslash}log(n))\$ with the number of copies measured \$n\$, while retaining the same reconstruction accuracy. This corresponds to a factor of a million speedup for \$10{\textasciicircum}7\$ copies measured. We demonstrate that our algorithm learns to work with basis, symmetric informationally complete (SIC), as well as other types of POVMs. We discuss the value of measurement adaptivity for each POVM type, demonstrating that its effect is significant only for basis POVMs. Our algorithm can be retrained within hours on a single laptop for a two-qubit situation, which suggests a feasible time-cost when extended to larger systems. It can also adapt to a subset of possible states, a choice of the type of measurement, and other experimental details.},
	urldate = {2019-03-15},
	journal = {arXiv:1812.06693 [quant-ph]},
	author = {Quek, Yihui and Fort, Stanislav and Ng, Hui Khoon},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.06693}
}

@article{aaronson_shadow_2017,
	title = {Shadow {Tomography} of {Quantum} {States}},
	url = {http://arxiv.org/abs/1711.01053},
	abstract = {We introduce the problem of *shadow tomography*: given an unknown \$D\$-dimensional quantum mixed state \${\textbackslash}rho\$, as well as known two-outcome measurements \$E\_\{1\},{\textbackslash}ldots,E\_\{M\}\$, estimate the probability that \$E\_\{i\}\$ accepts \${\textbackslash}rho\$, to within additive error \${\textbackslash}varepsilon\$, for each of the \$M\$ measurements. How many copies of \${\textbackslash}rho\$ are needed to achieve this, with high probability? Surprisingly, we give a procedure that solves the problem by measuring only \${\textbackslash}widetilde\{O\}{\textbackslash}left( {\textbackslash}varepsilon{\textasciicircum}\{-4\}{\textbackslash}cdot{\textbackslash}log{\textasciicircum}\{4\} M{\textbackslash}cdot{\textbackslash}log D{\textbackslash}right)\$ copies. This means, for example, that we can learn the behavior of an arbitrary \$n\$-qubit state, on all accepting/rejecting circuits of some fixed polynomial size, by measuring only \$n{\textasciicircum}\{O{\textbackslash}left( 1{\textbackslash}right)\}\$ copies of the state. This resolves an open problem of the author, which arose from his work on private-key quantum money schemes, but which also has applications to quantum copy-protected software, quantum advice, and quantum one-way communication. Recently, building on this work, Brand{\textbackslash}{\textasciitilde}ao et al. have given a different approach to shadow tomography using semidefinite programming, which achieves a savings in computation time.},
	urldate = {2019-03-15},
	journal = {arXiv:1711.01053 [quant-ph]},
	author = {Aaronson, Scott},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.01053},
}

@article{banchi_modelling_2018,
	title = {Modelling non-markovian quantum processes with recurrent neural networks},
	volume = {20},
	issn = {1367-2630},
	url = {https://doi.org/10.1088%2F1367-2630%2Faaf749},
	doi = {10.1088/1367-2630/aaf749},
	abstract = {Quantum systems interacting with an unknown environment are notoriously difficult to model, especially in presence of non-Markovian and non-perturbative effects. Here we introduce a neural network based approach, which has the mathematical simplicity of the Gorini–Kossakowski–Sudarshan–Lindblad master equation, but is able to model non-Markovian effects in different regimes. This is achieved by using recurrent neural networks (RNNs) for defining Lindblad operators that can keep track of memory effects. Building upon this framework, we also introduce a neural network architecture that is able to reproduce the entire quantum evolution, given an initial state. As an application we study how to train these models for quantum process tomography, showing that RNNs are accurate over different times and regimes.},
	language = {en},
	number = {12},
	urldate = {2019-03-15},
	journal = {New Journal of Physics},
	author = {Banchi, Leonardo and Grant, Edward and Rocchetto, Andrea and Severini, Simone},
	month = dec,
	year = {2018},
	pages = {123030},
	file = {IOP Full Text PDF:/Users/gcarleo/Zotero/storage/85EADXUY/Banchi et al. - 2018 - Modelling non-markovian quantum processes with rec.pdf:application/pdf}
}

@article{gray_machine-learning-assisted_2018,
	title = {Machine-{Learning}-{Assisted} {Many}-{Body} {Entanglement} {Measurement}},
	volume = {121},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.121.150503},
	doi = {10.1103/PhysRevLett.121.150503},
	abstract = {Entanglement not only plays a crucial role in quantum technologies, but is key to our understanding of quantum correlations in many-body systems. However, in an experiment, the only way of measuring entanglement in a generic mixed state is through reconstructive quantum tomography, requiring an exponential number of measurements in the system size. Here, we propose a machine-learning-assisted scheme to measure the entanglement between arbitrary subsystems of size NA and NB, with O(NA+NB) measurements, and without any prior knowledge of the state. The method exploits a neural network to learn the unknown, nonlinear function relating certain measurable moments and the logarithmic negativity. Our procedure will allow entanglement measurements in a wide variety of systems, including strongly interacting many-body systems in both equilibrium and nonequilibrium regimes.},
	number = {15},
	urldate = {2019-03-15},
	journal = {Physical Review Letters},
	author = {Gray, Johnnie and Banchi, Leonardo and Bayat, Abolfazl and Bose, Sougato},
	month = oct,
	year = {2018},
	pages = {150503},
	file = {APS Snapshot:/Users/gcarleo/Zotero/storage/EUKXRXZX/PhysRevLett.121.html:text/html;Full Text PDF:/Users/gcarleo/Zotero/storage/CVMDHLLR/Gray et al. - 2018 - Machine-Learning-Assisted Many-Body Entanglement M.pdf:application/pdf}
}

@article{novikov_exponential_2016,
	title = {Exponential {Machines}},
	url = {http://arxiv.org/abs/1605.03795},
	abstract = {Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2{\textasciicircum}160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.},
	urldate = {2019-03-11},
	journal = {arXiv:1605.03795},
	author = {Novikov, Alexander and Trofimov, Mikhail and Oseledets, Ivan},
	month = may,
	year = {2016},
	note = {arXiv: 1605.03795},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICLR-2017 workshop track paper},
}

%%%%%% Maria and Laurent LIT %%%%%%

@article{ambrogio2018equivalent,
  title={Equivalent-accuracy accelerated neural-network training using analogue memory},
  author={Ambrogio, Stefano and Narayanan, Pritish and Tsai, Hsinyu and Shelby, Robert M and Boybat, Irem and Nolfo, Carmelo and Sidler, Severin and Giordano, Massimo and Bodini, Martina and Farinha, Nathan CP and others},
  journal={Nature},
  volume={558},
  number={7708},
  pages={60},
  year={2018},
  publisher={Nature Publishing Group}
}

@inproceedings{steinberg2015full,
title	= {Full-Chip Simulations, Keys to Success},
author	= {Dan Steinberg},
year	= {2015},
URL	= {http://www.synopsys.com/news/pubs/snug/2015/silicon-valley/tb04_steinberg_paper.pdf},
booktitle	= {SNUG Silicon Valley 2015 Proceedings},
address	= {Silicon Valley}
}

@article{JL84,
  title={Extensions of Lipschitz mappings into a Hilbert space},
  author={Johnson, William B and Lindenstrauss, Joram},
  journal={Contemporary mathematics},
  volume={26},
  number={189-206},
  pages={1},
  year={1984}
}

@article{CS06,
  title={Compressed sensing},
  author={Donoho, David L},
  journal={IEEE Transactions on information theory},
  volume={52},
  number={4},
  pages={1289--1306},
  year={2006},
  publisher={IEEE}
}

@inproceedings{kernel16,
  title={Random projections through multiple optical scattering: Approximating Kernels at the speed of light},
  author={Saade, Alaa and Caltagirone, Francesco and Carron, Igor and Daudet, Laurent and Dr{\'e}meau, Ang{\'e}lique and Gigan, Sylvain and Krzakala, Florent},
  booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on},
  pages={6215--6219},
  year={2016},
  organization={IEEE}
}

@inproceedings{dong18,
  title={Scaling up Echo-State Networks with multiple light scattering},
  author={Dong, Jonathan and Gigan, Sylvain and Krzakala, Florent and Wainrib, Gilles},
  booktitle={2018 IEEE Statistical Signal Processing Workshop (SSP)},
  pages={448--452},
  year={2018},
  organization={IEEE}
}

@article{newma18,
  title={NEWMA: a new method for scalable model-free online change-point detection},
  author={Keriven, Nicolas and Garreau, Damien and Poli, Iacopo},
  journal={arXiv preprint arXiv:1805.08061},
  year={2018}
}

@inproceedings{jouppi2017datacenter,
  title={In-datacenter performance analysis of a tensor processing unit},
  author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
  booktitle={Computer Architecture (ISCA), 2017 ACM/IEEE 44th Annual International Symposium on},
  pages={1--12},
  year={2017},
  organization={IEEE}
}

@article{markidis2018nvidia,
  title={NVIDIA Tensor Core Programmability, Performance \& Precision},
  author={Markidis, Stefano and Der Chien, Steven Wei and Laure, Erwin and Peng, Ivy Bo and Vetter, Jeffrey S},
  journal={IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
  year={2018}
}

@article{clements16,
author = {William R. Clements and Peter C. Humphreys and Benjamin J. Metcalf and W. Steven Kolthammer and Ian A. Walmsley},
journal = {Optica},
number = {12},
pages = {1460--1465},
publisher = {OSA},
title = {Optimal design for universal multiport interferometers},
volume = {3},
month = {Dec},
year = {2016}
}

@article{reck94,
  title={Experimental realization of any discrete unitary operator},
  author={Reck, Michael and Zeilinger, Anton and Bernstein, Herbert J and Bertani, Philip},
  journal={Physical Review Letters},
  volume={73},
  number={1},
  pages={58},
  year={1994},
  publisher={APS}
}

@unpublished{steinbrecher2018quantum,
  title = {Quantum optical neural networks},
  author = {Steinbrecher, Gregory R and Olson, Jonathan P and Englund, Dirk and Carolan, Jacques},
  archivePrefix = {arxiv},
  eprint = {1808.10047},
  year = {2018}
}

@article{shen2017deep,
  title={Deep learning with coherent nanophotonic circuits},
  author={Shen, Yichen and Harris, Nicholas C and Skirlo, Scott and Prabhu, Mihika and Baehr-Jones, Tom and Hochberg, Michael and Sun, Xin and Zhao, Shijie and Larochelle, Hugo and Englund, Dirk and others},
  journal={Nature Photonics},
  volume={11},
  number={7},
  pages={441},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{lu1989two,
  title={Two-dimensional programmable optical neural network},
  author={Lu, Taiwei and Wu, Shudong and Xu, Xin and Francis, TS},
  journal={Applied optics},
  volume={28},
  number={22},
  pages={4908--4913},
  year={1989},
  publisher={Optical Society of America}
}


@unpublished{killoran2018continuous,
  title = {Continuous-variable quantum neural networks},
  author = {Killoran, Nathan and Bromley, Thomas R and Arrazola, Juan Miguel and Schuld, Maria and Quesada, Nicol{\'a}s and Lloyd, Seth},
  year = {2018},
  note = {},
  archivePrefix = {arxiv},
  eprint = {1806.06871}
}

@article{lin2018all,
  title={All-optical machine learning using diffractive deep neural networks},
  author={Lin, Xing and Rivenson, Yair and Yardimci, Nezih T and Veli, Muhammed and Luo, Yi and Jarrahi, Mona and Ozcan, Aydogan},
  journal={Science},
  volume={361},
  number={6406},
  pages={1004--1008},
  year={2018},
  publisher={American Association for the Advancement of Science}
}


@article{havlicek2018,
  title={Supervised learning with quantum enhanced feature spaces},
  author={Havlicek, Vojtech and C{\'o}rcoles, Antonio D and Temme, Kristan and Harrow, Aram W and Chow, Jerry M and Gambetta, Jay M},
  journal={arXiv preprint arXiv:1804.11326},
  year={2018}
}

@article{schuld18feat,
  title={Quantum machine learning in feature Hilbert spaces},
  author={Schuld, Maria and Killoran, Nathan},
  journal={arXiv preprint arXiv:1803.07128v1},
  year={2018}
}

@article{hofmann08,
  title={Kernel methods in machine learning},
  author={Hofmann, Thomas and Sch{\"o}lkopf, Bernhard and Smola, Alexander J},
  journal={The Annals of Statistics},
  pages={1171--1220},
  year={2008},
  publisher={JSTOR}
}

@article{adachi15,
  title={Application of Quantum Annealing to Training of Deep Neural Networks},
  author={Adachi, Steven H and Henderson, Maxwell P},
  journal={arXiv preprint arXiv:1510.06356},
  year={2015}
}

@article{benedetti17a,
  title={Quantum-assisted Helmholtz machines: A quantum-classical deep learning framework for industrial datasets in near-term devices},
  author={Benedetti, Marcello and Realpe-G{\'o}mez, John and Perdomo-Ortiz, Alejandro},
  journal={arXiv preprint arXiv:1708.09784},
  year={2017}
}

@article{benedetti2017quantum,
  title={Quantum-assisted learning of hardware-embedded probabilistic graphical models},
  author={Benedetti, Marcello and Realpe-G{\'o}mez, John and Biswas, Rupak and Perdomo-Ortiz, Alejandro},
  journal={Physical Review X},
  volume={7},
  number={4},
  pages={041052},
  year={2017},
  publisher={APS}
}


@article{benedetti16b,
  title={Quantum-assisted learning of graphical models with arbitrary pairwise connectivity},
  author={Benedetti, Marcello and Realpe-G{\'o}mez, John and Biswas, Rupak and Perdomo-Ortiz, Alejandro},
  journal={arXiv preprint arXiv:1609.02542},
  year={2016}
}

@article{perdomo17a,
  title={Opportunities and challenges for quantum-assisted machine learning in near-term quantum computers},
  author={Perdomo-Ortiz, Alejandro and Benedetti, Marcello and Realpe-G{\'o}mez, John and Biswas, Rupak},
  journal={arXiv preprint arXiv:1708.09757},
  year={2017}
}

@book{schuld2018supervised,
  title={Supervised Learning with Quantum Computers},
  author={Schuld, Maria and Petruccione, Francesco},
  year={2018},
  publisher={Springer}
}

@article{biamonte17,
  title={Quantum machine learning},
  author={Biamonte, Jacob and Wittek, Peter and Pancotti, Nicola and Rebentrost, Patrick and Wiebe, Nathan and Lloyd, Seth},
  journal={Nature},
  volume={549},
  number={7671},
  pages={195},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{cong2018quantum,
  title={Quantum Convolutional Neural Networks},
  author={Cong, Iris and Choi, Soonwon and Lukin, Mikhail D},
  journal={arXiv preprint arXiv:1810.03787},
  year={2018}
}

@article{lloyd14,
  title={Quantum principal component analysis},
  author={Lloyd, Seth and Mohseni, Masoud and Rebentrost, Patrick},
  journal={Nature Physics},
volume={10},
pages={631--633},
  year={2014}
}

@article{tang2018quantum,
  title={A quantum-inspired classical algorithm for recommendation systems},
  author={Tang, Ewin},
  journal={arXiv preprint arXiv:1807.04271},
  year={2018}
}

@article{mitarai2018quantum,
  title={Quantum circuit learning},
  author={Mitarai, Kosuke and Negoro, Makoto and Kitagawa, Masahiro and Fujii, Keisuke},
  journal={arXiv preprint arXiv:1803.00745},
  year={2018}
}

@article{ambs2010optical,
  title={Optical computing: a 60-year adventure},
  author={Ambs, Pierre},
  journal={Advances in Optical Technologies},
  volume={2010},
  year={2010},
  publisher={Hindawi}
}

@article{lundberg2005history,
  title={The history of analog computing: introduction to the special section},
  author={Lundberg, Kent H},
  journal={IEEE Control Systems},
  volume={25},
  number={3},
  pages={22--25},
  year={2005},
  publisher={IEEE}
}


@article{mcclean2016theory,
  title={The theory of variational hybrid quantum-classical algorithms},
  author={McClean, Jarrod R and Romero, Jonathan and Babbush, Ryan and Aspuru-Guzik, Al{\'a}n},
  journal={New Journal of Physics},
  volume={18},
  number={2},
  pages={023023},
  year={2016},
  publisher={IOP Publishing}
}


@article{Chmiela:2018ge,
author = {Chmiela, Stefan and Sauceda, Huziel E and M{\"u}ller, Klaus-Robert and Tkatchenko, Alexandre},
title = {{Towards exact molecular dynamics simulations with machine-learned force fields}},
journal = {Nat. Commun.},
year = {2018},
volume = {9},
pages = {3887},
month = sep
}

@article{Mardt:2018bl,
author = {Mardt, Andreas and Pasquali, Luca and Wu, Hao and No{\'e}, Frank},
title = {{VAMPnets for deep learning of molecular kinetics}},
journal = {Nat. Commun.},
year = {2018},
volume = {9},
pages = {5},
month = oct
}

@article{Deringer:2018in,
author = {Deringer, Volker L and Bernstein, Noam and Bart{\'o}k, Albert P and Cliffe, Matthew J and Kerber, Rachel N and Marbella, Lauren E and Grey, Clare P and Elliott, Stephen R and Cs{\'a}nyi, G{\'a}bor},
title = {{Realistic Atomistic Structure of Amorphous Silicon from Machine- Learning-Driven Molecular Dynamics}},
journal = {J. Phys. Chem. Lett.},
year = {2018},
volume = {9},
number = {11},
pages = {2879--2885},
month = may
}

@article{Ramakrishnan:2014fd,
author = {Ramakrishnan, Raghunathan and Dral, Pavlo O and Rupp, Matthias and von Lilienfeld, O Anatole},
title = {{Quantum chemistry structures and properties of 134 kilo molecules}},
journal = {Sci. Data},
year = {2014},
volume = {1},
pages = {191},
month = aug
}

@article{Behler:2016fh,
author = {Behler, J{\"o}rg},
title = {{Perspective: Machine learning potentials for atomistic simulations}},
journal = {J. Chem. Phys.},
year = {2016},
volume = {145},
number = {17},
pages = {170901},
month = nov
}

@article{Hollingsworth:2018er,
author = {Hollingsworth, Jacob and Baker, Thomas E and Burke, Kieron},
title = {{Can exact conditions improve machine-learned density functionals?}},
journal = {J. Chem. Phys.},
year = {2018},
volume = {148},
number = {24},
pages = {241743},
month = jun
}

@article{Rupp:2018kp,
author = {Rupp, Matthias and von Lilienfeld, O Anatole and Burke, Kieron},
title = {{Guest Editorial: Special Topic on Data-Enabled Theoretical Chemistry}},
journal = {J. Chem. Phys.},
year = {2018},
volume = {148},
number = {24},
pages = {241401},
month = jun
}

@article{Smith:2017dq,
author = {Smith, J S and Isayev, O and Roitberg, A E},
title = {{ANI-1: an extensible neural network potential with DFT accuracy at force field computational cost}},
journal = {Chem. Sci.},
year = {2017},
volume = {8},
number = {4},
pages = {3192--3203}
}

@article{Schmidt:2018ix,
author = {Schmidt, Eric and Fowler, Andrew T and Elliott, James A and Bristowe, Paul D},
title = {{Learning models for electron densities with Bayesian regression}},
journal = {Comput. Mater. Sci.},
year = {2018},
volume = {149},
pages = {250--258},
month = jun
}

@article{Anelli:2018bw,
author = {Anelli, Andrea and Engel, Edgar A and Pickard, Chris J and Ceriotti, Michele},
title = {{Generalized convex hull construction for materials discovery}},
journal = {Phys. Rev. Mater.},
year = {2018},
volume = {2},
pages = {103804},
month = oct
}

@article{Butler:2018fl,
author = {Butler, Keith T and Davies, Daniel W and Cartwright, Hugh and Isayev, Olexandr and Walsh, Aron},
title = {{Machine learning for molecular and materials science}},
journal = {Nature},
year = {2018},
volume = {559},
pages = {547},
month = jul
}

@article{Schutt:2018hm,
author = {Sch{\"u}tt, K T and Sauceda, H E and Kindermans, P J and Tkatchenko, A and M{\"u}ller, K R},
title = {{SchNet -- A deep learning architecture for molecules and materials}},
journal = {J. Chem. Phys.},
year = {2018},
volume = {148},
number = {24},
pages = {241722},
month = jun
}

@article{Sidky:2018ji,
author = {Sidky, Hythem and Whitmer, Jonathan K},
title = {{Learning free energy landscapes using artificial neural networks}},
journal = {J. Chem. Phys.},
year = {2018},
volume = {148},
number = {10},
pages = {104111},
month = mar
}

@article{Nguyen:2018iy,
author = {Nguyen, Thuong T and Sz{\'e}kely, Eszter and Imbalzano, Giulio and Behler, J{\"o}rg and Cs{\'a}nyi, G{\'a}bor and Ceriotti, Michele and G{\"o}tz, Andreas W and Paesani, Francesco},
title = {{Comparison of permutationally invariant polynomials, neural networks, and Gaussian approximation potentials in representing water interactions through many-body expansions}},
journal = {J. Chem. Phys.},
year = {2018},
volume = {148},
number = {24},
pages = {241725},
month = jun
}

@article{Bereau:2018ig,
author = {Bereau, Tristan and DiStasio, Jr., Robert A and Tkatchenko, Alexandre and von Lilienfeld, O Anatole},
title = {{Non-covalent interactions across organic and biological subsets of chemical space: Physics-based potentials parametrized from machine learning}},
journal = {J. Chem. Phys.},
year = {2018},
volume = {148},
number = {24},
pages = {241706},
month = jun
}

@article{Engel:2018ki,
author = {Engel, Edgar A and Anelli, Andrea and Ceriotti, Michele and Pickard, Chris J and Needs, Richard J},
title = {{Mapping uncharted territory in ice from zeolite networks to ice structures}},
journal = {Nat. Commun.},
year = {2018},
volume = {9},
pages = {2173},
month = may
}

@article{Ballard:2017eu,
author = {Ballard, Andrew J and Das, Ritankar and Martiniani, Stefano and Mehta, Dhagash and Sagun, Levent and Stevenson, Jacob D and Wales, David J},
title = {{Energy landscapes for machine learning}},
journal = {Phys. Chem. Chem. Phys.},
year = {2017},
volume = {19},
number = {20},
pages = {12585--12603}
}

@article{Brockherde:2017vd,
author = {Brockherde, Felix and Vogt, Leslie and Li, Li and Tuckerman, Mark E and Burke, Kieron and M{\"u}ller, Klaus-Robert},
title = {{Bypassing the Kohn-Sham equations with machine learning}},
journal = {Nat. Commun.},
year = {2017},
volume = {8},
number = {1},
pages = {872},
month = oct
}

@article{Sosso:2018hv,
author = {Sosso, Gabriele C and Deringer, Volker L and Elliott, Stephen R and Cs{\'a}nyi, G{\'a}bor},
title = {{Understanding the thermal properties of amorphous solids using machine-learning-based interatomic potentials}},
journal = {Mol. Simulat.},
year = {2018},
volume = {44},
number = {11},
pages = {866--880},
month = apr
}

@article{Wehmeyer:2018bk,
author = {Wehmeyer, Christoph and No{\'e}, Frank},
title = {{Time-lagged autoencoders: Deep learning of slow collective variables for molecular kinetics}},
journal = {J. Chem. Phys.},
year = {2018},
volume = {148},
number = {24},
pages = {241703},
month = jun
}

@article{Faber:2017cs,
author = {Faber, Felix A and Hutchison, Luke and Huang, Bing and Gilmer, Justin and Schoenholz, Samuel S and Dahl, George E and Vinyals, Oriol and Kearnes, Steven and Riley, Patrick F and von Lilienfeld, O Anatole},
title = {{Prediction Errors of Molecular Machine Learning Models Lower than Hybrid DFT Error}},
journal = {J. Chem. Theory Comput.},
year = {2017},
volume = {13},
number = {11},
pages = {5255--5264},
month = oct
}

@article{Zhang:2018kz,
author = {Zhang, Linfeng and Han, Jiequn and Wang, Han and Car, Roberto and E, Weinan},
title = {{Deep Potential Molecular Dynamics: A Scalable Model with the Accuracy of Quantum Mechanics}},
journal = {Phys. Rev. Lett.},
year = {2018},
volume = {120},
number = {14},
pages = {143001},
month = apr
}

@article{bartok2013soap,
  title = {On representing chemical environments},
  author = {Bart\'ok, Albert P. and Kondor, Risi and Cs\'anyi, G\'abor},
  journal = {Phys. Rev. B},
  volume = {87},
  issue = {18},
  pages = {184115},
  numpages = {16},
  year = {2013},
  month = {May},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.87.184115},
  url = {https://link.aps.org/doi/10.1103/PhysRevB.87.184115}
}

@article{zhang2018active,
  title={Active Learning of Uniformly Accurate Inter-atomic Potentials for Materials Simulation},
  author={Zhang, Linfeng and Lin, De-Ye and Wang, Han and Car, Roberto and others},
  journal={arXiv preprint arXiv:1810.11890},
  year={2018}
}

@article{Gastegger:2017bi,
author = {Gastegger, Michael and Behler, J{\"o}rg and Marquetand, Philipp},
title = {{Machine learning molecular dynamics for the simulation of infrared spectra}},
journal = {Chem. Sci.},
year = {2017},
volume = {8},
number = {10},
pages = {6924--6935}
}

@article{Paruzzo:2018kl,
author = {Paruzzo, Federico M and Hofstetter, Albert and Musil, F{\'e}lix and De, Sandip and Ceriotti, Michele and Emsley, Lyndon},
title = {{Chemical shifts in molecular solids by machine learning}},
journal = {Nat. Commun.},
year = {2018},
volume = {9},
pages = {4501},
month = oct
}

@article{Yao:2018kn,
author = {Yao, Kun and Herr, John E and Toth, David W and Mckintyre, Ryker and Parkhill, John},
title = {{The TensorMol-0.1 model chemistry: a neural network augmented with long-range physics}},
journal = {Chem. Sci.},
year = {2018},
volume = {9},
number = {8},
pages = {2261--2269}
}

@article{Sifain:2018fr,
author = {Sifain, Andrew E and Lubbers, Nicholas and Nebgen, Benjamin T and Smith, Justin S and Lokhov, Andrey Y and Isayev, Olexandr and Roitberg, Adrian E and Barros, Kipton and Tretiak, Sergei},
title = {{Discovering a Transferable Charge Assignment Model Using Machine Learning}},
journal = {J. Phys. Chem. Lett.},
year = {2018},
volume = {9},
number = {16},
pages = {4495--4501},
month = jul
}

@article{Snyder:2012jm,
author = {Snyder, John C and Rupp, Matthias and Hansen, Katja and M{\"u}ller, Klaus-Robert and Burke, Kieron},
title = {{Finding Density Functionals with Machine Learning}},
journal = {Phys. Rev. Lett.},
year = {2012},
volume = {108},
number = {25},
pages = {1875},
month = jun
}

@article{Kamath:2018gf,
author = {Kamath, Aditya and Vargas-Hern{\'a}ndez, Rodrigo A and Krems, Roman V and Carrington, Jr, Tucker and Manzhos, Sergei},
title = {{Neural networks vs Gaussian process regression for representing potential energy surfaces: A comparative study of fit quality and vibrational spectrum accuracy}},
journal = {J. Chem. Phys.},
year = {2018},
volume = {148},
number = {24},
pages = {241702},
month = jun
}

@article{Schneider:2017cj,
author = {Schneider, Elia and Dai, Luke and Topper, Robert Q and Drechsel-Grau, Christof and Tuckerman, Mark E},
title = {{Stochastic Neural Network Approach for Learning High-Dimensional Free Energy Surfaces}},
journal = {Phys. Rev. Lett.},
year = {2017},
volume = {119},
number = {15},
pages = {150601},
month = oct
}

@article{Smith:2018ho,
author = {Smith, Justin S and Nebgen, Ben and Lubbers, Nicholas and Isayev, Olexandr and Roitberg, Adrian E},
title = {{Less is more: Sampling chemical space with active learning}},
journal = {J. Chem. Phys.},
year = {2018},
volume = {148},
number = {24},
pages = {241733},
month = jun
}

@article{Eickenberg:2018hta,
author = {Eickenberg, Michael and Exarchakis, Georgios and Hirn, Matthew and Mallat, St{\'e}phane and Thiry, Louis},
title = {{Solid harmonic wavelet scattering for predictions of molecule properties}},
journal = {J. Chem. Phys.},
year = {2018},
volume = {148},
number = {24},
pages = {241732},
month = jun
}

@article{Hu:2018gba,
author = {Hu, Deping and Xie, Yu and Li, Xusong and Li, Lingyue and Lan, Zhenggang},
title = {{Inclusion of Machine Learning Kernel Ridge Regression Potential Energy Surfaces in On-the-Fly Nonadiabatic Molecular Dynamics Simulation}},
journal = {J. Phys. Chem. Lett.},
year = {2018},
volume = {9},
number = {11},
pages = {2725--2732},
month = may
}

@article{Bartok:2010js,
author = {Bart{\'o}k, Albert P and Payne, Mike C and Kondor, Risi and Cs{\'a}nyi, G{\'a}bor},
title = {{Gaussian Approximation Potentials: The Accuracy of Quantum Mechanics, without the Electrons}},
journal = {Phys. Rev. Lett.},
year = {2010},
volume = {104},
number = {13},
pages = {136403},
month = apr
}

@article{Rupp:2012atomenergy,
  title = {Fast and Accurate Modeling of Molecular Atomization Energies with Machine Learning},
  author = {Rupp, Matthias and Tkatchenko, Alexandre and M\"uller, Klaus-Robert and von Lilienfeld, O. Anatole},
  journal = {Phys. Rev. Lett.},
  volume = {108},
  issue = {5},
  pages = {058301},
  numpages = {5},
  year = {2012},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.108.058301},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.108.058301}
}

@article{Lubbers:2018in,
author = {Lubbers, Nicholas and Smith, Justin S and Barros, Kipton},
title = {{Hierarchical modeling of molecular energies using a deep neural network}},
journal = {J. Chem. Phys.},
year = {2018},
volume = {148},
number = {24},
pages = {241715},
month = jun
}



@article{Li:2015ej,
author = {Li, Li and Snyder, John C and Pelaschier, Isabelle M and Huang, Jessica and Niranjan, Uma-Naresh and Duncan, Paul and Rupp, Matthias and M{\"u}ller, Klaus-Robert and Burke, Kieron},
title = {{Understanding machine-learned density functionals}},
journal = {Int. J. Quantum Chem.},
year = {2015},
volume = {116},
number = {11},
pages = {819--833},
month = nov
}


@article{Zhang:2019bf,
author = {Zhang, Linfeng and Lin, De-Ye and Wang, Han and Car, Roberto and E, Weinan},
title = {{Active learning of uniformly accurate interatomic potentials for materials simulation}},
year = {2019},
pages = {1--9},
month = feb
}


%%%%%%%%%%%%%%%%

@article{bang2014strategy,
  title={A strategy for quantum algorithm design assisted by machine learning},
  author={Bang, Jeongho and Ryu, Junghee and Yoo, Seokwon and Paw{\l}owski, Marcin and Lee, Jinhyoung},
  journal={New Journal of Physics},
  volume={16},
  number={7},
  pages={073017},
  year={2014},
  publisher={IOP Publishing}
}

@article{wecker2016training,
  title={Training a quantum optimizer},
  author={Wecker, Dave and Hastings, Matthew B and Troyer, Matthias},
  journal={Physical Review A},
  volume={94},
  number={2},
  pages={022309},
  year={2016},
  publisher={APS}
}

@article{sweke2018reinforcement,
  title={Reinforcement learning decoders for fault-tolerant quantum computation},
  author={Sweke, Ryan and Kesselring, Markus S and van Nieuwenburg, Evert PL and Eisert, Jens},
  journal={arXiv preprint arXiv:1810.07207},
  year={2018}
}

@article{biamonte17quantum,
  title={Quantum machine learning},
  author={Biamonte, Jacob and Wittek, Peter and Pancotti, Nicola and Rebentrost, Patrick and Wiebe, Nathan and Lloyd, Seth},
  journal={Nature},
  volume={549},
  number={7671},
  pages={195},
  year={2017},
  publisher={Nature Publishing Group}
}

@book{schuld18quantum,
	title={Quantum computing for supervised learning},
	author={Schuld, Maria and Petruccione, Francesco},
	publisher={Springer},
	year = {2018}
}

@book{wittek14quantum,
  title={Quantum machine learning: {W}hat quantum computing means to data mining},
  author={Wittek, Peter},
  year={2014},
  publisher={Elsevier Academic Press}
}

@article{niu2018universal,
  title={Universal Quantum Control through Deep Reinforcement Learning},
  author={Niu, Murphy Yuezhen and Boixo, Sergio and Smelyanskiy, Vadim and Neven, Hartmut},
  journal={arXiv preprint arXiv:1803.01857},
  year={2018}
}

@article{arunachalam2017guest,
  title={Guest column: a survey of quantum learning theory},
  author={Arunachalam, Srinivasan and de Wolf, Ronald},
  journal={ACM SIGACT News},
  volume={48},
  number={2},
  pages={41--67},
  year={2017},
  publisher={ACM}
}

@article{varsamopoulos2017decoding,
  title={Decoding small surface codes with feedforward neural networks},
  author={Varsamopoulos, Savvas and Criger, Ben and Bertels, Koen},
  journal={Quantum Science and Technology},
  volume={3},
  number={1},
  pages={015004},
  year={2017},
  publisher={IOP Publishing}
}


@article{krastanov2017deep,
  title={Deep neural network probabilistic decoder for stabilizer codes},
  author={Krastanov, Stefan and Jiang, Liang},
  journal={Scientific reports},
  volume={7},
  number={1},
  pages={11003},
  year={2017},
  publisher={Nature Publishing Group}
}

@misc{nielsen2002quantum,
  title={Quantum computation and quantum information},
  author={Nielsen, Michael A and Chuang, Isaac},
  year={2002},
  publisher={AAPT}
}


@article{zhang2019reinforcement,
  title={When reinforcement learning stands out in quantum control? A comparative study on state preparation},
  author={Zhang, Xiao-Ming and Wei, Zezhu and Asad, Raza and Yang, Xu-Chen and Wang, Xin},
  journal={arXiv preprint arXiv:1902.02157},
  year={2019}
}


@article{varsamopoulos2018designing,
  title={Designing neural network based decoders for surface codes},
  author={Varsamopoulos, Savvas and Bertels, Koen and Almudever, Carmen G},
  journal={arXiv preprint arXiv:1811.12456},
  year={2018}
}

@article{varsamopoulos2019decoding,
  title={Decoding surface code with a distributed neural network based decoder},
  author={Varsamopoulos, Savvas and Bertels, Koen and Almudever, Carmen G},
  journal={arXiv preprint arXiv:1901.10847},
  year={2019}
}

@article{nautrup2018optimizing,
  title={Optimizing quantum error correction codes with reinforcement learning},
  author={Nautrup, Hendrik Poulsen and Delfosse, Nicolas and Dunjko, Vedran and Briegel, Hans J and Friis, Nicolai},
  journal={arXiv preprint arXiv:1812.08451},
  year={2018}
}

@article{palittapongarnpim2017learning,
  title={Learning in quantum control: High-dimensional global optimization for noisy quantum dynamics},
  author={Palittapongarnpim, Pantita and Wittek, Peter and Zahedinejad, Ehsan and Vedaie, Shakib and Sanders, Barry C},
  journal={Neurocomputing},
  volume={268},
  pages={116--126},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{NIPS2011_8e6b42f1,
 author = {Delalleau, Olivier and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Shallow vs. Deep Sum-Product Networks},
 url = {https://proceedings.neurips.cc/paper/2011/file/8e6b42f1644ecb1327dc03ab345e618b-Paper.pdf},
 volume = {24},
 year = {2011}
}