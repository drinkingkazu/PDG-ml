\begin{verbatim}

Fundamentals:
 - discriminative vs. generative
 - Loss and Risk 
 - Emperical Risk
 - Test, train loss, generalization. 

Tasks, applications, and loss functions:
 - Supervised learning
   - regression
     - squared loss
   - classification
     - cross entropy
   - segmentation
   - unsupervised / self-supervised learning
    - density estimation
       - KL divergence
    - clustering
 - weakly supervised and semi-supervised
    - examples
 - reinforcement learning, model based planning, control
 - anomaly detection / out of distribution detection
 - Simulation-based Inference


Flavors of ML models
 - kernel machines
    - SVR and SVM
    - convex optimizatin, unique minima
 - decision trees
    - Gini index
    - boosting
 - neural networks components (Kazu can start here)
    - MLP
    - CNN
    - RNN (LSTM and GRU) and TreeRNN
    - DeepSets
    - Residual networks
    - Attention and Transformers
    - Graph Networks

Gradient-based optimization
 - Back propagation
 - stochastic gradient descent
 - optimization algorithms
    - ADAM
 - phenomena of over-parameterized models and implicit regularization

Deep Generative Models
 - GANs
 - VAEs
 - Normalizing Flows

Uncertainties
 - aleatoric \& epistemic uncertainty
   - calibration
   - robustness
 - Domain shift, learning to pivot
 - 

Applications
 - energy frontier
 - intensity frontier
 - cosmic frontier
 - accelerator frontier?
 
 
 Other topics to be incorporated:
  - in the trigger and data acquisition systems:  FPGAs etc.?
  - regularization (in fundamentals)
  
  - inductive bias
     - symmetries
     - connection to interpretability 
       - 
  - symbolic regression
 

\end{verbatim}