s@misc{planck-image,
author = "{ESA and {the Planck Collaboration}}",
howpublished = {\url{https://www.cosmos.esa.int/web/planck/picture-gallery}},
title = {{Planck image gallery}},
year = {2018}
}
@misc{epidemiology-image,
author = "{IBM Research}",
howpublished = {\url{https://researcher.watson.ibm.com/researcher/view\_group.php?id=883}},
title = {{Public Health Research - The Spatiotemporal Epidemiological Modeler (STEM)}},
year = {2019}
}
@misc{CMS:2021490,
author = "{CMS Collaboration}",
howpublished = {\url{http://cds.cern.ch/record/2021490}},
month = {jun},
title = {{CMS collisions at 13 TeV}},
year = {2015}
}
@misc{ppx,
author = "{PPX Developers}",
title = {{Probabilistic Programming eXecution protocol (PPX)}},
url = {http://github.com/probprog/ppx},
year = {2019}
}
@software{lukas_heinrich_2018_1634428,
author = {Heinrich, Lukas and Louppe, Gilles and Cranmer, Kyle},
doi = {10.5281/zenodo.1634428},
month = {nov},
publisher = {Zenodo},
title = {excursion},
howpublished={doi:10.5281/zenodo.1634428},
url = {https://doi.org/10.5281/zenodo.1634428},
year = {2018}
}
@inproceedings{Brehmer:2019bvj,
abstract = {One major challenge for the legacy measurements at the LHC is that the likelihood function is not tractable when the collected data is high-dimensional and the detector response has to be modeled. We review how different analysis strategies solve this issue, including the traditional histogram approach used in most particle physics analyses, the Matrix Element Method, Optimal Observables, and modern techniques based on neural density estimation. We then discuss powerful new inference methods that use a combination of matrix element information and machine learning to accurately estimate the likelihood function. The MadMiner package automates all necessary data-processing steps. In first studies we find that these new techniques have the potential to substantially improve the sensitivity of the LHC legacy measurements.},
archivePrefix = {arXiv},
arxivId = {1906.01578},
author = {Brehmer, Johann and Cranmer, Kyle and Espejo, Irina and Kling, Felix and Louppe, Gilles and Pavez, Juan},
booktitle = {19th International Workshop on Advanced Computing and Analysis Techniques in Physics Research: Empowering the revolution: Bringing Machine Learning to High Performance Computing (ACAT 2019) Saas-Fee, Switzerland, March 11-15, 2019},
eprint = {1906.01578},
title = {{Effective LHC measurements with matrix elements and machine learning}},
url = {http://arxiv.org/abs/1906.01578},
year = {2019}
}
@inproceedings{cutler2014reinforcement,
author = {Cutler, Mark and Walsh, Thomas J and How, Jonathan P},
booktitle = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
organization = {IEEE},
pages = {3888--3895},
title = {{Reinforcement learning with multi-fidelity simulators}},
year = {2014}
}
@article{2015arXiv150505770J,
abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
archivePrefix = {arXiv},
arxivId = {1505.05770},
author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
eprint = {1505.05770},
isbn = {9781510810587},
journal = {32nd International Conference on Machine Learning, ICML 2015},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
month = {may},
pages = {1530--1538},
title = {{Variational inference with normalizing flows}},
volume = {2},
year = {2015}
}
@article{2015arXiv150203509G,
abstract = {There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder's parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.},
archivePrefix = {arXiv},
arxivId = {1502.03509},
author = {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
eprint = {1502.03509},
isbn = {9781510810587},
journal = {32nd International Conference on Machine Learning, ICML 2015},
keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Comput,Statistics - Machine Learning},
month = {feb},
pages = {881--889},
title = {{MADE: Masked autoencoder for distribution estimation}},
volume = {2},
year = {2015}
}
@article{2016arXiv160106759V,
abstract = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two- dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNel dataset. Samples generated from the model appear crisp, varied and globally coherent.},
archivePrefix = {arXiv},
arxivId = {1601.06759},
author = {{Van Den Oord}, A{\"{a}}ron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
doi = {10.4249/scholarpedia.1888},
eprint = {1601.06759},
isbn = {9781510829008},
issn = {1941-6016},
journal = {33rd International Conference on Machine Learning, ICML 2016},
keywords = {Computer Science - Computer Vision and Pattern Rec,Computer Science - Learning,Computer Science - Neural and Evolutionary Comput},
month = {jan},
pages = {2611--2620},
title = {{Pixel recurrent neural networks}},
volume = {4},
year = {2016}
}
@article{2016arXiv160206701P,
abstract = {We introduce a new approach for amortizing inference in directed graphical models by learning heuristic approximations to stochastic inverses, designed specifically for use as proposal distributions in sequential Monte Carlo methods. We describe a procedure for constructing and learning a structured neural network which represents an inverse factorization of the graphical model, resulting in a conditional density estimator that takes as input particular values of the observed random variables, and returns an approximation to the distribution of the latent variables. This recognition model can be learned offline, independent from any particular dataset, prior to performing inference. The output of these networks can be used as automatically-learned high-quality proposal distributions to accelerate sequential Monte Carlo across a diverse range of problem settings.},
archivePrefix = {arXiv},
arxivId = {1602.06701},
author = {Paige, Brooks and Wood, Frank},
eprint = {1602.06701},
isbn = {9781510829008},
journal = {33rd International Conference on Machine Learning, ICML 2016},
keywords = {Statistics - Machine Learning},
month = {feb},
pages = {4434--4444},
title = {{Inference networks for sequential monte carlo in graphical models}},
volume = {6},
year = {2016}
}
@inproceedings{kandasamy2017multi,
abstract = {Bandit methods for black-box optimisation, such as Bayesian optimisation, arc used in a variety of applications including hyper-parameter tuning and experiment design. Recently, multi-fidelity methods have garnered considerable attention since function evaluations have become increasingly expensive in such applications. Multi-fidelity methods use cheap approximations to the function of interest to speed up the overall optimisation process. However, most multi-fidelity methods assume only a finite number of approximations. On the other hand, in many practical applications, a continuous spectrum of approximations might be available. For instance, when tuning an expensive neural network, one might choose to approximate the cross validation performance using less data N and/or few training iterations T. Here, the approximations are best viewed as arising out of a continuous two dimensional space (iV, T). In this work, we develop a Bayesian optimisation method, BOCA, for this setting. We characterise its theoretical properties and show that it achieves better regret than than strategies which ignore the approximations. BOCA outperforms several other baselines in synthetic and real experiments.},
archivePrefix = {arXiv},
arxivId = {1703.06240},
author = {Kandasamy, Kirthevasan and Dasarathy, Gautam and Schneider, Jeff and P{\'{o}}czos, Barnab{\'{a}}s},
booktitle = {34th International Conference on Machine Learning, ICML 2017},
eprint = {1703.06240},
isbn = {9781510855144},
organization = {JMLR. org},
pages = {2861--2878},
title = {{Multi-fidelity Bayesian optimisation with continuous approximations}},
volume = {4},
year = {2017}
}
@article{2018arXiv180400779H,
abstract = {Normalizing flows and autoregressive models have been successfully combined to produce state-of-the-art results in density estimation, via Masked Autoregressive Flows (MAF) (Papamakarios et al., 2017), and to accelerate stateof-the-art WaveNet-based speech synthesis to 20x faster than real-time (Oord et al., 2017), via Inverse Autoregressive Flows (IAF) (Kingma et al., 2016). We unify and generalize these approaches, replacing the (conditionally) affine univariate transformations of MAF/IAF with a more general class of inver{\"{u}}ble univariate transformations expressed as monotonic neural networks. We demonstrate that the proposed neural autoregressive flows (NAF) are universal approximators for continuous probability distributions, and their greater expressivity allows them to better capture multimodal target distributions. Experimentally, NAF yields state-of-the-art performance on a suite of density estimation tasks and outperforms IAF in variational autoencoders trained on binarized MNIST.},
archivePrefix = {arXiv},
arxivId = {1804.00779},
author = {Huang, Chin Wei and Krueger, David and Lacoste, Alexandre and Courville, Aaron},
eprint = {1804.00779},
isbn = {9781510867963},
journal = {35th International Conference on Machine Learning, ICML 2018},
keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
month = {apr},
pages = {3309--3324},
title = {{Neural autoregressive flows}},
volume = {5},
year = {2018}
}
@article{2014arXiv1410.8516D,
abstract = {We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the determinant of the Jacobian and inverse Jacobian is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.},
archivePrefix = {arXiv},
arxivId = {1410.8516},
author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
eprint = {1410.8516},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Workshop Track Proceedings},
keywords = {Computer Science - Learning},
month = {oct},
title = {{NICE: Non-linear independent components estimation}},
year = {2015}
}
@article{tran2017deep,
abstract = {We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations-random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.},
archivePrefix = {arXiv},
arxivId = {1701.03757},
author = {Tran, Dustin and Brevdo, Eugene and Hoffman, Matthew D. and Murphy, Kevin and Saurous, Rif A. and Blei, David M.},
doi = {10.1016/c2013-0-11637-6},
eprint = {1701.03757},
journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
title = {{Deep probabilistic programming}},
year = {2019}
}
@article{hamrick2017metacontrol,
abstract = {Many machine learning systems are built to solve the hardest examples of a particular task, which often makes them large and expensive to run-especially with respect to the easier examples, which might require much less computation. For an agent with a limited computational budget, this “one-size-fits-all” approach may result in the agent wasting valuable computation on easy examples, while not spending enough on hard examples. Rather than learning a single, fixed policy for solving all instances of a task, we introduce a metacontroller which learns to optimize a sequence of “imagined” internal simulations over predictive models of the world in order to construct a more informed, and more economical, solution. The metacontroller component is a model-free reinforcement learning agent, which decides both how many iterations of the optimization procedure to run, as well as which model to consult on each iteration. The models (which we call “experts”) can be state transition models, action-value functions, or any other mechanism that provides information useful for solving the task, and can be learned on-policy or off-policy in parallel with the metacontroller. When the metacontroller, controller, and experts were trained with “interaction networks” (Battaglia et al., 2016) as expert models, our approach was able to solve a challenging decision-making problem under complex non-linear dynamics. The metacontroller learned to adapt the amount of computation it performed to the difficulty of the task, and learned how to choose which experts to consult by factoring in both their reliability and individual computational resource costs. This allowed the metacontroller to achieve a lower overall cost (task loss plus computational cost) than more traditional fixed policy approaches. These results demonstrate that our approach is a powerful framework for using rich forward models for efficient model-based reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1705.02670},
author = {Hamrick, Jessica B. and Ballard, Andrew J. and Pascanu, Razvan and Vinyals, Oriol and Heess, Nicolas and Battaglia, Peter W.},
eprint = {1705.02670},
journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
title = {{Metacontrol for adaptive imagination-based optimization}},
year = {2019}
}
@inproceedings{2016arXiv160508803D,
abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful, stably invertible, and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact and efficient sampling, exact and efficient inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation, and latent variable manipulations.},
archivePrefix = {arXiv},
arxivId = {1605.08803},
author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
booktitle = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
eprint = {1605.08803},
title = {{Density estimation using real NVP}},
url = {https://openreview.net/forum?id=HkpbnH9lx},
year = {2019}
}
@inproceedings{2018arXiv181001367G,
abstract = {Reversible generative models map points from a simple distribution to a complex distribution through an easily invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, improving the state-of-the-art among exact likelihood methods with efficient sampling.},
archivePrefix = {arXiv},
arxivId = {1810.01367},
author = {Grathwohl, Will and Chen, Ricky T.Q. and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
booktitle = {7th International Conference on Learning Representations, ICLR 2019},
eprint = {1810.01367},
title = {{Ffjord: Free-form continuous dynamics for scalable reversible generative models}},
url = {https://openreview.net/forum?id=rJxgknCcK7},
year = {2019}
}
@article{2017arXiv170507057P,
abstract = {Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.},
archivePrefix = {arXiv},
arxivId = {1705.07057},
author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
eprint = {1705.07057},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
keywords = {Computer Science - Learning,Statistics - Machine Learning},
month = {may},
pages = {2339--2348},
title = {{Masked autoregressive flow for density estimation}},
volume = {2017-December},
year = {2017}
}
@article{2017arXiv171101861L,
abstract = {Mechanistic models of single-neuron dynamics have been extensively studied in computational neuroscience. However, identifying which models can quantitatively reproduce empirically measured data has been challenging. We propose to overcome this limitation by using likelihood-free inference approaches (also known as Approximate Bayesian Computation, ABC) to perform full Bayesian inference on single-neuron models. Our approach builds on recent advances in ABC by learning a neural network which maps features of the observed data to the posterior distribution over parameters. We learn a Bayesian mixture-density network approximating the posterior over multiple rounds of adaptively chosen simulations. Furthermore, we propose an efficient approach for handling missing features and parameter settings for which the simulator fails, as well as a strategy for automatically learning relevant features using recurrent neural networks. On synthetic data, our approach efficiently estimates posterior distributions and recovers ground-truth parameters. On in-vitro recordings of membrane voltages, we recover multivariate posteriors over biophysical parameters, which yield model-predicted voltage traces that accurately match empirical data. Our approach will enable neuroscientists to perform Bayesian inference on complex neuron models without having to design model-specific algorithms, closing the gap between mechanistic and statistical approaches to single-neuron modelling.},
archivePrefix = {arXiv},
arxivId = {1711.01861},
author = {Lueckmann, Jan Matthis and Gon{\c{c}}alves, Pedro J. and Bassetto, Giacomo and {\"{O}}cal, Kaan and Nonnenmacher, Marcel and Mackey, Jakob H.},
eprint = {1711.01861},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
keywords = {Statistics - Machine Learning},
month = {nov},
pages = {1290--1300},
title = {{Flexible statistical inference for mechanistic models of neural dynamics}},
volume = {2017-December},
year = {2017}
}
@article{2015arXiv150603693M,
abstract = {We describe an embarrassingly parallel, anytime Monte Carlo method for likelihood-free models. The algorithm starts with the view that the stochasticity of the pseudo-samples generated by the simulator can be controlled externally by a vector of random numbers u, in such a way that the outcome, knowing u, is deterministic. For each instantiation of u we run an optimization procedure to minimize the distance between summary statistics of the simulator and the data. After reweighing these samples using the prior and the Jacobian (accounting for the change of volume in transforming from the space of summary statistics to the space of parameters) we show that this weighted ensemble represents a Monte Carlo estimate of the posterior distribution. The procedure can be run embarrassingly parallel (each node handling one sample) and anytime (by allocating resources to the worst performing sample). The procedure is validated on six experiments.},
archivePrefix = {arXiv},
arxivId = {1506.03693},
author = {Meeds, Edward and Welling, Max},
eprint = {1506.03693},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
month = {jun},
pages = {2080--2088},
title = {{Optimization Monte Carlo: Efficient and embarrassingly parallel likelihood-free inference}},
volume = {2015-January},
year = {2015}
}
@article{2016arXiv160604934K,
abstract = {The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.},
archivePrefix = {arXiv},
arxivId = {1606.04934},
author = {Kingma, Diederik P. and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
eprint = {1606.04934},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
month = {jun},
pages = {4743--4751},
title = {{Improved variational inference with inverse autoregressive flow}},
year = {2016}
}
@article{2016arXiv160605328V,
abstract = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
archivePrefix = {arXiv},
arxivId = {1606.05328},
author = {{Van Den Oord}, A{\"{a}}ron and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
eprint = {1606.05328},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
keywords = {Computer Science - Computer Vision and Pattern Rec,Computer Science - Learning},
month = {jun},
pages = {4797--4805},
title = {{Conditional image generation with PixelCNN decoders}},
year = {2016}
}
@inproceedings{NIPS2016_6084,
abstract = {Many statistical models can be simulated forwards but have intractable likelihoods. Approximate Bayesian Computation (ABC) methods are used to infer properties of these models from data. Traditionally these methods approximate the posterior over parameters by conditioning on data being inside an ∈-ball around the observed data, which is only correct in the limit ∈→0. Monte Carlo methods can then draw samples from the approximate posterior to approximate predictions or error bars on parameters. These algorithms critically slow down as ∈ → 0, and in practice draw samples from a broader distribution than the posterior. We propose a new approach to likelihood-free inference based on Bayesian conditional density estimation. Preliminary inferences based on limited simulation data are used to guide later simulations. In some cases, learning an accurate parametric representation of the entire true posterior distribution requires fewer model simulations than Monte Carlo ABC methods need to produce a single sample from an approximate posterior.},
archivePrefix = {arXiv},
arxivId = {1605.06376},
author = {Papamakarios, George and Murray, Iain},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1605.06376},
issn = {10495258},
pages = {1036--1044},
title = {{Fast e-free inference of simulation models with Bayesian conditional density estimation}},
year = {2016}
}
@incollection{2017arXiv170208896T,
abstract = {Implicit probabilistic models are a flexible class of models defined by a simulation process for data. They form the basis for theories which encompass our understanding of the physical world. Despite this fundamental nature, the use of implicit models remains limited due to challenges in specifying complex latent structure in them, and in performing inferences in such models with large data sets. In this paper, we first introduce hierarchical implicit models (HIMs). HIMs combine the idea of implicit densities with hierarchical Bayesian modeling, thereby defining models via simulators of data with rich hidden structure. Next, we develop likelihood-free variational inference (LFVI), a scalable variational inference algorithm for HIMs. Key to LFVI is specifying a variational family that is also implicit. This matches the model's flexibility and allows for accurate approximation of the posterior. We demonstrate diverse applications: a large-scale physical simulator for predator-prey populations in ecology; a Bayesian generative adversarial network for discrete data; and a deep implicit model for text generation.},
archivePrefix = {arXiv},
arxivId = {1702.08896},
author = {Tran, Dustin and Ranganath, Rajesh and Blei, David M.},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
eprint = {1702.08896},
issn = {10495258},
pages = {5524--5534},
title = {{Hierarchical implicit models and likelihood-free variational inference}},
volume = {2017-December},
year = {2017}
}
@article{2018arXiv180703039K,
abstract = {Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1 × 1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a flow-based generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow.},
archivePrefix = {arXiv},
arxivId = {1807.03039},
author = {Kingma, Diederik P. and Dhariwal, Prafulla},
eprint = {1807.03039},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
month = {jul},
pages = {10215--10224},
title = {{Glow: Generative flow with invertible 1×1 convolutions}},
volume = {2018-December},
year = {2018}
}
@article{Cranmer:2015bka,
abstract = {In many fields of science, generalized likelihood ratio tests are established tools for statistical inference. At the same time, it has become increasingly common that a simulator (or generative model) is used to describe complex processes that tie parameters {\$}\backslashtheta{\$} of an underlying theory and measurement apparatus to high-dimensional observations {\$}\backslashmathbf{\{}x{\}}\backslashin \backslashmathbb{\{}R{\}}{\^{}}p{\$}. However, simulator often do not provide a way to evaluate the likelihood function for a given observation {\$}\backslashmathbf{\{}x{\}}{\$}, which motivates a new class of likelihood-free inference algorithms. In this paper, we show that likelihood ratios are invariant under a specific class of dimensionality reduction maps {\$}\backslashmathbb{\{}R{\}}{\^{}}p \backslashmapsto \backslashmathbb{\{}R{\}}{\$}. As a direct consequence, we show that discriminative classifiers can be used to approximate the generalized likelihood ratio statistic when only a generative model for the data is available. This leads to a new machine learning-based approach to likelihood-free inference that is complementary to Approximate Bayesian Computation, and which does not require a prior on the model parameters. Experimental results on artificial problems with known exact likelihoods illustrate the potential of the proposed method.},
archivePrefix = {arXiv},
arxivId = {1506.02169},
author = {Cranmer, Kyle and Pavez, Juan and Louppe, Gilles},
eprint = {1506.02169},
journal = {arXiv:1506.02169},
title = {{Approximating Likelihood Ratios with Calibrated Discriminative Classifiers}},
url = {http://arxiv.org/abs/1506.02169},
year = {2015}
}
@article{2016arXiv160202210K,
abstract = {When data analysts train a classifier and check if its accuracy is significantly different from a half, they are implicitly performing a two-sample test. We investigate the statistical optimality of this indirect but flexible method in the high-dimensional setting of {\$}d/n \backslashto c \backslashin (0,\backslashinfty){\$}. We provide a concrete answer for the case of distinguishing Gaussians with mean-difference {\$}\backslashdelta{\$} and common (known or unknown) covariance {\$}\backslashSigma{\$}, by contrasting the indirect approach using variants of linear discriminant analysis (LDA) such as naive Bayes, with the direct approach using corresponding variants of Hotelling's test. Somewhat surprisingly, the indirect approach achieves the same power as the direct approach in terms of {\$}n,d,\backslashdelta,\backslashSigma{\$}, and is only worse by a constant factor, achieving an asymptotic relative efficiency of {\$}1/\backslashpi{\$} for the balanced sample case. Other results of independent interest are provided, such as minimax lower bounds, and optimality of Hotelling's test when {\$}d=o(n){\$}. Simulation results validate our theory, and we present practical takeaway messages along with several open problems.},
archivePrefix = {arXiv},
arxivId = {1602.02210},
author = {Kim, Ilmun and Ramdas, Aaditya and Singh, Aarti and Wasserman, Larry},
eprint = {1602.02210},
journal = {arXiv:1602.02210},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
month = {feb},
title = {{Classification accuracy as a proxy for two sample testing}},
url = {http://arxiv.org/abs/1602.02210},
year = {2016}
}
@article{2016arXiv160903499V,
abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
archivePrefix = {arXiv},
arxivId = {1609.03499},
author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
eprint = {1609.03499},
journal = {arXiv:1609.03499},
keywords = {Computer Science - Learning,Computer Science - Sound},
month = {sep},
title = {{WaveNet: A Generative Model for Raw Audio}},
url = {http://arxiv.org/abs/1609.03499},
year = {2016}
}
@article{2016arXiv161003483M,
abstract = {Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models--models that only specify a stochastic procedure with which to generate data--and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.},
archivePrefix = {arXiv},
arxivId = {1610.03483},
author = {Mohamed, Shakir and Lakshminarayanan, Balaji},
eprint = {1610.03483},
journal = {arXiv:1610.03483},
keywords = {Computer Science - Learning,Statistics - Computation,Statistics - Machine Learning},
month = {oct},
title = {{Learning in Implicit Generative Models}},
url = {http://arxiv.org/abs/1610.03483},
year = {2016}
}
@article{2016arXiv161110242D,
abstract = {We consider the problem of parametric statistical inference when likelihood computations are prohibitively expensive but sampling from the model is possible. Several so-called likelihood-free methods have been developed to perform inference in the absence of a likelihood function. The popular synthetic likelihood approach infers the parameters by modelling summary statistics of the data by a Gaussian probability distribution. In another popular approach called approximate Bayesian computation, the inference is performed by identifying parameter values for which the summary statistics of the simulated data are close to those of the observed data. Synthetic likelihood is easier to use as no measure of `closeness' is required but the Gaussianity assumption is often limiting. Moreover, both approaches require judiciously chosen summary statistics. We here present an alternative inference approach that is as easy to use as synthetic likelihood but not as restricted in its assumptions, and that, in a natural way, enables automatic selection of relevant summary statistic from a large set of candidates. The basic idea is to frame the problem of estimating the posterior as a problem of estimating the ratio between the data generating distribution and the marginal distribution. This problem can be solved by logistic regression, and including regularising penalty terms enables automatic selection of the summary statistics relevant to the inference task. We illustrate the general theory on canonical examples and employ it to perform inference for challenging stochastic nonlinear dynamical systems and high-dimensional summary statistics.},
archivePrefix = {arXiv},
arxivId = {1611.10242},
author = {Thomas, Owen and Dutta, Ritabrata and Corander, Jukka and Kaski, Samuel and Gutmann, Michael U.},
eprint = {1611.10242},
journal = {arXiv:1611.10242},
keywords = {Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
month = {nov},
title = {{Likelihood-free inference by ratio estimation}},
url = {http://arxiv.org/abs/1611.10242},
year = {2016}
}
@article{2017arXiv170604008P,
abstract = {Much of the recent research on solving iterative inference problems focuses on moving away from hand-chosen inference algorithms and towards learned inference. In the latter, the inference process is unrolled in time and interpreted as a recurrent neural network (RNN) which allows for joint learning of model and inference parameters with back-propagation through time. In this framework, the RNN architecture is directly derived from a hand-chosen inference algorithm, effectively limiting its capabilities. We propose a learning framework, called Recurrent Inference Machines (RIM), in which we turn algorithm construction the other way round: Given data and a task, train an RNN to learn an inference algorithm. Because RNNs are Turing complete [1, 2] they are capable to implement any inference algorithm. The framework allows for an abstraction which removes the need for domain knowledge. We demonstrate in several image restoration experiments that this abstraction is effective, allowing us to achieve state-of-the-art performance on image denoising and super-resolution tasks and superior across-task generalization.},
archivePrefix = {arXiv},
arxivId = {1706.04008},
author = {Putzky, Patrick and Welling, Max},
eprint = {1706.04008},
journal = {arXiv:1706.04008},
keywords = {Computer Science - Computer Vision and Pattern Re,Computer Science - Neural and Evolutionary Computi},
month = {jun},
title = {{Recurrent Inference Machines for Solving Inverse Problems}},
url = {http://arxiv.org/abs/1706.04008},
year = {2017}
}
@article{2018arXiv180511542A,
abstract = {In this paper, we introduce a new form of amortized variational inference by using the forward KL divergence in a joint-contrastive variational loss. The resulting forward amortized variational inference is a likelihood-free method as its gradient can be sampled without bias and without requiring any evaluation of either the model joint distribution or its derivatives. We prove that our new variational loss is optimized by the exact posterior marginals in the fully factorized mean-field approximation, a property that is not shared with the more conventional reverse KL inference. Furthermore, we show that forward amortized inference can be easily marginalized over large families of latent variables in order to obtain a marginalized variational posterior. We consider two examples of variational marginalization. In our first example we train a Bayesian forecaster for predicting a simplified chaotic model of atmospheric convection. In the second example we train an amortized variational approximation of a Bayesian optimal classifier by marginalizing over the model space. The result is a powerful meta-classification network that can solve arbitrary classification problems without further training.},
archivePrefix = {arXiv},
arxivId = {1805.11542},
author = {Ambrogioni, Luca and G{\"{u}}{\c{c}}l{\"{u}}, Umut and Berezutskaya, Julia and van den Borne, Eva W. P. and G{\"{u}}{\c{c}}l{\"{u}}t{\"{u}}rk, Yağmur and Hinne, Max and Maris, Eric and van Gerven, Marcel A. J.},
eprint = {1805.11542},
journal = {arXiv:1805.11542},
keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
month = {may},
title = {{Forward Amortized Inference for Likelihood-Free Variational Marginalization}},
url = {http://arxiv.org/abs/1805.11542},
year = {2018}
}
@article{Stoye:2018ovl,
abstract = {We extend recent work (Brehmer, et. al., 2018) that use neural networks as surrogate models for likelihood-free inference. As in the previous work, we exploit the fact that the joint likelihood ratio and joint score, conditioned on both observed and latent variables, can often be extracted from an implicit generative model or simulator to augment the training data for these surrogate models. We show how this augmented training data can be used to provide a new cross-entropy estimator, which provides improved sample efficiency compared to previous loss functions exploiting this augmented training data.},
archivePrefix = {arXiv},
arxivId = {1808.00973},
author = {Stoye, Markus and Brehmer, Johann and Louppe, Gilles and Pavez, Juan and Cranmer, Kyle},
eprint = {1808.00973},
journal = {arXiv:1808.00973},
title = {{Likelihood-free inference with an improved cross-entropy estimator}},
url = {http://arxiv.org/abs/1808.00973},
year = {2018}
}
@article{2018arXiv181009899D,
abstract = {Parametric statistical models that are implicitly defined in terms of a stochastic data generating process are used in a wide range of scientific disciplines because they enable accurate modeling. However, learning the parameters from observed data is generally very difficult because their likelihood function is typically intractable. Likelihood-free Bayesian inference methods have been proposed which include the frameworks of approximate Bayesian computation (ABC), synthetic likelihood, and its recent generalization that performs likelihood-free inference by ratio estimation (LFIRE). A major difficulty in all these methods is choosing summary statistics that reduce the dimensionality of the data to facilitate inference. While several methods for choosing summary statistics have been proposed for ABC, the literature for synthetic likelihood and LFIRE is very thin to date. We here address this gap in the literature, focusing on the important special case of time-series models. We show that convolutional neural networks trained to predict the input parameters from the data provide suitable summary statistics for LFIRE. On a wide range of time-series models, a single neural network architecture produced equally or more accurate posteriors than alternative methods.},
archivePrefix = {arXiv},
arxivId = {1810.09899},
author = {Dinev, Traiko and Gutmann, Michael U.},
eprint = {1810.09899},
journal = {arXiv:1810.09899},
keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
month = {oct},
title = {{Dynamic Likelihood-free Inference via Ratio Estimation (DIRE)}},
url = {http://arxiv.org/abs/1810.09899},
year = {2018}
}
@article{2018arXiv181108723D,
abstract = {Likelihood-free inference refers to inference when a likelihood function cannot be explicitly evaluated, which is often the case for models based on simulators. Most of the literature is based on sample-based `Approximate Bayesian Computation' methods, but recent work suggests that approaches based on deep neural conditional density estimators can obtain state-of-the-art results with fewer simulations. The neural approaches vary in how they choose which simulations to run and what they learn: an approximate posterior or a surrogate likelihood. This work provides some direct controlled comparisons between these choices.},
archivePrefix = {arXiv},
arxivId = {1811.08723},
author = {Durkan, Conor and Papamakarios, George and Murray, Iain},
eprint = {1811.08723},
journal = {arXiv:1811.08723},
keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
month = {nov},
title = {{Sequential Neural Methods for Likelihood-free Inference}},
url = {http://arxiv.org/abs/1811.08723},
year = {2018}
}
@inproceedings{Pesah:2018tbc,
abstract = {Likelihood-free inference is concerned with the estimation of the parameters of a non-differentiable stochastic simulator that best reproduce real observations. In the absence of a likelihood function, most of the existing inference methods optimize the simulator parameters through a handcrafted iterative procedure that tries to make the simulated data more similar to the observations. In this work, we explore whether meta-learning can be used in the likelihood-free context, for learning automatically from data an iterative optimization procedure that would solve likelihood-free inference problems. We design a recurrent inference machine that learns a sequence of parameter updates leading to good parameter estimates, without ever specifying some explicit notion of divergence between the simulated data and the real data distributions. We demonstrate our approach on toy simulators, showing promising results both in terms of performance and robustness.},
archivePrefix = {arXiv},
arxivId = {1811.12932},
author = {Pesah, Arthur and Wehenkel, Antoine and Louppe, Gilles},
booktitle = {arXiv:1811.12932},
eprint = {1811.12932},
title = {{Recurrent machines for likelihood-free inference}},
url = {http://arxiv.org/abs/1811.12932},
year = {2018}
}
@article{Hermans:2019ioj,
abstract = {Posterior inference with an intractable likelihood is becoming an increasingly common task in scientific domains which rely on sophisticated computer simulations. Typically, these forward models do not admit tractable densities forcing practitioners to rely on approximations. This work introduces a novel approach to address the intractability of the likelihood and the marginal model. We achieve this by learning a flexible amortized estimator which approximates the likelihood-to-evidence ratio. We demonstrate that the learned ratio estimator can be embedded in MCMC samplers to approximate likelihood-ratios between consecutive states in the Markov chain, allowing us to draw samples from the intractable posterior. Techniques are presented to improve the numerical stability and to measure the quality of an approximation. The accuracy of our approach is demonstrated on a variety of benchmarks against well-established techniques. Scientific applications in physics show its applicability.},
archivePrefix = {arXiv},
arxivId = {1903.04057},
author = {Hermans, Joeri and Begy, Volodimir and Louppe, Gilles},
eprint = {1903.04057},
journal = {arXiv:1903.04057},
title = {{Likelihood-free MCMC with Amortized Approximate Ratio Estimators}},
url = {http://arxiv.org/abs/1903.04057},
year = {2019}
}
@article{2019arXiv190511505D,
abstract = {Complex phenomena in engineering and the sciences are often modeled with computationally intensive feed-forward simulations for which a tractable analytic likelihood does not exist. In these cases, it is sometimes necessary to estimate an approximate likelihood or fit a fast emulator model for efficient statistical inference; such surrogate models include Gaussian synthetic likelihoods and more recently neural density estimators such as autoregressive models and normalizing flows. To date, however, there is no consistent way of quantifying the quality of such a fit. Here we propose a statistical framework that can distinguish any arbitrary misspecified model from the target likelihood, and that in addition can identify with statistical confidence the regions of parameter as well as feature space where the fit is inadequate. Our validation method applies to settings where simulations are extremely costly and generated in batches or "ensembles" at fixed locations in parameter space. At the heart of our approach is a two-sample test that quantifies the quality of the fit at fixed parameter values, and a global test that assesses goodness-of-fit across simulation parameters. While our general framework can incorporate any test statistic or distance metric, we specifically argue for a new two-sample test that can leverage any regression method to attain high power and provide diagnostics in complex data settings.},
archivePrefix = {arXiv},
arxivId = {1905.11505},
author = {Dalmasso, Niccol{\`{o}} and Lee, Ann B. and Izbicki, Rafael and Pospisil, Taylor and Kim, Ilmun and Lin, Chieh-An},
eprint = {1905.11505},
journal = {arXiv:1905.11505},
keywords = {Statistics - Machine Learning,Statistics - Methodology},
month = {may},
title = {{Validation of Approximate Likelihood and Emulator Models for Computationally Intensive Simulations}},
url = {http://arxiv.org/abs/1905.11505},
year = {2019}
}
@article{2019arXiv190602145D,
archivePrefix = {arXiv},
arxivId = {stat.ML/1906.02145},
author = {Durkan, Conor and Bekasov, Artur and Murray, Iain and Papamakarios, George},
eprint = {1906.02145},
journal = {arXiv:1906.02145},
keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
month = {jun},
primaryClass = {stat.ML},
title = {{Cubic-Spline Flows}},
year = {2019}
}
@article{2019arXiv190604032D,
abstract = {A normalizing flow models a complex probability density as an invertible transformation of a simple base density. Flows based on either coupling or autoregressive transforms both offer exact density evaluation and sampling, but rely on the parameterization of an easily invertible elementwise transformation, whose choice determines the flexibility of these models. Building upon recent work, we propose a fully-differentiable module based on monotonic rational-quadratic splines, which enhances the flexibility of both coupling and autoregressive transforms while retaining analytic invertibility. We demonstrate that neural spline flows improve density estimation, variational inference, and generative modeling of images.},
archivePrefix = {arXiv},
arxivId = {1906.04032},
author = {Durkan, Conor and Bekasov, Artur and Murray, Iain and Papamakarios, George},
eprint = {1906.04032},
journal = {arXiv:1906.04032},
keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
month = {jun},
title = {{Neural Spline Flows}},
url = {http://arxiv.org/abs/1906.04032},
year = {2019}
}
@article{Andreassen:2019nnm,
abstract = {Precise scientific analysis in collider-based particle physics is possible because of complex simulations that connect fundamental theories to observable quantities. The significant computational cost of these programs limits the scope, precision, and accuracy of Standard Model measurements and searches for new phenomena. We therefore introduce Deep neural networks using Classification for Tuning and Reweighting (DCTR), a neural network-based approach to reweight and fit simulations using all kinematic and flavor information -- the full phase space. DCTR can perform tasks that are currently not possible with existing methods, such as estimating non-perturbative fragmentation uncertainties. The core idea behind the new approach is to exploit powerful high-dimensional classifiers to reweight phase space as well as to identify the best parameters for describing data. Numerical examples from {\$}e{\^{}}+e{\^{}}-\backslashrightarrow\backslashtext{\{}jets{\}}{\$} demonstrate the fidelity of these methods for simulation parameters that have a big and broad impact on phase space as well as those that have a minimal and/or localized impact. The high fidelity of the full phase-space reweighting enables a new paradigm for simulations, parameter tuning, and model systematic uncertainties across particle physics and possibly beyond.},
archivePrefix = {arXiv},
arxivId = {1907.08209},
author = {Andreassen, Anders and Nachman, Benjamin},
eprint = {1907.08209},
journal = {arXiv:1907.08209},
title = {{Neural Networks for Full Phase-space Reweighting and Parameter Tuning}},
url = {http://arxiv.org/abs/1907.08209},
year = {2019}
}
@article{munk2019deep,
abstract = {We present a framework for automatically structuring and training fast, approximate, deep neural surrogates of existing stochastic simulators. Unlike traditional approaches to surrogate modeling, our surrogates retain the interpretable structure of the reference simulators. The particular way we achieve this allows us to replace the reference simulator with the surrogate when undertaking amortized inference in the probabilistic programming sense. The fidelity and speed of our surrogates allow for not only faster "forward" stochastic simulation but also for accurate and substantially faster inference. We support these claims via experiments that involve a commercial composite-materials curing simulator. Employing our surrogate modeling technique makes inference an order of magnitude faster, opening up the possibility of doing simulator-based, non-invasive, just-in-time parts quality testing; in this case inferring safety-critical latent internal temperature profiles of composite materials undergoing curing from surface temperature profile measurements.},
archivePrefix = {arXiv},
arxivId = {1910.11950},
author = {Munk, Andreas and {\'{S}}cibior, Adam and Baydin, Atılım G{\"{u}}neş and Stewart, Andrew and Fernlund, Goran and Poursartip, Anoush and Wood, Frank},
eprint = {1910.11950},
journal = {arXiv:1910.11950},
title = {{Deep Probabilistic Surrogate Networks for Universal Simulator Approximation}},
url = {http://arxiv.org/abs/1910.11950},
year = {2019}
}
@article{2020arXiv200305033A,
abstract = {Legendre duality provides a variational lower-bound for the Kullback-Leibler divergence (KL) which can be estimated using samples, without explicit knowledge of the density ratio. We use this estimator, the $\backslash$textit{\{}KL Approximate Lower-bound Estimate{\}} (KALE), in a contrastive setting for learning energy-based models, and show that it provides a maximum likelihood estimate (MLE). We then extend this procedure to adversarial training, where the discriminator represents the energy and the generator is the base measure of the energy-based model. Unlike in standard generative adversarial networks (GANs), the learned model makes use of both generator and discriminator to generate samples. This is achieved using Hamiltonian Monte Carlo in the latent space of the generator, using information from the discriminator, to find regions in that space that produce better quality samples. We also show that, unlike the KL, KALE enjoys smoothness properties that make it suitable for adversarial training, and provide convergence rates for KALE when the negative log density ratio belongs to the variational family. Finally, we demonstrate the effectiveness of this approach on simple datasets.},
archivePrefix = {arXiv},
arxivId = {2003.05033},
author = {Arbel, Michael and Zhou, Liang and Gretton, Arthur},
eprint = {2003.05033},
journal = {arXiv:2003.05033},
keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
month = {mar},
title = {{KALE: When Energy-Based Learning Meets Adversarial Training}},
url = {http://arxiv.org/abs/2003.05033},
year = {2020}
}
@misc{hubble-image,
author = "{Hubble Space Telescope}",
booktitle = {Astronomy},
howpublished = {\url{https://apod.nasa.gov/apod/image/1112/lensshoe\_hubble\_3235.jpg}},
pages = {1--40},
title = {{Based on observations made with the NASA/ESA}},
year = {2011}
}
@article{2017arXiv170400520J,
abstract = {Approximate Bayesian computation (ABC) is a method for Bayesian inference when the likelihood is unavailable but simulating from the model is possible. However, many ABC algorithms require a large number of simulations, which can be costly. To reduce the computational cost, Bayesian optimisation (BO) and surrogate models such as Gaussian processes have been proposed. Bayesian optimisation enables one to intelligently decide where to evaluate the model next but common BO strategies are not designed for the goal of estimating the posterior distribution. Our paper addresses this gap in the literature. We propose to compute the uncertainty in the ABC posterior density, which is due to a lack of simulations to estimate this quantity accurately, and define a loss function that measures this uncertainty. We then propose to select the next evaluation location to minimise the expected loss. Experiments show that the proposed method often produces the most accurate approximations as compared to common BO strategies.},
archivePrefix = {arXiv},
arxivId = {1704.00520},
author = {J{\"{a}}rvenp{\"{a}}{\"{a}}, Marko and Gutmann, Michael U. and Pleska, Arijus and Vehtari, Aki and Marttinen, Pekka},
doi = {10.1214/18-BA1121},
eprint = {1704.00520},
issn = {19316690},
journal = {Bayesian Analysis},
keywords = {Approximate Bayesian computation,Bayesian optimisation,Gaussian processes,Intractable likelihood,Sequential experiment design},
month = {apr},
number = {2},
pages = {595--622},
title = {{Efficient acquisition rules for model-based approximate Bayesian computation}},
volume = {14},
year = {2019}
}
@article{arulampalam2002tutorial,
abstract = {Increasingly, for many application areas, it is becoming important to include elements of nonlinearity and non-Gaussianity in order to model accurately the underlying dynamics of a physical system. Moreover, it is typically crucial to process data on-line as it arrives, both from the point of view of storage costs as well as for rapid adaptation to changing signal characteristics. In this paper, we review both optimal and suboptimal Bayesian algorithms for nonlinear/non-Gaussian tracking problems, with a focus on particle filters. Particle filters are sequential Monte Carlo methods based on point mass (or “particle”) representations of probability densities, which can be applied to any state-space model and which generalize the traditional Kalman filtering methods. Several variants of the particle filter such as SIR, ASIR, and RPF are introduced within a generic framework of the sequential importance sampling (SIS) algorithm. These are discussed and compared with the standard EKF through an illustrative example.},
author = {Arulampalam, M. Sanjeev and Maskell, Simon and Gordon, Neil and Clapp, Tim},
doi = {10.1109/9780470544198.ch73},
isbn = {9780470544198},
journal = {Bayesian Bounds for Parameter Estimation and Nonlinear Filtering/Tracking},
keywords = {Approximation algorithms,Approximation methods,Bayesian methods,Filtering algorithms,Particle filters},
number = {2},
pages = {723--737},
publisher = {Ieee},
title = {{A tutorial on particle filters for online nonlinear/nongaussian bayesian tracking}},
volume = {50},
year = {2007}
}
@article{Hastings1970,
abstract = {SUMMARY: A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed. {\textcopyright} 1970 Oxford University Press.},
author = {Hastings, W. K.},
doi = {10.1093/biomet/57.1.97},
issn = {00063444},
journal = {Biometrika},
number = {1},
pages = {97--109},
title = {{Monte carlo sampling methods using Markov chains and their applications}},
volume = {57},
year = {1970}
}
@article{2019arXiv190805164W,
abstract = {Monotonic neural networks have recently been proposed as a way to define invertible transformations. These transformations can be combined into powerful autoregressive flows that have been shown to be universal approximators of continuous probability distributions. Architectures that ensure monotonicity typically enforce constraints on weights and activation functions, which enables invertibility but leads to a cap on the expressiveness of the resulting transformations. In this work, we propose the Unconstrained Monotonic Neural Network (UMNN) architecture based on the insight that a function is monotonic as long as its derivative is strictly positive. In particular, this latter condition can be enforced with a free-form neural network whose only constraint is the positiveness of its output. We evaluate our new invertible building block within a new autoregressive flow (UMNN-MAF) and demonstrate its eectiveness on density estimation experiments. We also illustrate the ability of UMNNs to improve variational inference.},
archivePrefix = {arXiv},
arxivId = {1908.05164},
author = {Wehenkel, Antoine and Louppe, Gilles},
eprint = {1908.05164},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {Density estimator,Invertible networks,Normalizing flow,Unsupervised learning},
month = {aug},
pages = {arXiv:1908.05164},
title = {{Unconstrained monotonic neural networks}},
volume = {2491},
year = {2019}
}
@inproceedings{2017arXiv170707113L,
abstract = {Complex computer simulators are increasingly used across fields of science as generative models tying parameters of an underlying theory to experimental observations. Inference in this setup is often difficult, as simulators rarely admit a tractable density or likelihood function. We introduce Adversarial Variational Optimization (AVO), a likelihood-free inference algorithm for fitting a non-differentiable generative model incorporating ideas from generative adversarial networks, variational optimization and empirical Bayes. We adapt the training procedure of generative adversarial networks by replacing the differentiable generative network with a domain-specific simulator. We solve the resulting non-differentiable minimax problem by minimizing variational upper bounds of the two adversarial objectives. Effectively, the procedure results in learning a proposal distribution over simulator parameters, such that the JS divergence between the marginal distribution of the synthetic data and the empirical distribution of observed data is minimized. We evaluate and compare the method with simulators producing both discrete and continuous data.},
archivePrefix = {arXiv},
arxivId = {1707.07113},
author = {Louppe, Gilles and Hermans, Joeri and Cranmer, Kyle},
booktitle = {CEUR Workshop Proceedings},
editor = {Chaudhuri, Kamalika and Sugiyama, Masashi},
eprint = {1707.07113},
issn = {16130073},
keywords = {Likelihood-free inference,Physics,Simulator-based inference},
pages = {1438--1447},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Adversarial variational optimization of non-differentiable simulators}},
url = {http://proceedings.mlr.press/v89/louppe19a.html},
volume = {2491},
year = {2019}
}
@article{deCastro:2018mgh,
abstract = {Complex computer simulations are commonly required for accurate data modelling in many scientific disciplines, making statistical inference challenging due to the intractability of the likelihood evaluation for the observed data. Furthermore, sometimes one is interested on inference drawn over a subset of the generative model parameters while taking into account model uncertainty or misspecification on the remaining nuisance parameters. In this work, we show how non-linear summary statistics can be constructed by minimising inference-motivated losses via stochastic gradient descent such that they provide the smallest uncertainty for the parameters of interest. As a use case, the problem of confidence interval estimation for the mixture coefficient in a multi-dimensional two-component mixture model (i.e. signal vs background) is considered, where the proposed technique clearly outperforms summary statistics based on probabilistic classification, a commonly used alternative which does not account for the presence of nuisance parameters.},
archivePrefix = {arXiv},
arxivId = {1806.04743},
author = {de Castro, Pablo and Dorigo, Tommaso},
doi = {10.1016/j.cpc.2019.06.007},
eprint = {1806.04743},
issn = {00104655},
journal = {Computer Physics Communications},
keywords = {High energy physics,Likelihood-free inference,Neural networks,Nuisance parameters},
pages = {170--179},
title = {{INFERNO: Inference-Aware Neural Optimisation}},
volume = {244},
year = {2019}
}
@article{Brehmer:2019xox,
abstract = {Precision measurements at the LHC often require analyzing high-dimensional event data for subtle kinematic signatures, which is challenging for established analysis methods. Recently, a powerful family of multivariate inference techniques that leverage both matrix element information and machine learning has been developed. This approach neither requires the reduction of high-dimensional data to summary statistics nor any simplifications to the underlying physics or detector response. In this paper we introduce MadMiner, a Python module that streamlines the steps involved in this procedure. Wrapping around MadGraph5{\_}aMC and Pythia 8, it supports almost any physics process and model. To aid phenomenological studies, the tool also wraps around Delphes 3, though it is extendable to a full Geant4-based detector simulation. We demonstrate the use of MadMiner in an example analysis of dimension-six operators in ttH production, finding that the new techniques substantially increase the sensitivity to new physics.},
archivePrefix = {arXiv},
arxivId = {1907.10621},
author = {Brehmer, Johann and Kling, Felix and Espejo, Irina and Cranmer, Kyle},
doi = {10.1007/s41781-020-0035-2},
eprint = {1907.10621},
issn = {2510-2036},
journal = {Computing and Software for Big Science},
number = {1},
title = {{MadMiner: Machine Learning-Based Inference for Particle Physics}},
volume = {4},
year = {2020}
}
@article{olah2018the,
abstract = {Preface IBM{\textregistered} SPSS{\textregistered} Statistics is a comprehensive system for analyzing data. The Regression optional add-on module provides the additional analytic techniques described in this manual. The Regression add-on module must be used with the SPSS Statistics Core system and is completely integrated into that system. About IBM Business Analytics IBM Business Analytics software delivers complete, consistent and accurate information that decision-makers trust to improve business performance. A comprehensive portfolio of business intelligence, predictive analytics, financial performance and strategy management, and analytic applications provides clear, immediate and actionable insights into current performance and the ability to predict future outcomes. Combined with rich industry solutions, proven practices and professional services, organizations of every size can drive the highest productivity, confidently automate decisions and deliver better results. As part of this portfolio, IBM SPSS Predictive Analytics software helps organizations predict future events and proactively act upon that insight to drive better business outcomes. Commercial, government and academic customers worldwide rely on IBM SPSS technology as a competitive advantage in attracting, retaining and growing customers, while reducing fraud and mitigating risk. By incorporating IBM SPSS software into their daily operations, organizations become predictive enterprises – able to direct and automate decisions to meet business goals and achieve measurable competitive advantage. For further information or to reach a representative visit},
annote = {https://distill.pub/2018/building-blocks},
author = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
doi = {10.23915/distill.00010},
journal = {Distill},
number = {3},
title = {{The Building Blocks of Interpretability}},
volume = {3},
year = {2018}
}
@article{2018arXiv181208927K,
abstract = {Two-sample testing is a fundamental problem in statistics. Despite its long history, there has been renewed interest in this problem with the advent of high-dimensional and complex data. Specifically, in the machine learning literature, there have been recent methodological developments such as classification accuracy tests. The goal of this work is to present a regression approach to comparing multivariate distributions of complex data. Depending on the chosen regression model, our framework can efficiently handle different types of variables and various structures in the data, with competitive power under many practical scenarios. Whereas previous work has been largely limited to global tests which conceal much of the local information, our approach naturally leads to a local two-sample testing framework in which we identify local differences between multivariate distributions with statistical confidence. We demonstrate the efficacy of our approach both theoretically and empirically, under some well-known parametric and nonparametric regression methods. Our proposed methods are applied to simulated data as well as a challenging astronomy data set to assess their practical usefulness.},
archivePrefix = {arXiv},
arxivId = {1812.08927},
author = {Kim, Ilmun and Lee, Ann B. and Lei, Jing},
doi = {10.1214/19-EJS1648},
eprint = {1812.08927},
issn = {19357524},
journal = {Electronic Journal of Statistics},
keywords = {Galaxy morphology,Intrinsic dimension,Kernel regression,Nearest neighbor regression,Permutation test,Random forests},
month = {dec},
number = {2},
pages = {5253--5305},
title = {{Global and local two-sample tests via regression}},
volume = {13},
year = {2019}
}
@article{graham2017asymptotically,
abstract = {Many generative models can be expressed as a differentiable function applied to input variables sampled from a known probability distribution. This framework includes both the generative component of learned parametric models such as variational autoencoders and generative adversarial networks, and also procedurally defined simulator models which involve only differentiable operations. Though the distribution on the input variables to such models is known, often the distribution on the output variables is only implicitly defined. We present a method for performing efficient Markov chain Monte Carlo inference in such models when conditioning on observations of the model output. For some models this offers an asymptotically exact inference method where approximate Bayesian computation might otherwise be employed. We use the intuition that computing conditional expectations is equivalent to integrating over a density defined on the manifold corresponding to the set of inputs consistent with the observed outputs. This motivates the use of a constrained variant of Hamiltonian Monte Carlo which leverages the smooth geometry of the manifold to move between inputs exactly consistent with observations. We validate the method by performing inference experiments in a diverse set of models.},
archivePrefix = {arXiv},
arxivId = {1605.07826},
author = {Graham, Matthew M. and Storkey, Amos J.},
doi = {10.1214/17-EJS1340SI},
eprint = {1605.07826},
issn = {19357524},
journal = {Electronic Journal of Statistics},
keywords = {Approximate Bayesian computation,Generative models,Implicit models,Markov chain Monte Carlo},
number = {2},
pages = {5105--5164},
publisher = {The Institute of Mathematical Statistics and the Bernoulli Society},
title = {{Asymptotically exact inference in differentiable generative models}},
volume = {11},
year = {2017}
}
@article{Cowan:2010js,
abstract = {We describe likelihood-based statistical tests for use in high energy physics for the discovery of new phenomena and for construction of confidence intervals on model parameters. We focus on the properties of the test procedures that allow one to account for systematic uncertainties. Explicit formulae for the asymptotic distributions of test statistics are derived using results ofWilks andWald.We motivate and justify the use of a representative data set, called the “Asimov data set”, which provides a simple method to obtain the median experimental sensitivity of a search or measurement as well as fluctuations about this expectation.},
annote = {[Erratum: Eur.$\backslash$ Phys.$\backslash$ J.$\backslash$ C73, p.$\backslash$ 2501, 2013]},
archivePrefix = {arXiv},
arxivId = {1007.1727},
author = {Cowan, Glen and Cranmer, Kyle and Gross, Eilam and Vitells, Ofer},
doi = {10.1140/epjc/s10052-011-1554-0},
eprint = {1007.1727},
issn = {14346052},
journal = {European Physical Journal C},
number = {2},
pages = {1554},
title = {{Asymptotic formulae for likelihood-based tests of new physics}},
volume = {71},
year = {2011}
}
@article{Andreassen:2018apy,
abstract = {In applications of machine learning to particle physics, a persistent challenge is how to go beyond discrimination to learn about the underlying physics. To this end, a powerful tool would be a framework for unsupervised learning, where the machine learns the intricate high-dimensional contours of the data upon which it is trained, without reference to pre-established labels. In order to approach such a complex task, an unsupervised network must be structured intelligently, based on a qualitative understanding of the data. In this paper, we scaffold the neural network's architecture around a leading-order model of the physics underlying the data. In addition to making unsupervised learning tractable, this design actually alleviates existing tensions between performance and interpretability. We call the framework Junipr: “Jets from UNsupervised Interpretable PRobabilistic models”. In this approach, the set of particle momenta composing a jet are clustered into a binary tree that the neural network examines sequentially. Training is unsupervised and unrestricted: the network could decide that the data bears little correspondence to the chosen tree structure. However, when there is a correspondence, the network's output along the tree has a direct physical interpretation. Junipr models can perform discrimination tasks, through the statistically optimal likelihood-ratio test, and they permit visualizations of discrimination power at each branching in a jet's tree. Additionally, Junipr models provide a probability distribution from which events can be drawn, providing a data-driven Monte Carlo generator. As a third application, Junipr models can reweight events from one (e.g. simulated) data set to agree with distributions from another (e.g. experimental) data set.},
archivePrefix = {arXiv},
arxivId = {1804.09720},
author = {Andreassen, Anders and Feige, Ilya and Frye, Christopher and Schwartz, Matthew D.},
doi = {10.1140/epjc/s10052-019-6607-9},
eprint = {1804.09720},
issn = {14346052},
journal = {European Physical Journal C},
number = {2},
pages = {102},
title = {{JUNIPR: a framework for unsupervised machine learning in particle physics}},
volume = {79},
year = {2019}
}
@inproceedings{Gordon2014ProbabilisticP,
abstract = {Probabilistic programs are usual functional or imperative programs with two added constructs: (1) the ability to draw values at random from distributions, and (2) the ability to condition values of variables in a program via observations. Models from diverse application areas such as computer vision, coding theory, cryptographic protocols, biology and reliability analysis can be written as probabilistic programs. Probabilistic inference is the problem of computing an explicit representation of the probability distribution implicitly specified by a probabilistic program. Depending on the application, the desired output from inference may vary-we may want to estimate the expected value of some function f with respect to the distribution, or the mode of the distribution, or simply a set of samples drawn from the distribution. In this paper, we describe connections this research area called $\backslash$Probabilistic Programming" has with programming languages and software engineering, and this includes language design, and the static and dynamic analysis of programs. We survey current state of the art and speculate on promising directions for future research.},
author = {Gordon, Andrew D. and Henzinger, Thomas A. and Nori, Aditya V. and Rajamani, Sriram K.},
booktitle = {Future of Software Engineering, FOSE 2014 - Proceedings},
doi = {10.1145/2593882.2593900},
isbn = {9781450328654},
keywords = {Machine learning,Probabilistic programming,Program analysis},
pages = {167--181},
title = {{Probabilistic programming}},
year = {2014}
}
@article{beaumont2002approximate,
abstract = {We propose a new method for approximate Bayesian statistical inference on the basis of summary statistics. The method is suited to complex problems that arise in population genetics, extending ideas developed in this setting by earlier authors. Properties of the posterior distribution of a parameter, such as its mean or density curve, are approximated without explicit likelihood calculations. This is achieved by fitting a local-linear regression of simulated parameter values on simulated summary statistics, and then substituting the observed summary statistics into the regression equation. The method combines many of the advantages of Bayesian statistical inference with the computational efficiency of methods based on summary statistics. A key advantage of the method is that the nuisance parameters are automatically integrated out in the simulation step, so that the large numbers of nuisance parameters that arise in population genetics problems can be handled without difficulty. Simulation results indicate computational and statistical efficiency that compares favorably with those of alternative methods previously proposed in the literature. We also compare the relative efficiency of inferences obtained using methods based on summary statistics with those obtained directly from the data using MCMC.},
author = {Beaumont, Mark A. and Zhang, Wenyang and Balding, David J.},
doi = {10.1111/j.1937-2817.2010.tb01236.x},
issn = {00166731},
journal = {Genetics},
number = {4},
pages = {2025--2035},
pmid = {12524368},
publisher = {Genetics Soc America},
title = {{Approximate Bayesian computation in population genetics}},
volume = {162},
year = {2002}
}
@misc{fritz2017,
author = {Goodman, Noah and Blingham, Eli and Chen, Jonathan P. and Jankowiak, Martin and Karaletsos, Theofanis and Obermeyer, Fritz and Pradhan, Neeraj and Singh, Rohit and Szerlip, Paul},
booktitle = {GitHub repository},
publisher = {GitHub},
title = {{Pyro, a Deep Probabilistic Programming Language}},
url = {http://pyro.ai/},
year = {2017}
}
@book{sisson2018handbook,
abstract = {As the world becomes increasingly complex, so do the statistical models required to analyse the challenging problems ahead. For the very first time in a single volume, the Handbook of Approximate Bayesian Computation (ABC) presents an extensive overview of the theory, practice and application of ABC methods. These simple, but powerful statistical techniques, take Bayesian statistics beyond the need to specify overly simplified models, to the setting where the model is defined only as a process that generates data. This process can be arbitrarily complex, to the point where standard Bayesian techniques based on working with tractable likelihood functions would not be viable. ABC methods finesse the problem of model complexity within the Bayesian framework by exploiting modern computational power, thereby permitting approximate Bayesian analyses of models that would otherwise be impossible to implement. The Handbook of ABC provides illuminating insight into the world of Bayesian modelling for intractable models for both experts and newcomers alike. It is an essential reference book for anyone interested in learning about and implementing ABC techniques to analyse complex models in the modern world.},
author = {Sisson, Scott A.},
booktitle = {Handbook of Approximate Bayesian Computation},
doi = {10.1201/9781315117195},
publisher = {Chapman and Hall/CRC},
title = {{Handbook of Approximate Bayesian Computation}},
year = {2018}
}
@article{doucet2009tutorial,
abstract = {Optimal estimation problems for non-linear non-Gaussian state-space models do not typically admit analytic solutions. Since their introduction in 1993, particle filtering methods have become a very popular class of algorithms to solve these estimation problems numerically in an online manner, i.e. recursively as observations become available, and are now routinely used in fields as diverse as computer vision, econometrics, robotics and navigation. The objective of this tutorial is to provide a complete, up-to-date survey of this field as of 2008. Basic and advanced particle methods for filtering as well as smoothing are presented.},
author = {Doucet, Arnaud and Johansen, A M},
journal = {Handbook of Nonlinear Filtering},
number = {December},
pages = {4--6},
title = {{A tutorial on particle filtering and smoothing: Fifteen years later}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.157.772{\&}rep=rep1{\&}type=pdf},
volume = {12},
year = {2009}
}
@article{kanamori2011f,
abstract = {A density ratio is defined by the ratio of two probability densities. We study the inference problem of density ratios and apply a semiparametric density-ratio estimator to the two-sample homogeneity test. In the proposed test procedure, the f-divergence between two probability densities is estimated using a density-ratio estimator. The f-divergence estimator is then exploited for the two-sample homogeneity test. We derive an optimal estimator of f-divergence in the sense of the asymptotic variance in a semiparametric setting, and provide a statistic for two-sample homogeneity test based on the optimal estimator. We prove that the proposed test dominates the existing empirical likelihood score test. Through numerical studies, we illustrate the adequacy of the asymptotic theory for finite-sample inference. {\textcopyright} 2011 IEEE.},
archivePrefix = {arXiv},
arxivId = {1010.4945},
author = {Kanamori, Takafumi and Suzuki, Taiji and Sugiyama, Masashi},
doi = {10.1109/TIT.2011.2163380},
eprint = {1010.4945},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Asymptotic expansion,Density ratio,Divergence,Semiparametric model,Two-sample test},
number = {2},
pages = {708--720},
publisher = {IEEE},
title = {{F-Divergence estimation and two-sample homogeneity test under semiparametric density-ratio models}},
volume = {58},
year = {2012}
}
@article{2019arXiv190703382G,
abstract = {Probabilistic programming languages (PPLs) are receiving widespread attention for performing Bayesian inference in complex generative models. However, applications to science remain limited because of the impracticability of rewriting complex scientific simulators in a PPL, the computational cost of inference, and the lack of scalable implementations. To address these, we present a novel PPL framework that couples directly to existing scientific simulators through a cross-platform probabilistic execution protocol and provides Markov chain Monte Carlo (MCMC) and deep-learning-based inference compilation (IC) engines for tractable inference. To guide IC inference, we perform distributed training of a dynamic 3DCNN-LSTM architecture with a PyTorch-MPI-based framework on 1,024 32-core CPU nodes of the Cori supercomputer with a global mini-batch size of 128k: achieving a performance of 450 Tflop/s through enhancements to PyTorch. We demonstrate a Large Hadron Collider (LHC) use-case with the C++ Sherpa simulator and achieve the largest-scale posterior inference in a Turing-complete PPL.},
archivePrefix = {arXiv},
arxivId = {1907.03382},
author = {Baydin, Atilim G{\"{u}}ne and Shao, Lei and Bhimji, Wahid and Heinrich, Lukas and Meadows, Lawrence and Liu, Jialin and Munk, Andreas and Naderiparizi, Saeid and Gram-Hansen, Bradley and Louppe, Gilles and Ma, Mingfei and Zhao, Xiaohui and Torr, Philip and Lee, Victor and Cranmer, Kyle and Prabhat and Wood, Frank},
doi = {10.1145/3295500.3356180},
eprint = {1907.03382},
isbn = {9781450362290},
issn = {21674337},
journal = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
keywords = {Deep learning,Inference,Probabilistic programming,Simulation},
month = {jul},
pages = {arXiv:1907.03382},
title = {{Etalumis: Bringing probabilistic programming to scientific simulators at scale}},
year = {2019}
}
@article{2018arXiv180507226P,
abstract = {We present Sequential Neural Likelihood (SNL), a new method for Bayesian inference in simulator models, where the likelihood is intractable but simulating data from the model is possible. SNL trains an autoregressive flow on simulated data in order to learn a model of the likelihood in the region of high posterior density. A sequential training procedure guides simulations and reduces simulation cost by orders of magnitude. We show that SNL is more robust, more accurate and requires less tuning than related neural-based methods, and we discuss diagnostics for assessing calibration, convergence and goodness-of-fit.},
archivePrefix = {arXiv},
arxivId = {1805.07226},
author = {Papamakarios, George and Sterratt, David C. and Murray, Iain},
eprint = {1805.07226},
journal = {International Conference on Artificial Intelligence and Statistics},
title = {{Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows}},
url = {http://arxiv.org/abs/1805.07226},
year = {2018}
}
@article{szekely2004testing,
abstract = {We propose a new nonparametric test for equality of two or more multivariate distributions based on Euclidean distance between sample elements. Several consistent tests for comparing multivariate distributions can be developed from the underlying theoretical results. The test procedure for the multisample problem is developed and applied for testing the composite hypothesis of equal distributions, when distributions are unspecified. The proposed test is universally consistent against all fixed alternatives (not necessarily continuous) with finite second moments. The test is implemented by conditioning on the pooled sample to obtain an approximate permutation test, which is distribution free. Our Monte Carlo power study suggests that the new test may be much more sensitive than tests based on nearest neighbors against several classes of alternatives, and performs particularly well in high dimension. Computational complexity of our test procedure is independent of dimension and number of populations sampled. The test is applied in a high dimensional problem, testing microarray data from cancer samples.},
author = {Szekely, Gabor J. and Rizzo, Maria L.},
doi = {10.1.1.226.377},
journal = {InterStat},
keywords = {e-distance,e-statistics,energy statistics,homogeneity,multisample problem,permu-,tation test,two-sample problem},
number = {11},
pages = {1249--1272},
title = {{Testing for Equal Distributions in High Dimension}},
volume = {10},
year = {2004}
}
@book{tarantola2005inverse,
abstract = {This paper concerns the ultrasonic characterization of human cancellous bone samples by solving the inverse problem using experimental transmitted signals. The ultrasonic propagation in cancellous bone is modeled using the Biot theory modified by the Johnson et al. model for viscous exchange between fluid and structure. The sensitivity of the Young modulus and the Poisson ratio of the skeletal frame is studied showing their effect on the fast and slow wave forms. The inverse problem is solved numerically by the least squares method. Five parameters are inverted: the porosity, tortuosity, viscous characteristic length, Young modulus, and Poisson ratio of the skeletal frame. The minimization of the discrepancy between experiment and theory is made in the time domain. The inverse problem is shown to be well posed, and its solution to be unique. Experimental results for slow and fast waves transmitted through human cancellous bone samples are given and compared with theoretical predictions.},
author = {Tarantola, Albert},
booktitle = {Inverse Problem Theory and Methods for Model Parameter Estimation},
doi = {10.1137/1.9780898717921},
publisher = {siam},
title = {{Inverse Problem Theory and Methods for Model Parameter Estimation}},
volume = {89},
year = {2005}
}
@article{2018arXiv180505480I,
abstract = {Approximate Bayesian computation (ABC) is typically used when the likelihood is either unavailable or intractable but where data can be simulated under different parameter settings using a forward model. Despite the recent interest in ABC, high-dimensional data and costly simulations still remain a bottleneck in some applications. There is also no consensus as to how to best assess the performance of such methods without knowing the true posterior. We show how a nonparametric conditional density estimation (CDE) framework, which we refer to as ABC–CDE, help address three nontrivial challenges in ABC: (i) how to efficiently estimate the posterior distribution with limited simulations and different types of data, (ii) how to tune and compare the performance of ABC and related methods in estimating the posterior itself, rather than just certain properties of the density, and (iii) how to efficiently choose among a large set of summary statistics based on a CDE surrogate loss. We provide theoretical and empirical evidence that justify ABC–CDE procedures that directly estimate and assess the posterior based on an initial ABC sample, and we describe settings where standard ABC and regression-based approaches are inadequate. Supplemental materials for this article are available online.},
archivePrefix = {arXiv},
arxivId = {1805.05480},
author = {Izbicki, Rafael and Lee, Ann B. and Pospisil, Taylor},
doi = {10.1080/10618600.2018.1546594},
eprint = {1805.05480},
issn = {15372715},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Approximate Bayesian computation,Conditional density estimation,Likelihood-free inference,Nonparametric methods},
number = {3},
pages = {481--492},
publisher = {Taylor {\&} Francis},
title = {{ABC–CDE: Toward Approximate Bayesian Computation With Complex High-Dimensional Data and Limited Simulations}},
volume = {28},
year = {2019}
}
@article{gelman2015stan,
abstract = {Stan is a free and open-source C++ program that performs Bayesian inference or optimization for arbitrary user-specified models and can be called from the command line, R, Python, Matlab, or Julia and has great promise for fitting large and complex statistical models in many areas of application. We discuss Stan from users' and developers' perspectives and illustrate with a simple but nontrivial nonlinear regression example.},
author = {Gelman, Andrew and Lee, Daniel and Guo, Jiqiang},
doi = {10.3102/1076998615606113},
issn = {10769986},
journal = {Journal of Educational and Behavioral Statistics},
keywords = {Bayesian inference,hierarchical models,probabilistic programming,statistical computing},
number = {5},
pages = {530--543},
publisher = {SAGE Publications Sage CA: Los Angeles, CA},
title = {{Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization}},
volume = {40},
year = {2015}
}
@article{Brehmer:2019gmn,
abstract = {Simplified template cross sections define a framework for the measurement and dissemination of kinematic information in Higgs measurements. We benchmark the currently proposed setup in an analysis of dimension-6 effective field theory operators for W H production. Calculating the Fisher information allows us to quantify the sensitivity of this framework to new physics and study its dependence on phase space. New machine- learning techniques let us compare the simplified template cross section framework to the full, high-dimensional kinematic information. We show that the way in which we truncate the effective theory has a sizable impact on the definition of the optimal simplified template cross sections.},
archivePrefix = {arXiv},
arxivId = {hep-ph/1908.06980},
author = {Brehmer, Johann and Dawson, Sally and Homiller, Samuel and Kling, Felix and Plehn, Tilman},
doi = {10.1007/JHEP11(2019)034},
eprint = {1908.06980},
issn = {10298479},
journal = {Journal of High Energy Physics},
keywords = {Beyond Standard Model,Effective Field Theories,Higgs Physics},
number = {11},
primaryClass = {hep-ph},
title = {{Benchmarking simplified template cross sections in W H production}},
volume = {2019},
year = {2019}
}
@article{Louppe:2017ipp,
abstract = {Recent progress in applying machine learning for jet physics has been built upon an analogy between calorimeters and images. In this work, we present a novel class of recursive neural networks built instead upon an analogy between QCD and natural languages. In the analogy, four-momenta are like words and the clustering history of sequential recombination jet algorithms is like the parsing of a sentence. Our approach works directly with the four-momenta of a variable-length set of particles, and the jet-based tree structure varies on an event-by-event basis. Our experiments highlight the flexibility of our method for building task-specific jet embeddings and show that recursive architectures are significantly more accurate and data efficient than previous image-based networks. We extend the analogy from individual jets (sentences) to full events (paragraphs), and show for the first time an event-level classifier operating on all the stable particles produced in an LHC event.},
archivePrefix = {arXiv},
arxivId = {1702.00748},
author = {Louppe, Gilles and Cho, Kyunghyun and Becot, Cyril and Cranmer, Kyle},
doi = {10.1007/JHEP01(2019)057},
eprint = {1702.00748},
issn = {10298479},
journal = {Journal of High Energy Physics},
keywords = {Jets,QCD Phenomenology},
number = {1},
pages = {57},
title = {{QCD-aware recursive neural networks for jet physics}},
volume = {2019},
year = {2019}
}
@article{gretton2012kernel,
author = {Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte J and Sch{\"{o}}lkopf, Bernhard and Smola, Alexander},
journal = {Journal of Machine Learning Research},
number = {Mar},
pages = {723--773},
title = {{A kernel two-sample test}},
volume = {13},
year = {2012}
}
@article{bingham2018pyro,
abstract = {Pyro is a probabilistic programming language built on Python as a platform for developing advanced probabilistic models in AI research. To scale to large data sets and high-dimensional models, Pyro uses stochastic variational inference algorithms and probability distributions built on top of PyTorch, a modern GPU-accelerated deep learning framework. To accommodate complex or model-specific algorithmic behavior, Pyro leverages Poutine, a library of composable building blocks for modifying the behavior of probabilistic programs.},
archivePrefix = {arXiv},
arxivId = {1810.09538},
author = {Bingham, Eli and Chen, Jonathan P. and Jankowiak, Martin and Obermeyer, F. and Pradhan, Neeraj and Karaletsos, Theofanis and Singh, Rohit and Szerlip, Paul and Horsfall, Paul and Goodman, Noah D.},
eprint = {1810.09538},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Approximate Bayesian inference,Deep learning,Generative models,Graphical models,Probabilistic programming},
title = {{Pyro: Deep universal probabilistic programming}},
volume = {20},
year = {2019}
}
@article{2016arXiv160502226U,
abstract = {We present Neural Autoregressive Distribution Estimation (NADE) models, which are neural network architectures applied to the problem of unsupervised distribution and density estimation. They leverage the probability product rule and a weight sharing scheme inspired from restricted Boltzmann machines, to yield an estimator that is both tractable and has good generalization performance. We discuss how they achieve competitive performance in modeling both binary and real-valued observations. We also present how deep NADE models can be trained to be agnostic to the ordering of input dimensions used by the autoregressive product rule decomposition. Finally, we also show how to exploit the topological structure of pixels in images using a deep convolutional architecture for NADE.},
archivePrefix = {arXiv},
arxivId = {1605.02226},
author = {Uria, Benigno and Cote, Marc Alexandre and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
eprint = {1605.02226},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Deep Learning,Density Modeling,Neural Networks,Unsupervised Learning},
month = {may},
pages = {1--37},
title = {{Neural autoregressive distribution estimation}},
volume = {17},
year = {2016}
}
@inproceedings{wood-aistats-2014,
abstract = {We introduce and demonstrate a new approach to inference in expressive probabilistic programming languages based on particle Markov chain Monte Carlo. Our approach is simple to implement and easy to parallelize. It applies to Turing-complete probabilistic programming languages and supports accurate inference in models that make use of complex control flow, including stochastic recursion. It also includes primitives from Bayesian nonparametric statistics. Our experiments show that this approach can be more efficient than previously introduced single-site Metropolis-Hastings methods.},
archivePrefix = {arXiv},
arxivId = {1507.00996},
author = {Wood, Frank and {Van De Meent}, Jan Willem and Mansinghka, Vikash},
booktitle = {Journal of Machine Learning Research},
eprint = {1507.00996},
issn = {15337928},
pages = {1024--1032},
title = {{A new approach to probabilistic programming inference}},
volume = {33},
year = {2014}
}
@article{baydin2018automatic,
abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply “autodiff”, is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names “dynamic computational graphs” and “differentiable programming”. We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms “autodiff”, “automatic differentiation”, and “symbolic differentiation” as these are encountered more and more in machine learning settings.},
archivePrefix = {arXiv},
arxivId = {1502.05767},
author = {{G{\"{u}}neş Baydin}, Atılım and Pearlmutter, Barak A. and {Andreyevich Radul}, Alexey and {Mark Siskind}, Jeffrey},
eprint = {1502.05767},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Backpropagation,Differentiable Programming},
number = {153},
pages = {1--43},
title = {{Automatic differentiation in machine learning: A survey}},
volume = {18},
year = {2018}
}
@article{2015arXiv150103291G,
abstract = {Our paper deals with inferring simulator-based statistical models given some observed data. A simulator-based model is a parametrized mechanism which specifies how data are generated. It is thus also referred to as generative model. We assume that only a finite number of parameters are of interest and allow the generative process to be very general; it may be a noisy nonlinear dynamical system with an unrestricted number of hidden variables. This weak assumption is useful for devising realistic models but it renders statistical inference very difficult. The main challenge is the intractability of the likelihood function. Several likelihood-free inference methods have been proposed which share the basic idea of identifying the parameters by finding values for which the discrepancy between simulated and observed data is small. A major obstacle to using these methods is their computational cost. The cost is largely due to the need to repeatedly simulate data sets and the lack of knowledge about how the parameters affect the discrepancy. We propose a strategy which combines probabilistic modeling of the discrepancy with optimization to facilitate likelihood-free inference. The strategy is implemented using Bayesian optimization and is shown to accelerate the inference through a reduction in the number of required simulations by several orders of magnitude.},
archivePrefix = {arXiv},
arxivId = {1501.03291},
author = {Gutmann, Michael U. and Corander, Jukka},
eprint = {1501.03291},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Approximate Bayesian computation,Bayesian inference,Computational efficiency,Intractable likelihood,Latent variables},
month = {jan},
number = {1},
pages = {1--47},
publisher = {JMLR.org},
title = {{Bayesian optimization for likelihood-free inference of simulator-based statistical models}},
url = {http://dl.acm.org/citation.cfm?id=2946645.3007078},
volume = {17},
year = {2016}
}
@article{2018arXiv180910756V,
abstract = {Probabilistic topic models are a suite of algorithms whose aim is to discover the hidden thematic structure in large archives of documents. In this article, we review the main ideas of this field, survey the current state-of-the-art, and describe some promising future directions. We first describe latent Dirichlet allocation (LDA) [8], which is the simplest kind of topic model. We discuss its connections to probabilistic modeling, and describe two kinds of algorithms for topic discovery. We then survey the growing body of research that extends and applies topic models in interesting ways. These extensions have been developed by relaxing some of the statistical assumptions of LDA, incorporating meta-data into the analysis of the documents, and using similar kinds of models on a diversity of data types such as social networks, images and genetics. Finally, we give our thoughts as to some of the important unexplored directions for topic modeling. These include rigorous methods for checking models built for data exploration, new approaches to visualizing text and other high dimensional data, and moving beyond traditional information engineering applications towards using topic models for more scientific ends.},
archivePrefix = {arXiv},
arxivId = {stat.ML/1809.10756},
author = {Gaver, Donald P. and Jacobs, Patricia A. and Bremaud, Pierre},
doi = {10.2307/2289687},
eprint = {1809.10756},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Programming Languages,Statistics - Machine Learning},
month = {sep},
number = {407},
pages = {843},
primaryClass = {stat.ML},
title = {{An Introduction to Probabilistic Modeling.}},
volume = {84},
year = {1989}
}
@inproceedings{Diggle1984MonteCM,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. SUMMARY A prescribed statistical model is a parametric specification of the distribution of a random vector, whilst an implicit statistical model is one defined at a more funda-mental level in terms of a generating stochastic mechanism. This paper develops methods of inference which can be used for implicit statistical models whose distribution theory is intractable. The kernel method of probability density estimation is advocated for estimating a log-likelihood from simulations of such a model. The development and testing of an algorithm for maximizing this estimated log-likelihood function is described. An illustrative example involving a stochastic model for quantal response assays is given. Possible applications of the maximization algorithm to ad hoc methods of parameter estimation are noted briefly, and illustrated},
author = {Diggle, Peter J. and Gratton, Richard J.},
booktitle = {Journal of the Royal Statistical Society: Series B (Methodological)},
doi = {10.1111/j.2517-6161.1984.tb01290.x},
issn = {0035-9246},
number = {2},
pages = {193--212},
title = {{Monte Carlo Methods of Inference for Implicit Statistical Models}},
volume = {46},
year = {1984}
}
@article{anderson1987mean,
abstract = {We introduce a learning algorithm for unsupervised neural networks based on ideas from statistical mechanics. The algorithm is derived from a mean field approximation for large, layered sigmoid belief networks. We show how to (approximately) infer the statistics of these networks without resort to sampling. This is done by solving the mean field equations, which relate the statistics of each unit to those of its Markov blanket. Using these statistics as target values, the weights in the network are adapted by a local delta rule. We evaluate the strengths and weaknesses of these networks for problems in statistical pattern recognition.},
author = {Saul, Lawrence and Jordan, Michael},
doi = {10.1007/978-94-011-5014-9_20},
journal = {Learning in Graphical Models},
pages = {541--554},
title = {{A Mean Field Learning Algorithm for Unsupervised Neural Networks}},
volume = {1},
year = {1998}
}
@article{Alsing:2018eau,
abstract = {Many statistical models in cosmology can be simulated forwards but have intractable likelihood functions. Likelihood-free inference methods allow us to perform Bayesian inference from these models using only forward simulations, free from any likelihood assumptions or approximations. Likelihood-free inference generically involves simulating mock data and comparing to the observed data; this comparison in data space suffers from the curse of dimensionality and requires compression of the data to a small number of summary statistics to be tractable. In this paper, we use massive asymptotically optimal data compression to reduce the dimensionality of the data space to just one number per parameter, providing a natural and optimal framework for summary statistic choice for likelihood-free inference. Secondly, we present the first cosmological application of Density Estimation Likelihood-Free Inference (DELFI), which learns a parametrized model for joint distribution of data and parameters, yielding both the parameter posterior and the model evidence. This approach is conceptually simple, requires less tuning than traditional Approximate Bayesian Computation approaches to likelihood-free inference and can give high-fidelity posteriors from orders of magnitude fewer forward simulations. As an additional bonus, it enables parameter inference and Bayesian model comparison simultaneously. We demonstrate DELFI with massive data compression on an analysis of the joint light-curve analysis supernova data, as a simple validation case study. We show that high-fidelity posterior inference is possible for full-scale cosmological data analyses with as few as {\~{}}104 simulations, with substantial scope for further improvement, demonstrating the scalability of likelihood-free inference to large and complex cosmological data sets.},
archivePrefix = {arXiv},
arxivId = {1801.01497},
author = {Alsing, Justin and Wandelt, Benjamin and Feeney, Stephen},
doi = {10.1093/mnras/sty819},
eprint = {1801.01497},
issn = {13652966},
journal = {Monthly Notices of the Royal Astronomical Society},
keywords = {Methods: Data analysis},
number = {3},
pages = {2874--2885},
title = {{Massive optimal data compression and density estimation for scalable, likelihood-free inference in cosmology}},
volume = {477},
year = {2018}
}
@article{Alsing:2019xrx,
abstract = {Likelihood-free inference provides a framework for performing rigorous Bayesian inference using only forward simulations, properly accounting for all physical and observational effects that can be successfully included in the simulations. The key challenge for likelihood-free applications in cosmology, where simulation is typically expensive, is developing methods that can achieve high-fidelity posterior inference with as few simulations as possible. Density-estimation likelihood-free inference (DELFI) methods turn inference into a density estimation task on a set of simulated data-parameter pairs, and give orders of magnitude improvements over traditional Approximate Bayesian Computation approaches to likelihood-free inference. In this paper we use neural density estimators (NDEs) to learn the likelihood function from a set of simulated datasets, with active learning to adaptively acquire simulations in the most relevant regions of parameter space on-the-fly. We demonstrate the approach on a number of cosmological case studies, showing that for typical problems high-fidelity posterior inference can be achieved with just {\$}\backslashmathcal {\{}O{\}}(10{\^{}}3){\$} simulations or fewer. In addition to enabling efficient simulation-based inference, for simple problems where the form of the likelihood is known, DELFI offers a fast alternative to MCMC sampling, giving orders of magnitude speed-up in some cases. Finally, we introduce pydelfi – a flexible public implementation of DELFI with NDEs and active learning – available at https://github.com/justinalsing/pydelfi.},
archivePrefix = {arXiv},
arxivId = {1903.00007},
author = {Alsing, Justin and Charnock, Tom and Feeney, Stephen and Wandelt, Benjamin},
doi = {10.1093/mnras/stz1960},
eprint = {1903.00007},
issn = {0035-8711},
journal = {Monthly Notices of the Royal Astronomical Society},
keywords = {Astrophysics - Cosmology and Nongalactic Astrophy,data analysis: methods},
month = {sep},
number = {3},
pages = {4440--4458},
title = {{Fast likelihood-free cosmology with neural density estimators and active learning}},
volume = {488},
year = {2019}
}
@article{Alsing:2019dvb,
abstract = {We show how nuisance parameter marginalized posteriors can be inferred directly from simulations in a likelihood-free setting, without having to jointly infer the higher dimensional interesting and nuisance parameter posterior first and marginalize a posteriori. The result is that for an inference task with a given number of interesting parameters, the number of simulations required to perform likelihood-free inference can be kept (roughly) the same irrespective of the number of additional nuisances to be marginalized over. To achieve this, we introduce two extensions to the standard likelihood-free inference set-up. First, we show how nuisance parameters can be recast as latent variables and hence automatically marginalized over in the likelihood-free framework. Secondly, we derive an asymptotically optimal compression from N data to n summaries – one per interesting parameter - such that the Fisher information is (asymptotically) preserved, but the summaries are insensitive to the nuisance parameters. This means that the nuisance marginalized inference task involves learning n interesting parameters from n ‘nuisance hardened' data summaries, regardless of the presence or number of additional nuisance parameters to be marginalized over. We validate our approach on two examples from cosmology: supernovae and weak-lensing data analyses with nuisance parametrized systematics. For the supernova problem, high-fidelity posterior inference of $\Omega$m and w0 (marginalized over systematics) can be obtained from just a few hundred data simulations. For the weak-lensing problem, six cosmological parameters can be inferred from just {\$}\backslashmathcal {\{}O{\}}(10{\^{}}3){\$} simulations, irrespective of whether 10 additional nuisance parameters are included in the problem or not.},
archivePrefix = {arXiv},
arxivId = {1903.01473},
author = {Alsing, Justin and Wandelt, Benjamin},
doi = {10.1093/mnras/stz1900},
eprint = {1903.01473},
issn = {0035-8711},
journal = {Monthly Notices of the Royal Astronomical Society},
number = {4},
pages = {5093--5103},
title = {{Nuisance hardened data compression for fast likelihood-free inference}},
volume = {488},
year = {2019}
}
@article{Alsing:2017var,
abstract = {Data compression has become one of the cornerstones of modern astronomical data analysis, with the vast majority of analyses compressing large raw datasets down to a manageable number of informative summaries. In this paper we provide a general procedure for optimally compressing {\$}N{\$} data down to {\$}n{\$} summary statistics, where {\$}n{\$} is equal to the number of parameters of interest. We show that compression to the score function -- the gradient of the log-likelihood with respect to the parameters -- yields {\$}n{\$} compressed statistics that are optimal in the sense that they preserve the Fisher information content of the data. Our method generalizes earlier work on linear Karhunen-Lo$\backslash$'{\{}e{\}}ve compression for Gaussian data whilst recovering both lossless linear compression and quadratic estimation as special cases when they are optimal. We give a unified treatment that also includes the general non-Gaussian case as long as mild regularity conditions are satisfied, producing optimal non-linear summary statistics when appropriate. As a worked example, we derive explicitly the {\$}n{\$} optimal compressed statistics for Gaussian data in the general case where both the mean and covariance depend on the parameters.},
archivePrefix = {arXiv},
arxivId = {1712.00012},
author = {Alsing, Justin and Wandelt, Benjamin},
doi = {10.1093/mnrasl/sly029},
eprint = {1712.00012},
issn = {1745-3925},
journal = {Monthly Notices of the Royal Astronomical Society: Letters},
number = {1},
pages = {L60--L64},
title = {{Generalized massive optimal data compression}},
volume = {476},
year = {2018}
}
@article{2015Natur.521..436L,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
issn = {14764687},
journal = {Nature},
month = {may},
number = {7553},
pages = {436--444},
pmid = {26017442},
title = {{Deep learning}},
volume = {521},
year = {2015}
}
@article{2017arXiv170309930W,
abstract = {We consider Bayesian inference problems with computationally intensive likelihood functions. We propose a Gaussian process (GP)-based method to approximate the joint distribution of the unknown parameters and the data, built on recent work (Kandasamy, Schneider, {\&} P{\'{o}}czos, 2015). In particular, we write the joint density approximately as a product of an approximate posterior density and an exponentiated GP surrogate. We then provide an adaptive algorithm to construct such an approximation, where an active learning method is used to choose the design points. With numerical examples, we illustrate that the proposed method has competitive performance against existing approaches for Bayesian computation.},
archivePrefix = {arXiv},
arxivId = {1703.09930},
author = {Wang, Hongqiao and Li, Jinglai},
doi = {10.1162/neco_a_01127},
eprint = {1703.09930},
issn = {1530888X},
journal = {Neural Computation},
keywords = {Statistics - Computation,Statistics - Machine Learning},
month = {mar},
number = {11},
pages = {3072--3094},
title = {{Adaptive Gaussian process approximation for Bayesian inference with expensive likelihood functions}},
volume = {30},
year = {2018}
}
@article{sugiyama2011least,
author = {Sugiyama, Masashi and Suzuki, Taiji and Itoh, Yuta and Kanamori, Takafumi and Kimura, Manabu},
journal = {Neural Networks},
number = {7},
pages = {735--751},
publisher = {Elsevier},
title = {{Least-squares two-sample test}},
volume = {24},
year = {2011}
}
@misc{olah_2015,
author = {Olah, Christopher},
booktitle = {Neural Networks, Types, and Functional Programming},
month = {sep},
title = {{Neural Networks, Types, and Functional Programming}},
url = {https://colah.github.io/posts/2015-09-NN-Types-FP/},
year = {2015}
}
@misc{lecun_2018,
author = {LeCun, Yann},
booktitle = {Official Facebook Post},
month = {jan},
title = {{Deep Learning est mort. Vive Differentiable Programming!}},
howpublished = {\url{https://www.facebook.com/yann.lecun/posts/10155003011462143}},
url = {https://www.facebook.com/yann.lecun/posts/10155003011462143 and https://techburst.io/deep-learning-est-mort-vive-differentiable-programming-5060d3c55074},
year = {2018}
}
@article{Brehmer:2018eca,
abstract = {We develop, discuss, and compare several inference techniques to constrain theory parameters in collider experiments. By harnessing the latent-space structure of particle physics processes, we extract extra information from the simulator. This augmented data can be used to train neural networks that precisely estimate the likelihood ratio. The new methods scale well to many observables and high-dimensional parameter spaces, do not require any approximations of the parton shower and detector response, and can be evaluated in microseconds. Using weak-boson-fusion Higgs production as an example process, we compare the performance of several techniques. The best results are found for likelihood ratio estimators trained with extra information about the score, the gradient of the log likelihood function with respect to the theory parameters. The score also provides sufficient statistics that contain all the information needed for inference in the neighborhood of the Standard Model. These methods enable us to put significantly stronger bounds on effective dimension-six operators than the traditional approach based on histograms. They also outperform generic machine learning methods that do not make use of the particle physics structure, demonstrating their potential to substantially improve the new physics reach of the Large Hadron Collider legacy results.},
archivePrefix = {arXiv},
arxivId = {1805.00020},
author = {Brehmer, Johann and Cranmer, Kyle and Louppe, Gilles and Pavez, Juan},
doi = {10.1103/PhysRevD.98.052004},
eprint = {1805.00020},
issn = {24700029},
journal = {Physical Review D},
number = {5},
pages = {052004},
title = {{A guide to constraining effective field theories with machine learning}},
volume = {98},
year = {2018}
}
@article{Brehmer:2018kdj,
abstract = {We present powerful new analysis techniques to constrain effective field theories at the LHC. By leveraging the structure of particle physics processes, we extract extra information from Monte Carlo simulations, which can be used to train neural network models that estimate the likelihood ratio. These methods scale well to processes with many observables and theory parameters, do not require any approximations of the parton shower or detector response, and can be evaluated in microseconds. We show that they allow us to put significantly stronger bounds on dimension-six operators than existing methods, demonstrating their potential to improve the precision of the LHC legacy constraints.},
archivePrefix = {arXiv},
arxivId = {1805.00013},
author = {Brehmer, Johann and Cranmer, Kyle and Louppe, Gilles and Pavez, Juan},
doi = {10.1103/PhysRevLett.121.111801},
eprint = {1805.00013},
issn = {10797114},
journal = {Physical Review Letters},
number = {11},
pages = {111801},
title = {{Constraining Effective Field Theories with Machine Learning}},
volume = {121},
year = {2018}
}
@inproceedings{Neal:2007zz,
abstract = {When searching for new phenomena in high-energy physics, statistical analysis is complicated by the presence of nuisance parameters, representing uncertainty in the physics of interactions or in detector properties. Another complication, even with no nuisance parameters, is that the probability distributions of the models are specified only by simulation programs, with no way of evaluating their probability density functions. I advocate expressing the result of an experiment by means of the likelihood function, rather than by frequentist confidence intervals or p-values. A likelihood function for this problem is difficult to obtain, however, for both of the reasons given above. I discuss ways of circumventing these problems by reducing dimensionality using a classifier and employing simulations with multiple values for the nuisance parameters.},
author = {Neal, Radford M.},
booktitle = {PHYSTAT LHC Workshop on Statistical Issues for LHC Physics, PHYSTAT 2007 - Proceedings},
pages = {111--118},
title = {{Computing likelihood functions for high-energy physics experiments when distributions are defined by simulators with nuisance parameters}},
url = {http://cds.cern.ch/record/1099977/files/p111.pdf},
year = {2008}
}
@article{10.1371/journal.pbio.0040029,
abstract = {Despite decades of evidence for functional plasticity in the adult brain, the role of structural plasticity in its manifestation remains unclear. To examine the extent of neuronal remodeling that occurs in the brain on a day-to-day basis, we used a multiphoton-based microscopy system for chronic in vivo imaging and reconstruction of entire neurons in the superficial layers of the rodent cerebral cortex. Here we show the first unambiguous evidence (to our knowledge) of dendrite growth and remodeling in adult neurons. Over a period of months, neurons could be seen extending and retracting existing branches, and in rare cases adding new branch tips. Neurons exhibiting dynamic arbor rearrangements were GABA-positive non-pyramidal interneurons, while pyramidal cells remained stable. These results are consistent with the idea that dendritic structural remodeling is a substrate for adult plasticity and they suggest that circuit rearrangement in the adult cortex is restricted by cell type-specific rules. Copyright: {\textcopyright} 2006 Lee et al.},
author = {Lee, Wei Chung Allen and Huang, Hayden and Feng, Guoping and Sanes, Joshua R. and Brown, Emery N. and So, Peter T. and Nedivi, Elly},
doi = {10.1371/journal.pbio.0040029},
issn = {15457885},
journal = {PLoS Biology},
number = {2},
pages = {271--280},
publisher = {Public Library of Science},
title = {{Dynamic remodeling of dendritic arbors in GABAergic interneurons of adult visual cortex}},
url = {https://doi.org/10.1371/journal.pbio.0040029},
volume = {4},
year = {2006}
}
@inproceedings{2018arXiv180509294L,
abstract = {Approximate Bayesian Computation (ABC) provides methods for Bayesian inference in simulation-based stochastic models which do not permit tractable likelihoods. We present a new ABC method which uses probabilistic neural emulator networks to learn synthetic likelihoods on simulated data -- both local emulators which approximate the likelihood for specific observed data, as well as global ones which are applicable to a range of data. Simulations are chosen adaptively using an acquisition function which takes into account uncertainty about either the posterior distribution of interest, or the parameters of the emulator. Our approach does not rely on user-defined rejection thresholds or distance functions. We illustrate inference with emulator networks on synthetic examples and on a biophysical neuron model, and show that emulators allow accurate and efficient inference even on high-dimensional problems which are challenging for conventional ABC approaches.},
archivePrefix = {arXiv},
arxivId = {1805.09294},
author = {Lueckmann, Jan-Matthis and Bassetto, Giacomo and Karaletsos, Theofanis and Macke, Jakob H.},
booktitle = {Proceedings of The 1st Symposium on Advances in Approximate Bayesian Inference},
editor = {Ruiz, Francisco and Zhang, Cheng and Liang, Dawen and Bui, Thang},
eprint = {1805.09294},
pages = {32--53},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Likelihood-free inference with emulator networks}},
url = {http://arxiv.org/abs/1805.09294},
volume = {96},
year = {2018}
}
@inproceedings{le2017inference,
abstract = {We introduce a method for using deep neural networks to amortize the cost of inference in models from the family induced by universal probabilistic programming languages, establishing a framework that combines the strengths of probabilistic programming and deep learning methods. We call what we do “compilation of inference” because our method transforms a denotational specification of an inference problem in the form of a probabilistic program written in a universal programming language into a trained neural network denoted in a neural network specification language. When at test time this neural network is fed observational data and executed, it performs approximate inference in the original model specified by the probabilistic program. Our training objective and learning procedure are designed to allow the trained neural network to be used as a proposal distribution in a sequential importance sampling inference engine. We illustrate our method on mixture models and Captcha solving and show significant speedups in the efficiency of inference.},
address = {Fort Lauderdale, FL, USA},
archivePrefix = {arXiv},
arxivId = {1610.09900},
author = {Le, Tuan Anh and Baydin, Atılım G{\"{u}}neş and Wood, Frank},
booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017},
eprint = {1610.09900},
pages = {1338--1348},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Inference compilation and universal probabilistic programming}},
volume = {54},
year = {2017}
}
@article{Brehmer:2018hga,
abstract = {Simulators often provide the best description of real-world phenomena. However, the probability density that they implicitly define is often intractable, leading to challenging inverse problems for inference. Recently, a number of techniques have been introduced in which a surrogate for the intractable density is learned, including normalizing flows and density ratio estimators. We show that additional information that characterizes the latent process can often be extracted from simulators and used to augment the training data for these surrogate models. We introduce several loss functions that leverage these augmented data and demonstrate that these techniques can improve sample efficiency and quality of inference.},
archivePrefix = {arXiv},
arxivId = {1805.12244},
author = {Brehmer, Johann and Louppe, Gilles and Pavez, Juan and Cranmer, Kyle},
doi = {10.1073/pnas.1915980117},
eprint = {1805.12244},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Implicit models,Neural density estimation,Simulation-based inference},
month = {mar},
number = {10},
pages = {5242--5249},
pmid = {32079725},
publisher = {National Academy of Sciences},
title = {{Mining gold from implicit models to improve likelihood-free inference}},
url = {http://arxiv.org/abs/1805.12244{\%}0Ahttp://dx.doi.org/10.1073/pnas.1915980117 http://www.ncbi.nlm.nih.gov/pubmed/32079725},
volume = {117},
year = {2020}
}
@article{sisson2007sequential,
abstract = {Recent new methods in Bayesian simulation have provided ways of evaluating posterior distributions in the presence of analytically or computationally intractable likelihood functions. Despite representing a substantial methodological advance, existing methods based on rejection sampling or Markov chain Monte Carlo can be highly inefficient and accordingly require far more iterations than may be practical to implement. Here we propose a sequential Monte Carlo sampler that convincingly overcomes these inefficiencies. We demonstrate its implementation through an epidemiological study of the transmission rate of tuberculosis. {\textcopyright} 2007 by The National Academy of Sciences of the USA.},
author = {Sisson, S. A. and Fan, Y. and Tanaka, Mark M.},
doi = {10.1073/pnas.0607208104},
issn = {00278424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Approximate Bayesian computation,Bayesian inference,Importance sampling,Intractable likelihoods,Tuberculosis},
number = {6},
pages = {1760--1765},
pmid = {17264216},
publisher = {National Acad Sciences},
title = {{Sequential Monte Carlo without likelihoods}},
volume = {104},
year = {2007}
}
@article{marjoram2003markov,
abstract = {Many stochastic simulation approaches for generating observations from a posterior distribution depend on knowing a likelihood function. However, for many complex probability models, such likelihoods are either impossible or computationally prohibitive to obtain. Here we present a Markov chain Monte Carlo method for generating observations from a posterior distribution without the use of likelihoods. It can also be used in frequentist applications, in particular for maximum-likelihood estimation. The approach is illustrated by an example of ancestral inference in population genetics. A number of open problems are highlighted in the discussion.},
author = {Marjoram, Paul and Molitor, John and Plagnol, Vincent and Tavar{\'{e}}, Simon},
doi = {10.1073/pnas.0306899100},
issn = {00278424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {26},
pages = {15324--15328},
pmid = {14663152},
publisher = {National Acad Sciences},
title = {{Markov chain Monte Carlo without likelihoods}},
volume = {100},
year = {2003}
}
@article{Carleo:2019ptp,
abstract = {Machine learning (ML) encompasses a broad range of algorithms and modeling tools used for a vast array of data processing tasks, which has entered most scientific disciplines in recent years. This article reviews in a selective way the recent research on the interface between machine learning and the physical sciences. This includes conceptual developments in ML motivated by physical insights, applications of machine learning techniques to several domains in physics, and cross fertilization between the two fields. After giving a basic notion of machine learning methods and principles, examples are described of how statistical physics is used to understand methods in ML. This review then describes applications of ML methods in particle physics and cosmology, quantum many-body physics, quantum computing, and chemical and material physics. Research and development into novel computing architectures aimed at accelerating ML are also highlighted. Each of the sections describe recent successes as well as domain-specific methodology and challenges.},
archivePrefix = {arXiv},
arxivId = {1903.10563},
author = {Carleo, Giuseppe and Cirac, Ignacio and Cranmer, Kyle and Daudet, Laurent and Schuld, Maria and Tishby, Naftali and Vogt-Maranto, Leslie and Zdeborov{\'{a}}, Lenka},
doi = {10.1103/RevModPhys.91.045002},
eprint = {1903.10563},
issn = {15390756},
journal = {Reviews of Modern Physics},
number = {4},
title = {{Machine learning and the physical sciences}},
volume = {91},
year = {2019}
}
@article{2012arXiv1212.1479F,
abstract = {Approximate Bayesian computation (ABC) methods, which are applicable when the likelihood is difficult or impossible to calculate, are an active topic of current research. Most current ABC algorithms directly approximate the posterior distribution, but an alternative, less common strategy is to approximate the likelihood function. This has several advantages. First, in some problems, it is easier to approximate the likelihood than to approximate the posterior. Second, an approximation to the likelihood allows reference analyses to be constructed based solely on the likelihood. Third, it is straightforward to perform sensitivity analyses for several different choices of prior once an approximation to the likelihood is constructed, which needs to be done only once. The contribution of the present paper is to consider regression density estimation techniques to approximate the likelihood in the ABC setting. Our likelihood approximations build on recently developed marginal adaptation density estimators by extending them for conditional density estimation. Our approach facilitates reference Bayesian inference, as well as frequentist inference. The method is demonstrated via a challenging problem of inference for stereological extremes, where we perform both frequentist and Bayesian inference.},
archivePrefix = {arXiv},
arxivId = {stat.CO/1212.1479},
author = {Fan, Yanan and Nott, David J. and Sisson, Scott A.},
doi = {10.1002/sta4.15},
eprint = {1212.1479},
issn = {20491573},
journal = {Stat},
keywords = {Approximate Bayesian computation,Copulas,Likelihood-free inference,Multivariate density estimation,Regression density estimation},
month = {dec},
number = {1},
pages = {34--48},
primaryClass = {stat.CO},
title = {{Approximate Bayesian computation via regression density estimation}},
volume = {2},
year = {2013}
}
@article{gutmann2017likelihood,
abstract = {Increasingly complex generative models are being used across disciplines as they allow for realistic characterization of data, but a common difficulty with them is the prohibitively large computational cost to evaluate the likelihood function and thus to perform likelihood-based statistical inference. A likelihood-free inference framework has emerged where the parameters are identified by finding values that yield simulated data resembling the observed data. While widely applicable, a major difficulty in this framework is how to measure the discrepancy between the simulated and observed data. Transforming the original problem into a problem of classifying the data into simulated versus observed, we find that classification accuracy can be used to assess the discrepancy. The complete arsenal of classification methods becomes thereby available for inference of intractable generative models. We validate our approach using theory and simulations for both point estimation and Bayesian inference, and demonstrate its use on real data by inferring an individual-based epidemiological model for bacterial infections in child care centers.},
archivePrefix = {arXiv},
arxivId = {1407.4981},
author = {Gutmann, Michael U. and Dutta, Ritabrata and Kaski, Samuel and Corander, Jukka},
doi = {10.1007/s11222-017-9738-6},
eprint = {1407.4981},
issn = {15731375},
journal = {Statistics and Computing},
keywords = {Approximate Bayesian computation,Generative models,Intractable likelihood,Latent variable models,Simulator-based models},
number = {2},
pages = {411--425},
publisher = {Springer},
title = {{Likelihood-free inference via classification}},
volume = {28},
year = {2018}
}
@article{2019arXiv190604347R,
abstract = {Likelihood-free methods such as approximate Bayesian computation (ABC) have extended the reach of statistical inference to problems with computationally intractable likelihoods. Such approaches perform well for small-to-moderate dimensional problems, but suffer a curse of dimensionality in the number of model parameters. We introduce a likelihood-free approximate Gibbs sampler that naturally circumvents the dimensionality issue by focusing on lower-dimensional conditional distributions. These distributions are estimated by flexible regression models either before the sampler is run, or adaptively during sampler implementation. As a result, and in comparison to Metropolis-Hastings-based approaches, we are able to fit substantially more challenging statistical models than would otherwise be possible. We demonstrate the sampler's performance via two simulated examples, and a real analysis of Airbnb rental prices using a intractable high-dimensional multivariate nonlinear state-space model with a 36-dimensional latent state observed on 365 time points, which presents a real challenge to standard ABC techniques.},
archivePrefix = {arXiv},
arxivId = {1906.04347},
author = {Rodrigues, G. S. and Nott, David J. and Sisson, S. A.},
doi = {10.1007/s11222-020-09933-x},
eprint = {1906.04347},
issn = {15731375},
journal = {Statistics and Computing},
keywords = {Approximate Bayesian computation,Gibbs sampler,State- space models},
month = {jun},
pages = {arXiv:1906.04347},
title = {{Likelihood-free approximate Gibbs sampling}},
year = {2020}
}
@article{peters2012sequential,
abstract = {We present a variant of the sequential Monte Carlo sampler by incorporating the partial rejection control mechanism of Liu (2001). We show that the resulting algorithm can be considered as a sequential Monte Carlo sampler with a modified mutation kernel. We prove that the new sampler can reduce the variance of the incremental importance weights when compared with standard sequential Monte Carlo samplers, and provide a central limit theorem. Finally, the sampler is adapted for application under the challenging approximate Bayesian computation modelling framework. {\textcopyright} 2012 Springer Science+Business Media, LLC.},
archivePrefix = {arXiv},
arxivId = {0808.3466},
author = {Peters, G. W. and Fan, Y. and Sisson, S. A.},
doi = {10.1007/s11222-012-9315-y},
eprint = {0808.3466},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {Approximate Bayesian computation,Bayesian computation,Likelihood-free inference,Partial rejection control,Sequential Monte Carlo samplers},
number = {6},
pages = {1209--1222},
publisher = {Springer},
title = {{On sequential Monte Carlo, partial rejection control and approximate Bayesian computation}},
volume = {22},
year = {2012}
}
@article{Bect2012,
abstract = {This paper deals with the problem of estimating the volume of the excursion set of a function f:ℝd→ℝ above a given threshold, under a probability measure on ℝd that is assumed to be known. In the industrial world, this corresponds to the problem of estimating a probability of failure of a system. When only an expensive-to-simulate model of the system is available, the budget for simulations is usually severely limited and therefore classical Monte Carlo methods ought to be avoided. One of the main contributions of this article is to derive SUR (stepwise uncertainty reduction) strategies from a Bayesian formulation of the problem of estimating a probability of failure. These sequential strategies use a Gaussian process model of f and aim at performing evaluations of f as efficiently as possible to infer the value of the probability of failure. We compare these strategies to other strategies also based on a Gaussian process model for estimating a probability of failure. {\textcopyright} 2011 Springer Science+Business Media, LLC.},
author = {Bect, Julien and Ginsbourger, David and Li, Ling and Picheny, Victor and Vazquez, Emmanuel},
doi = {10.1007/s11222-011-9241-4},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {Computer experiments,Gaussian processes,Probability of failure,Sequential design,Stepwise uncertainty reduction},
month = {may},
number = {3},
pages = {773--793},
title = {{Sequential design of computer experiments for the estimation of a probability of failure}},
url = {https://doi.org/10.1007/s11222-011-9241-4},
volume = {22},
year = {2012}
}
@article{ranjan2008sequential,
abstract = {Computer simulation often is used to study complex physical and engineering processes. Although a computer simulator often can be viewed as an inexpensive way to gain insight into a system, it still can be computationally costly. Much of the recent work on the design and analysis of computer experiments has focused on scenarios where the goal is to fit a response surface or process optimization. In this article we develop a sequential methodology for estimating a contour from a complex computer code. The approach uses a stochastic process model as a surrogate for the computer simulator. The surrogate model and associated uncertainty are key components in a new criterion used to identify the computer trials aimed specifically at improving the contour estimate. The proposed approach is applied to exploration of a contour for a network queuing system. Issues related to practical implementation of the proposed approach also are addressed. {\textcopyright} 2008 American Statistical Association and the American Society for Quality.},
author = {Ranjan, Pritam and Bingham, Derek and Michailidis, George},
doi = {10.1198/004017008000000541},
issn = {00401706},
journal = {Technometrics},
keywords = {Computer experiment,Gaussian process,Inverse problem},
number = {4},
pages = {527--541},
publisher = {Taylor {\&} Francis},
title = {{Sequential experiment design for contour estimation from complex computer codes}},
volume = {50},
year = {2008}
}
@article{Wilks:1938dza,
abstract = {: Ann. Math. Statist. Volume 9, Number 1 (1938), 60-62. S. S. Wilks},
author = {Wilks, S. S.},
doi = {10.1214/aoms/1177732360},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
number = {1},
pages = {60--62},
title = {{The Large-Sample Distribution of the Likelihood Ratio for Testing Composite Hypotheses}},
volume = {9},
year = {1938}
}
@article{rubin1984,
abstract = {A common reaction among applied statisticians is that the Bayesian statistician's energies in an applied problem must be directed at the a priori elicitation of one model specification from which an optimal design and all inferences follow automatically by applying Bayes's theorem to calculate conditional distributions of unknowns given knowns. I feel, however, that the applied Bayesian statistician's tool-kit should be more extensive and include tools that may be usefully labeled frequency calculations. Three types of Bayesianly justifiable and relevant frequency calculations are presented using examples to convey their use for the applied statistician.},
author = {Rubin, Donald B.},
doi = {10.1214/aos/1176346785},
issn = {0090-5364},
journal = {The Annals of Statistics},
number = {4},
pages = {1151--1172},
publisher = {The Institute of Mathematical Statistics},
title = {{Bayesianly Justifiable and Relevant Frequency Calculations for the Applied Statistician}},
url = {https://doi.org/10.1214/aos/1176346785},
volume = {12},
year = {1984}
}
@article{Brehmer:2019jyt,
abstract = {The subtle and unique imprint of dark matter substructure on extended arcs in strong lensing systems contains a wealth of information about the properties and distribution of dark matter on small scales and, consequently, about the underlying particle physics. However, teasing out this effect poses a significant challenge since the likelihood function for realistic simulations of population-level parameters is intractable. We apply recently-developed simulation-based inference techniques to the problem of substructure inference in galaxy-galaxy strong lenses. By leveraging additional information extracted from the simulator, neural networks are efficiently trained to estimate likelihood ratios associated with population-level parameters characterizing substructure. Through proof-of-principle application to simulated data, we show that these methods can provide an efficient and principled way to simultaneously analyze an ensemble of strong lenses, and can be used to mine the large sample of lensing images deliverable by near-future surveys for signatures of dark matter substructure.},
archivePrefix = {arXiv},
arxivId = {1909.02005},
author = {Brehmer, Johann and Mishra-Sharma, Siddharth and Hermans, Joeri and Louppe, Gilles and Cranmer, Kyle},
doi = {10.3847/1538-4357/ab4c41},
eprint = {1909.02005},
issn = {1538-4357},
journal = {The Astrophysical Journal},
number = {1},
pages = {49},
title = {{Mining for Dark Matter Substructure: Inferring Subhalo Population Properties from Strong Lenses with Machine Learning}},
volume = {886},
year = {2019}
}
@article{metropolis1953equation,
abstract = {A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two-dimensional rigid-sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four-term virial coefficient expansion.},
author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
doi = {10.1063/1.1699114},
issn = {00219606},
journal = {The Journal of Chemical Physics},
number = {6},
pages = {1087--1092},
publisher = {AIP},
title = {{Equation of state calculations by fast computing machines}},
volume = {21},
year = {1953}
}
@article{Wald,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.},
author = {Wald, Abraham},
doi = {10.2307/1990256},
issn = {00029947},
journal = {Transactions of the American Mathematical Society},
number = {3},
pages = {426},
publisher = {American Mathematical Society},
title = {{Tests of Statistical Hypotheses Concerning Several Parameters When the Number of Observations is Large}},
volume = {54},
year = {1943}
}
@inproceedings{2014arXiv1401.2838M,
abstract = {Scientists often express their understanding of the world through a computationally demanding simulation program. Analyzing the posterior distribution of the parameters given observations (the inverse problem) can be extremely challenging. The Approximate Bayesian Computation (ABC) framework is the standard statistical tool to handle these likelihood free problems, but they require a very large number of simulations. In this work we develop two new ABC sampling algorithms that significantly reduce the number of simulations necessary for posterior inference. Both algorithms use confidence estimates for the accept probability in the Metropolis Hastings step to adaptively choose the number of necessary simulations. Our GPS-ABC algorithm stores the information obtained from every simulation in a Gaussian process which acts as a surrogate function for the simulated statistics. Experiments on a challenging realistic biological problem illustrate the potential of these algorithms.},
address = {Arlington, Virginia, United States},
archivePrefix = {arXiv},
arxivId = {1401.2838},
author = {Meeds, Edward and Welling, Max},
booktitle = {Uncertainty in Artificial Intelligence - Proceedings of the 30th Conference, UAI 2014},
eprint = {1401.2838},
isbn = {9780974903910},
pages = {593--602},
publisher = {AUAI Press},
series = {UAI'14},
title = {{GPS-ABC: Gaussian process surrogate approximate Bayesian computation}},
url = {http://dl.acm.org/citation.cfm?id=3020751.3020813},
year = {2014}
}
@article{meeds2015hamiltonian,
abstract = {Approximate Bayesian computation (ABC) is a powerful and elegant framework for performing inference in simulation-based models. However, due to the difficulty in scaling likelihood estimates, ABC remains useful for relatively lowdimensional problems. We introduce Hamiltonian ABC (HABC), a set of likelihood-free algorithms that apply recent advances in scaling Bayesian learning using Hamiltonian Monte Carlo (HMC) and stochastic gradients. We find that a small number forward simulations can effectively approximate the ABC gradient, allowing Hamiltonian dynamics to efficiently traverse parameter spaces. We also describe a new simple yet general approach of incorporating random seeds into the state of the Markov chain, further reducing the random walk behavior of HABC. We demonstrate HABC on several typical ABC problems, and show that HABC samples comparably to regular Bayesian inference using true gradients on a high-dimensional problem from machine learning.},
archivePrefix = {arXiv},
arxivId = {1503.01916},
author = {Meeds, Edward and Leenders, Robert and Welling, Max},
eprint = {1503.01916},
journal = {Uncertainty in Artificial Intelligence - Proceedings of the 31st Conference, UAI 2015},
pages = {582--591},
title = {{Hamiltonian ABC}},
year = {2015}
}
@article{DBLP:journals/corr/abs-1806-07366,
abstract = {Differential equations are as varied as the phenomena of nature described by them.},
archivePrefix = {arXiv},
arxivId = {1806.07366},
author = {Knauf, Andreas},
doi = {10.1007/978-3-662-55774-7_3},
eprint = {1806.07366},
issn = {20385757},
journal = {UNITEXT - La Matematica per il 3 piu 2},
pages = {31--60},
title = {{Ordinary Differential Equations}},
volume = {109},
year = {2018}
}
@article{2019arXiv190902736G,
abstract = {This paper provides a review of approximate Bayesian computation (ABC) methods for carrying out Bayesian posterior inference, through the lens of density estimation. We describe several recent algorithms and make connection with traditional approaches. We show advantages and limitations of models based on parametric approaches and we then draw attention to developments in machine learning, which we believe have the potential to make ABC scalable to higher dimensions and may be the future direction for research in this area. This article is categorized under: Algorithms and Computational Methods {\textless} Algorithms Statistical and Graphical Methods of Data Analysis {\textless} Bayesian Methods and Theory Statistical Models {\textless} Simulation Models.},
archivePrefix = {arXiv},
arxivId = {1909.02736},
author = {Grazian, Clara and Fan, Yanan},
doi = {10.1002/wics.1486},
eprint = {1909.02736},
issn = {19390068},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
keywords = {approximate Bayesian computation,neural density estimation,synthetic likelihood},
month = {sep},
pages = {arXiv:1909.02736},
title = {{A review of approximate Bayesian computation methods via density estimation: Inference for simulator-models}},
year = {2019}
}
