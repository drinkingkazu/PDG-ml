@book{vapnik2013nature,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={2013},
  publisher={Springer science \& business media}
}

@article{nakkiran2019deep,
  title={Deep double descent: Where bigger models and more data hurt},
  author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1912.02292},
  year={2019}
}

@article{zhang2021understanding-2,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{DBLP:journals/corr/RosascoTV14,
  publtype={informal},
  author={Lorenzo Rosasco and Andrea Tacchetti and Silvia Villa},
  title={Regularization by Early Stopping for Online Learning Algorithms.},
  year={2014},
  cdate={1388534400000},
  journal={CoRR},
  volume={abs/1405.0042},
  url={http://arxiv.org/abs/1405.0042}
}

@article{Kuusela:2015xqa,
    author = "Kuusela, Mikael and Panaretos, Victor M.",
    title = "{Statistical unfolding of elementary particle spectra: Empirical Bayes estimation and bias-corrected uncertainty quantification}",
    eprint = "1505.04768",
    archivePrefix = "arXiv",
    primaryClass = "stat.AP",
    reportNumber = "IMS-AOAS-AOAS857",
    doi = "10.1214/15-AOAS857",
    journal = "Ann. Appl. Stat.",
    volume = "9",
    pages = "1671--1705",
    year = "2015"
}


@InProceedings{pmlr-v80-gunasekar18a,
  title = 	 {Characterizing Implicit Bias in Terms of Optimization Geometry},
  author =       {Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1832--1841},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/gunasekar18a/gunasekar18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/gunasekar18a.html},
  abstract = 	 {We study the bias of generic optimization methods, including Mirror Descent, Natural Gradient Descent and Steepest Descent with respect to different potentials and norms, when optimizing underdetermined linear models or separable linear classification problems. We ask the question of whether the global minimum (among the many possible global minima) reached by optimization can be characterized in terms of the potential or norm, and indecently of hyper-parameter choices, such as stepsize and momentum.}
}

@article{zdeborova2020understanding,
  title={Understanding deep learning is also a job for physicists},
  author={Zdeborov{\'a}, Lenka},
  journal={Nature Physics},
  volume={16},
  number={6},
  pages={602--604},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{Stoye:2018ovl,
    author = "Stoye, Markus and Brehmer, Johann and Louppe, Gilles and Pavez, Juan and Cranmer, Kyle",
    title = "{Likelihood-free inference with an improved cross-entropy estimator}",
    eprint = "1808.00973",
    archivePrefix = "arXiv",
    primaryClass = "stat.ML",
    month = "8",
    year = "2018"
}

@misc{wikipedia-clustering,
howpublished = \url{https://en.wikipedia.org/wiki/Cluster_analysis}
}

@article{dropout,
  author    = {Geoffrey E. Hinton and
               Nitish Srivastava and
               Alex Krizhevsky and
               Ilya Sutskever and
               Ruslan Salakhutdinov},
  title     = {Improving neural networks by preventing co-adaptation of feature detectors},
  journal   = {CoRR},
  volume    = {abs/1207.0580},
  year      = {2012},
  url       = {http://arxiv.org/abs/1207.0580},
  archivePrefix = {arXiv},
  eprint    = {1207.0580},
  timestamp = {Mon, 13 Aug 2018 16:46:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1207-0580.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{baldi2013understanding,
  title={Understanding dropout},
  author={Baldi, Pierre and Sadowski, Peter J},
  journal={Advances in neural information processing systems},
  volume={26},
  pages={2814--2822},
  year={2013}
}



@article{10.1145/1541880.1541882,
author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
title = {Anomaly Detection: A Survey},
year = {2009},
issue_date = {July 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/1541880.1541882},
doi = {10.1145/1541880.1541882},
abstract = {Anomaly detection is an important problem that has been researched within diverse
research areas and application domains. Many anomaly detection techniques have been
specifically developed for certain application domains, while others are more generic.
This survey tries to provide a structured and comprehensive overview of the research
on anomaly detection. We have grouped existing techniques into different categories
based on the underlying approach adopted by each technique. For each category we have
identified key assumptions, which are used by the techniques to differentiate between
normal and anomalous behavior. When applying a given technique to a particular domain,
these assumptions can be used as guidelines to assess the effectiveness of the technique
in that domain. For each category, we provide a basic anomaly detection technique,
and then show how the different existing techniques in that category are variants
of the basic technique. This template provides an easier and more succinct understanding
of the techniques belonging to each category. Further, for each category, we identify
the advantages and disadvantages of the techniques in that category. We also provide
a discussion on the computational complexity of the techniques since it is an important
issue in real application domains. We hope that this survey will provide a better
understanding of the different directions in which research has been done on this
topic, and how techniques developed in one area can be applied in domains for which
they were not intended to begin with.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {15},
numpages = {58},
keywords = {Anomaly detection, outlier detection}
}

@article{Cranmer:2019eaq,
    author = "Cranmer, Kyle and Brehmer, Johann and Louppe, Gilles",
    title = "{The frontier of simulation-based inference}",
    eprint = "1911.01429",
    archivePrefix = "arXiv",
    primaryClass = "stat.ML",
    doi = "10.1073/pnas.1912789117",
    journal = "Proc. Nat. Acad. Sci.",
    volume = "117",
    number = "48",
    pages = "30055--30062",
    year = "2020"
}

@inbook{Brehmer:2020cvb,
    author = "Brehmer, Johann and Cranmer, Kyle",
  title        = {Artificial Intelligence for Particle Physics},
  chapter      = {Simulation-based inference methods for particle physics},
  publisher    = {World Scientific Publishing Co},
  year         = 2021,
}

@inproceedings{Diggle1984MonteCM,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. SUMMARY A prescribed statistical model is a parametric specification of the distribution of a random vector, whilst an implicit statistical model is one defined at a more funda-mental level in terms of a generating stochastic mechanism. This paper develops methods of inference which can be used for implicit statistical models whose distribution theory is intractable. The kernel method of probability density estimation is advocated for estimating a log-likelihood from simulations of such a model. The development and testing of an algorithm for maximizing this estimated log-likelihood function is described. An illustrative example involving a stochastic model for quantal response assays is given. Possible applications of the maximization algorithm to ad hoc methods of parameter estimation are noted briefly, and illustrated},
author = {Diggle, Peter J. and Gratton, Richard J.},
booktitle = {Journal of the Royal Statistical Society: Series B (Methodological)},
doi = {10.1111/j.2517-6161.1984.tb01290.x},
issn = {0035-9246},
number = {2},
pages = {193--212},
title = {{Monte Carlo Methods of Inference for Implicit Statistical Models}},
volume = {46},
year = {1984}
}

@article{rubin1984,
abstract = {A common reaction among applied statisticians is that the Bayesian statistician's energies in an applied problem must be directed at the a priori elicitation of one model specification from which an optimal design and all inferences follow automatically by applying Bayes's theorem to calculate conditional distributions of unknowns given knowns. I feel, however, that the applied Bayesian statistician's tool-kit should be more extensive and include tools that may be usefully labeled frequency calculations. Three types of Bayesianly justifiable and relevant frequency calculations are presented using examples to convey their use for the applied statistician.},
author = {Rubin, Donald B.},
doi = {10.1214/aos/1176346785},
issn = {0090-5364},
journal = {The Annals of Statistics},
number = {4},
pages = {1151--1172},
publisher = {The Institute of Mathematical Statistics},
title = {{Bayesianly Justifiable and Relevant Frequency Calculations for the Applied Statistician}},
url = {https://doi.org/10.1214/aos/1176346785},
volume = {12},
year = {1984}
}
@article{beaumont2002approximate,
abstract = {We propose a new method for approximate Bayesian statistical inference on the basis of summary statistics. The method is suited to complex problems that arise in population genetics, extending ideas developed in this setting by earlier authors. Properties of the posterior distribution of a parameter, such as its mean or density curve, are approximated without explicit likelihood calculations. This is achieved by fitting a local-linear regression of simulated parameter values on simulated summary statistics, and then substituting the observed summary statistics into the regression equation. The method combines many of the advantages of Bayesian statistical inference with the computational efficiency of methods based on summary statistics. A key advantage of the method is that the nuisance parameters are automatically integrated out in the simulation step, so that the large numbers of nuisance parameters that arise in population genetics problems can be handled without difficulty. Simulation results indicate computational and statistical efficiency that compares favorably with those of alternative methods previously proposed in the literature. We also compare the relative efficiency of inferences obtained using methods based on summary statistics with those obtained directly from the data using MCMC.},
author = {Beaumont, Mark A. and Zhang, Wenyang and Balding, David J.},
doi = {10.1111/j.1937-2817.2010.tb01236.x},
issn = {00166731},
journal = {Genetics},
number = {4},
pages = {2025--2035},
pmid = {12524368},
publisher = {Genetics Soc America},
title = {{Approximate Bayesian computation in population genetics}},
volume = {162},
year = {2002}
}

@article{parzen1962estimation,
  title={On estimation of a probability density function and mode},
  author={Parzen, Emanuel},
  journal={The annals of mathematical statistics},
  volume={33},
  number={3},
  pages={1065--1076},
  year={1962},
  publisher={JSTOR}
}

@incollection{davis2011remarks,
  title={Remarks on some nonparametric estimates of a density function},
  author={Davis, Richard A and Lii, Keh-Shin and Politis, Dimitris N},
  booktitle={Selected Works of Murray Rosenblatt},
  pages={95--100},
  year={2011},
  publisher={Springer}
}

@article{louppe2014understanding,
  title={Understanding random forests: From theory to practice},
  author={Louppe, Gilles},
  journal={arXiv preprint arXiv:1407.7502},
  year={2014}
}

@article{Cranmer:2000du,
    author = "Cranmer, Kyle S.",
    title = "{Kernel estimation in high-energy physics}",
    eprint = "hep-ex/0011057",
    archivePrefix = "arXiv",
    doi = "10.1016/S0010-4655(00)00243-5",
    journal = "Comput. Phys. Commun.",
    volume = "136",
    pages = "198--207",
    year = "2001"
}